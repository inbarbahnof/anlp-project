[
  {
    "input": "Problem:\n\nYou are working with a dataset containing customer information, and you want to perform data normalization and dimensionality reduction in preparation for a machine learning model. Given your data in a pandas DataFrame `df` with numerical features, you are using the `StandardScaler` for normalization followed by `PCA` for dimensionality reduction.\n\nHere is an example code setup:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# Assume df is your input DataFrame with numerical columns only\n```\n\nYou want to extract the transformed features after applying normalization but before PCA is applied. How can you achieve this?\n\nStore the output of the transformation from `StandardScaler` into the variable `scaled_data`.\n\nscaled_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n```"
  },
  {
    "input": "Problem:\n\nYou have collected a dataset containing various features of houses including their sizes (in square feet), number of bedrooms, and prices. Before training a machine learning model, you need to perform data preprocessing steps which include scaling the features and one-hot encoding any categorical variables.\n\nGiven the following dataset:\n\n```python\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'size': [1500, 1600, 1700, 1800, 1900],\n    'bedrooms': [3, 3, 3, 4, 4],\n    'price': [300000, 320000, 340000, 360000, 380000],\n    'location': ['city', 'suburb', 'city', 'suburb', 'city']\n})\n```\n\nYou need to scale the 'size' and 'price' columns to have a mean of 0 and a standard deviation of 1, and apply one-hot encoding to the 'location' column.\n\nPlease fill in the blank to create a preprocessed DataFrame called `X_processed`, which includes the necessary transformations:\n\n```python\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Define the preprocessing steps\nnumeric_features = ['size', 'price']\ncategorical_features = ['bedrooms', 'location']\n\n# Create preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n\n# Apply the transformations\nX_processed = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>X_processed = preprocessor.fit_transform(data)</code>"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame that contains sales data for different products across different months. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'C', 'A', 'B', 'C'],\n    'Month': ['2023-01', '2023-01', '2023-01', '2023-02', '2023-02', '2023-02'],\n    'Sales': [150, 200, np.nan, 100, np.nan, 300]\n}\ndf = pd.DataFrame(data)\n```\n\nMy goal is to compute the total sales per product per month, filling any missing values in 'Sales' with 0 before the aggregation. The final DataFrame should have a new column 'Total_Sales' that indicates the total sales for each product across the months.\n\nHow can I achieve this?\n\nA:\n```python\nimport pandas as pd\n\ndata = {\n    'Product': ['A', 'B', 'C', 'A', 'B', 'C'],\n    'Month': ['2023-01', '2023-01', '2023-01', '2023-02', '2023-02', '2023-02'],\n    'Sales': [150, 200, np.nan, 100, np.nan, 300]\n}\ndf = pd.DataFrame(data)\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```python\n\n",
    "output": "\n```python\ndf['Sales'].fillna(0, inplace=True)\nresult = df.groupby(['Product', 'Month'])['Sales'].sum().reset_index(name='Total_Sales')\n```"
  },
  {
    "input": "Problem:\nI have two dataframes that contain a list of products and their sales over time. The first dataframe `df_sales` contains columns for `product_id`, `date`, and `sales_amount`, while the second dataframe `df_details` contains columns for `product_id`, `product_name`, and `category`. I need to merge them based on `product_id`, ensuring that we also handle any missing product names from `df_details`. After that, I want to create a new column `sales_status` that categorizes sales into 'High' if sales are greater than 1000, 'Medium' if sales are between 500 and 1000, and 'Low' for sales below 500. Finally, I would like the resultant dataframe to be sorted by `date` in ascending order.\n\nHere are the two dataframes:\n\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3, 2],\n    'date': ['2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03'],\n    'sales_amount': [1000, 2000, 300, 500]\n})\n\ndf_details = pd.DataFrame({\n    'product_id': [1, 2, 3],\n    'product_name': ['Product A', 'Product B', None],\n    'category': ['Category 1', 'Category 2', 'Category 1']\n})\n```\n\nAfter merging, how can I generate the final dataframe as described? Use the following placeholder for the result:\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df_sales.merge(df_details, on='product_id', how='left')\nresult['sales_status'] = pd.cut(result['sales_amount'], bins=[-1, 500, 1000, float('inf')], labels=['Low', 'Medium', 'High'])\nresult = result.sort_values(by='date')\n</code>"
  },
  {
    "input": "Problem:\nI'm currently working with PyTorch to normalize a batch of feature vectors represented as a 2D tensor. My tensor `data` has shape (5, 3), and looks like this:\n```python\ndata = torch.tensor([[1.0, 2.0, 3.0],\n                     [4.0, 5.0, 6.0],\n                     [7.0, 8.0, 9.0],\n                     [10.0, 11.0, 12.0],\n                     [13.0, 14.0, 15.0]])\n```\nI want to normalize each feature (each column) by subtracting the mean and dividing by the standard deviation across each column. How should I implement this normalization while ensuring that the output tensor maintains the same shape?\n\nA:\n<code>\nimport torch\n\ndata = torch.tensor([[1.0, 2.0, 3.0],\n                     [4.0, 5.0, 6.0],\n                     [7.0, 8.0, 9.0],\n                     [10.0, 11.0, 12.0],\n                     [13.0, 14.0, 15.0]])\n\nnormalized_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nnormalized_data = (data - data.mean(dim=0)) / data.std(dim=0)\n</code>"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing customer transaction records from an e-commerce website. The dataset includes the following columns: `customer_id`, `transaction_date`, and `transaction_value`. Your goal is to preprocess this data to analyze customer spending behavior.\n\nYou want to determine the average transaction value per customer, but first, you need to ensure that the `transaction_date` is in datetime format and handle any missing values in `transaction_value` by replacing them with the median transaction value for that customer.\n\nGiven the initial setup as follows:\n\n<code>\nimport pandas as pd\nimport numpy as np\n\n# Sample data creation\ndata = {\n    'customer_id': [1, 1, 2, 2, 3, 3, 3, 4],\n    'transaction_date': ['2021-01-01', '2021-01-02', '2021-01-01', None, '2021-01-03', '2021-01-04', None, '2021-01-05'],\n    'transaction_value': [100.0, np.nan, 200.0, 300.0, 50.0, 60.0, np.nan, 80.0]\n}\ndf = pd.DataFrame(data)\n</code>\n\nYou need to fill in the blank for the following line to compute the average transaction value for each customer after handling the preprocessing tasks:\n\naverage_transaction_value = df.groupby('customer_id')['transaction_value'].mean()\n\nHow would you correctly preprocess the dataset to ensure that this line produces the intended results? Fill in the blank for the lines of code required for the preprocessing:\n\n<code>\n# Convert transaction_date to datetime\ndf['transaction_date'] = pd.to_datetime(df['transaction_date'])\n\n# Replace missing values in transaction_value with the median transaction value\ndf['transaction_value'].fillna(df.groupby('customer_id')['transaction_value'].transform('median'), inplace=True)\n\naverage_transaction_value = df.groupby('customer_id')['transaction_value'].mean()\n</code> \nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = average_transaction_value.reset_index()"
  },
  {
    "input": "Problem:\n\nI am working on a classification problem where I need to preprocess my data before training a machine learning model. The dataset contains several numerical features including some outliers that I want to handle. I would like to apply robust scaling to my features using the `RobustScaler` from `sklearn.preprocessing`. In addition, I would like to split the dataset into training and testing sets, ensuring that I maintain the original distribution by using stratified sampling based on the target labels.\n\nGiven that I have a DataFrame `df` containing my features in columns `feature1`, `feature2`, ..., `featureN` and the target labels in `label`, how would I scale the features and split the dataset? Please construct the code such that after executing the provided code, the variable `train_X` contains the scaled training features, `test_X` contains the scaled testing features, and `train_y` and `test_y` contain the corresponding labels.\n\nA:\n<code>\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n\ndf = load_data()  # Assume df has been loaded\nX = df[['feature1', 'feature2', 'featureN']]\ny = df['label']\n\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\n\ntrain_X, test_X, train_y, test_y = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n</code>\ntrain_X, test_X, train_y, test_y = ...  # put solution in this variable",
    "output": "\n<code>\ntrain_X, test_X, train_y, test_y = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames representing sales data from two different regions. The first DataFrame, `sales_region1`, contains sales figures from Region 1, and the second DataFrame, `sales_region2`, contains sales figures from Region 2. Each DataFrame has an 'id' column and a 'sales' column. I want to combine these DataFrames into a single DataFrame that contains the total sales for each id, ensuring that if an id appears in both DataFrames, the sales figures are summed. If an id appears in one DataFrame only, it should still be included. The 'id' should be sorted in ascending order.\n\nHere are the DataFrames:\n```python\nimport pandas as pd\n\nsales_region1 = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'sales': [100, 200, 300, 400]\n})\n\nsales_region2 = pd.DataFrame({\n    'id': [2, 3, 5],\n    'sales': [150, 250, 100]\n})\n```\n\nI'd like to merge these DataFrames while summing the sales, and output the result sorted by 'id'. What is the code to achieve this?\n\nA:\n```python\nimport pandas as pd\n\nsales_region1 = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'sales': [100, 200, 300, 400]\n})\n\nsales_region2 = pd.DataFrame({\n    'id': [2, 3, 5],\n    'sales': [150, 250, 100]\n})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.concat([sales_region1, sales_region2]).groupby('id', as_index=False).sum().sort_values('id')\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing customer transaction records, stored in a pandas DataFrame. The dataset includes information on the transaction date, customer ID, and the purchase amount. The goal is to analyze the spending behavior of customers by creating a new column that categorizes customers into \"low,\" \"medium,\" or \"high\" spenders based on the average transaction amount.\n\nGiven the following DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'transaction_date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-05', '2023-01-06', '2023-01-07'],\n    'customer_id': ['C1', 'C2', 'C1', 'C2', 'C1', 'C3'],\n    'purchase_amount': [20.0, 45.0, 30.0, 55.0, 10.0, 100.0]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to create a new column called \"spending_category\" that is determined as follows:\n- \"low\" if the average transaction amount is less than $25,\n- \"medium\" if the average transaction amount is between $25 and $50 (inclusive),\n- \"high\" if the average transaction amount is greater than $50.\n\nA:\n```python\n# Fill in the blank with the appropriate code to achieve the task.\ndf['spending_category'] = ...\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf['spending_category'] = pd.cut(df.groupby('customer_id')['purchase_amount'].transform('mean'), \n                                  bins=[-float('inf'), 25, 50, float('inf')], \n                                  labels=['low', 'medium', 'high'])\n```"
  },
  {
    "input": "Problem:\n\nI have two DataFrames, df_users and df_orders, representing user information and their corresponding orders as follows:\n\ndf_users\n```\n   user_id username\n0        1    Alice\n1        2      Bob\n2        3  Charlie\n```\n\ndf_orders\n```\n   order_id user_id  order_date amount\n0        101       1  2023-01-05   150\n1        102       2  2023-01-08   200\n2        103       2  2023-01-10   250\n3        104       4  2023-01-12   300\n```\n\nI need to merge these DataFrames such that you keep all users from df_users. For those users who have made orders, the corresponding order details should appear alongside their information. Where users haven\u2019t placed any orders, the order details should show NaNs. Furthermore, I want to replace any NaN values in the 'amount' column with 0, to represent no orders.\n\nThe expected output must look like this:\n```\n   user_id username  order_id  order_date amount\n0        1    Alice      101  2023-01-05   150.0\n1        2      Bob      102  2023-01-08   200.0\n2        2      Bob      103  2023-01-10   250.0\n3        3  Charlie      NaN         NaN     0.0\n```\n\nI attempted to merge using `pd.merge` but didn't know how to handle the NaN values correctly. \n\nA:\n<code>\nimport pandas as pd\n\ndf_users = pd.DataFrame({'user_id': [1, 2, 3],\n                         'username': ['Alice', 'Bob', 'Charlie']})\ndf_orders = pd.DataFrame({'order_id': [101, 102, 103, 104],\n                          'user_id': [1, 2, 2, 4],\n                          'order_date': ['2023-01-05', '2023-01-08', '2023-01-10', '2023-01-12'],\n                          'amount': [150, 200, 250, 300]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = pd.merge(df_users, df_orders, on='user_id', how='left').fillna({'amount': 0})"
  },
  {
    "input": "Problem:\n\nYou are working with a batch of image feature embeddings represented as a tensor in PyTorch, shaped as follows:\n\n```python\nfeatures = torch.tensor([[0.2, 0.8, 0.5],\n                         [0.9, 0.1, 0.4],\n                         [0.2, 0.5, 0.9]])\n```\n\nYou want to create a boolean mask that indicates which embeddings have a maximum value greater than 0.6. The resulting mask should have `True` for those indices and `False` where the condition is not met. How can you achieve this?\n\nA:\n```python\nimport torch\n\nfeatures = torch.tensor([[0.2, 0.8, 0.5],\n                         [0.9, 0.1, 0.4],\n                         [0.2, 0.5, 0.9]])\n```\nmask = ... # put solution in this variable\nBEGIN SOLUTION",
    "output": "\n```python\nmask = (features.max(dim=1).values > 0.6)\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products, including their categories and prices. The DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Category': ['Electronics', 'Furniture', 'Electronics', 'Clothing', 'Clothing', 'Furniture'],\n    'Product': ['TV', 'Sofa', 'Laptop', 'T-shirt', 'Jeans', 'Table'],\n    'Price': [300, 800, 1200, 40, 60, 150]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to normalize the 'Price' values within each product category to ensure that the data can be effectively analyzed, i.e., constrain the data values to a range between 0 and 1 within each category. Your goal is to add a new column 'Normalized_Price' to the existing DataFrame.\n\nHow can you achieve this?\n\n```python\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n```",
    "output": "\n<code>\ndf['Normalized_Price'] = df.groupby('Category')['Price'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>"
  },
  {
    "input": "Problem:\n\nYou have a 4D tensor of shape (100, 3, 32, 32), representing a batch of 100 RGB images of size 32x32 pixels. You want to normalize each pixel value across the batch so that the pixel values across each channel (RGB) have a mean of 0 and a standard deviation of 1. \n\nHere\u2019s what you tried:\n\n```python\nmean = tensor.mean(dim=0, keepdim=True) \nstd = tensor.std(dim=0, keepdim=True)\nnormalized_tensor = (tensor - mean) / std\n```\n\nHowever, you're getting unexpected results. \n\nHow can you adjust the code to ensure the normalization is done per channel correctly?\n\nA:\n```python\nimport torch\n\ntensor = load_data()  # Assume tensor is of shape (100, 3, 32, 32)\nnormalized_tensor = ...  # put solution in this variable\n```",
    "output": "\n```python\nmean = tensor.mean(dim=0, keepdim=True) \nstd = tensor.std(dim=0, keepdim=True)\nnormalized_tensor = (tensor - mean) / std\n```"
  },
  {
    "input": "Problem:\nI have a Pandas DataFrame which contains information about sales transactions with hierarchical column names representing different categories. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    ('Store', 'North'): [100, 150, 200],\n    ('Store', 'South'): [200, 250, 300],\n    ('Store', 'East'): [300, 350, 400],\n    ('Store', 'West'): [400, 450, 500],\n    ('Returns', 'North'): [10, 15, 20],\n    ('Returns', 'South'): [20, 25, 30],\n    ('Returns', 'East'): [30, 35, 40],\n    ('Returns', 'West'): [40, 45, 50]\n}\n\ndf = pd.DataFrame(data, index=['Q1', 'Q2', 'Q3'])\n```\n\nI want to calculate the total sales after accounting for returns and create a new multi-index DataFrame that shows these totals under a new level called `Total` with two sub-levels: `Store` and `Returns`. The output DataFrame should look like this:\n\n```\n           Store          Returns\n           North South East West\nTotal\nQ1        90    180  270  360\nQ2       135    225  315  405\nQ3       180    270  360  450\n```\n\nHow can I create this new DataFrame? Please provide the code to fill in the missing part.\n\nA:\n```python\ntotals = ... # put solution in this variable\n```",
    "output": "\n```python\ntotals = pd.DataFrame({\n    ('Total', 'Store'): df[('Store', '')].sum(axis=1) - df[('Returns', '')].sum(axis=1),\n    ('Total', 'Returns'): df[('Returns', '')].sum(axis=1)\n}).set_index(df.index)\n```"
  },
  {
    "input": "## Problem\nProblem:\nYou have two pandas DataFrames, `sales` and `returns`, representing product sales and returns data, respectively.\n\n```python\nsales = pd.DataFrame({\n    'product_id': [1, 2, 3, 4, 5, 6],\n    'quantity_sold': [100, 200, 150, 300, 50, 80],\n    'sale_date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-01', '2023-01-02']\n})\n\nreturns = pd.DataFrame({\n    'product_id': [1, 2, 2, 3, 6],\n    'quantity_returned': [10, 50, 20, 30, 5],\n    'return_date': ['2023-01-05', '2023-01-04', '2023-01-02', '2023-01-06', '2023-01-07']\n})\n```\n\nYou need to merge the two DataFrames on `product_id` such that it preserves all rows from the `sales` DataFrame while adding the respective `quantity_returned` where applicable. If a product has no returns, the `quantity_returned` should default to 0. Moreover, you want to create a new column `net_quantity` that indicates the number of products sold after returns, retaining the original order of `sales` DataFrame.\n\nThe expected output should look like this:\n\n```\n   product_id  quantity_sold   sale_date  quantity_returned  net_quantity\n0           1            100  2023-01-01                10           90\n1           2            200  2023-01-02                70          130\n2           3            150  2023-01-01                30          120\n3           4            300  2023-01-03                 0          300\n4           5             50  2023-01-01                 0           50\n5           6             80  2023-01-02                 5           75\n```\n\nYou currently have the following code snippet to initiate your analysis:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'product_id': [1, 2, 3, 4, 5, 6],\n    'quantity_sold': [100, 200, 150, 300, 50, 80],\n    'sale_date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-01', '2023-01-02']\n})\n\nreturns = pd.DataFrame({\n    'product_id': [1, 2, 2, 3, 6],\n    'quantity_returned': [10, 50, 20, 30, 5],\n    'return_date': ['2023-01-05', '2023-01-04', '2023-01-02', '2023-01-06', '2023-01-07']\n})\n\nresult = ... # put solution in this variable\n```\nFill in the solution to merge these DataFrames according to the specified requirements.",
    "output": "\n```python\nresult = sales.merge(returns.groupby('product_id', as_index=False).sum(), on='product_id', how='left').fillna(0)\nresult['quantity_returned'] = result['quantity_returned'].astype(int)\nresult['net_quantity'] = result['quantity_sold'] - result['quantity_returned']\n```"
  },
  {
    "input": "Problem:\n\nI have a 4D tensor representing a batch of images with the shape `(batch_size, channels, height, width)`. I want to apply a boolean mask to this tensor to zero out all pixels in the images where a corresponding condition fails (e.g., pixels below a certain threshold). The mask is of shape `(batch_size, height, width)` and contains boolean values indicating which pixels should be retained (True) or set to zero (False).\n\nHow can I apply this boolean mask to the 4D tensor to modify the images accordingly?\n\nA:\n```python\nimport torch\nimages = load_data()  # shape (batch_size, channels, height, width)\nmask = load_mask()  # shape (batch_size, height, width)\nresult = ...  # put solution in this variable\n```",
    "output": "\n```python\nresult = images * mask.unsqueeze(1).float()\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame representing sales data of a store over several months. The DataFrame has the following structure:\n\n```\n          Month  Sales\n0       January   100\n1      February   200\n2         March   NaN\n3         April   150\n4          May     300\n5         June    NaN\n6         July     250\n7       August    400\n8    September   200\n9      October   100\n10  November     NaN\n11  December    300\n```\n\nYou need to calculate a new column `Adjusted_Sales` where:\n- The value is the sales amount for that month.\n- If the sales amount is missing (NaN), it should be replaced by the average of the previous and the next month's sales values.\n- If both neighboring sales are NaN, the `Adjusted_Sales` should also be NaN.\n\nHere is the starting DataFrame:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Month': ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'],\n    'Sales': [100, 200, None, 150, 300, None, 250, 400, 200, 100, None, 300]\n})\n```\n\nFill in the blank with the appropriate pandas code to create the `Adjusted_Sales` column as described.\n\n```python\ndf['Adjusted_Sales'] = ...\n```",
    "output": "\n```python\ndf['Adjusted_Sales'] = df['Sales'].fillna((df['Sales'].shift() + df['Sales'].shift(-1)) / 2)\n```"
  },
  {
    "input": "Problem:\nYou are provided with two DataFrames, `sales` and `returns`, structured as follows:\n\n`sales`:\n```\n    OrderID    Date       Product  Amount\n0      101 2021-01-01   Widget A     20\n1      102 2021-01-02   Widget B     30\n2      103 2021-01-03   Widget A     25\n3      104 2021-01-04   Widget C     50\n4      105 2021-01-05   Widget B     40\n```\n\n`returns`:\n```\n    OrderID    Date       Product  Amount\n0      102 2021-01-03   Widget B     10\n1      104 2021-01-06   Widget C     50\n2      101 2021-01-01   Widget A     5\n```\n\nYour goal is to create a consolidated DataFrame that combines `sales` and `returns`, adjusting the `Amount` for returning products. For each `OrderID`, if there is a matching entry in `returns`, the `Amount` in `sales` should be reduced by the `Amount` from `returns`. If the `Amount` becomes negative, it should be set to zero. Additionally, you need to format the `Date` to \"dd-MMM-yyyy\" and sort the results by `Date`.\n\nThe resulting DataFrame should look like:\n```\n    OrderID    Date       Product  Amount\n0      101 01-Jan-2021   Widget A     15\n1      102 02-Jan-2021   Widget B     20\n2      103 03-Jan-2021   Widget A     25\n3      104 04-Jan-2021   Widget C      0\n4      105 05-Jan-2021   Widget B     40\n```\n\nAssuming you've already imported pandas and created the dataframes, complete the code below:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'OrderID': [101, 102, 103, 104, 105],\n    'Date': pd.to_datetime(['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05']),\n    'Product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],\n    'Amount': [20, 30, 25, 50, 40]\n})\n\nreturns = pd.DataFrame({\n    'OrderID': [102, 104, 101],\n    'Date': pd.to_datetime(['2021-01-03', '2021-01-06', '2021-01-01']),\n    'Product': ['Widget B', 'Widget C', 'Widget A'],\n    'Amount': [10, 50, 5]\n})\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = sales.copy()\nfor index, row in returns.iterrows():\n    result.loc[result['OrderID'] == row['OrderID'], 'Amount'] -= row['Amount']\nresult['Amount'] = result['Amount'].clip(lower=0)\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y')\nresult = result.sort_values(by='Date').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\n\nI am working on a recommendation system using PyTorch, where I need to compute the cosine similarity between user embeddings and item embeddings. The user embeddings are stored in a tensor of shape (1000, 128) and the item embeddings in another tensor of shape (5000, 128). I want to extract the top K most similar items for each user based on cosine similarity. \n\nHere is my current setup:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\nuser_embeddings = torch.randn(1000, 128)\nitem_embeddings = torch.randn(5000, 128)\n```\n\nHow do I compute the cosine similarity and then get the top K similar items for each user?\n\n```python\nK = 5\nsimilarity = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nsimilarity = F.cosine_similarity(user_embeddings.unsqueeze(1), item_embeddings.unsqueeze(0), dim=2)\ntop_k_indices = torch.topk(similarity, K, dim=1).indices\nEND SOLUTION\n```"
  },
  {
    "input": "## Problem\n\nI have a Pandas DataFrame that represents survey results from different demographic groups, and the structure looks something like this:\n\n```\n      Age    Income  Gender    Response\nGroup1 25.0  50000  Male    [Yes, No]\nGroup2 30.0  60000  Female  [Yes, Yes, No]\nGroup3 35.0  70000  Male    [No]\nGroup4 40.0  80000  Female  [Yes]\n```\n\nI want to expand the 'Response' column into multiple binary indicator columns for each unique response (like Yes and No) while also normalizing the 'Income' column to have a mean of 0 and standard deviation of 1. The final DataFrame should have the structure:\n\n```\n      Age   Income       Gender  Yes  No\nGroup1 25.0  -1.341641  Male  1.0  1.0\nGroup2 30.0  -0.447214  Female  1.0  0.0\nGroup3 35.0   0.447214  Male  0.0  1.0\nGroup4 40.0   1.341641  Female  1.0  0.0\n```\n\nHow can I normalize the 'Income' column and expand the 'Response' column appropriately using Pandas?\n\n```python\nimport pandas as pd\n\ndata = {\n    'Age': [25.0, 30.0, 35.0, 40.0],\n    'Income': [50000, 60000, 70000, 80000],\n    'Gender': ['Male', 'Female', 'Male', 'Female'],\n    'Response': [['Yes', 'No'], ['Yes', 'Yes', 'No'], ['No'], ['Yes']]\n}\ndf = pd.DataFrame(data, index=['Group1', 'Group2', 'Group3', 'Group4'])\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\n# Normalize the 'Income' column\ndf['Income'] = (df['Income'] - df['Income'].mean()) / df['Income'].std()\n\n# Expand the 'Response' column into binary indicator columns\nresponse_dummies = df['Response'].explode().str.get_dummies().sum(level=0)\n\n# Combine the original DataFrame with the new response columns\nresult = pd.concat([df.drop(columns='Response'), response_dummies], axis=1)\n```"
  },
  {
    "input": "Problem:\n\nContext:\nI have two DataFrames representing product sales data from two different regions. I need to combine these datasets while ensuring that products from the second region replace the sales figures of the first region wherever they coincide. Additionally, I want to add a column that denotes whether a product's sales data from the second region is filling in a duplicate entry.\n\nDataFrames:\ndf1:\n```\n    Product    Sales\n0   A          150\n1   B          200\n2   C          300\n```\n\ndf2:\n```\n    Product    Sales\n0   B          250\n1   D          400\n```\n\nThe goal is to achieve a final DataFrame like this, maintaining the order of products from df1:\n```\n    Product    Sales     is_duplicated\n0   A          150      False\n1   B          250      True\n2   C          300      False\n3   D          400      True\n```\n\nYou can start with this code snippet:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'Product': ['A', 'B', 'C'], 'Sales': [150, 200, 300]})\ndf2 = pd.DataFrame({'Product': ['B', 'D'], 'Sales': [250, 400]})\n```\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nmerged = pd.merge(df1, df2, on='Product', how='outer', suffixes=('_df1', '_df2'))\nmerged['Sales'] = merged['Sales_df2'].combine_first(merged['Sales_df1'])\nmerged['is_duplicated'] = merged['Sales_df2'].notnull()\nresult = merged[['Product', 'Sales', 'is_duplicated']]\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products across several months. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Product': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'Month': ['2022-01', '2022-02', '2022-01', '2022-02', '2022-01', '2022-02'],\n    'Sales': [100, 150, np.nan, 200, 300, np.nan]\n})\n```\n\nYour task is to calculate the total sales for each product, replacing any missing sales values in a month with the average sales of that product across all months. The expected output should show each product with the corrected sales total, and the DataFrame should have an additional column 'Adjusted_Sales' that reflects the total sales after this adjustment.\n\nThe resulting DataFrame should appear like this:\n\n```\n  Product     Month  Sales  Adjusted_Sales\n0       A  2022-01  100.0            250.0\n1       A  2022-02  150.0            250.0\n2       B  2022-01    NaN            200.0\n3       B  2022-02  200.0            200.0\n4       C  2022-01  300.0            300.0\n5       C  2022-02    NaN            300.0\n```\n\nFill in the blank:\n```python\ndf['Adjusted_Sales'] = ...\n```",
    "output": "\n```python\ndf['Adjusted_Sales'] = df.groupby('Product')['Sales'].transform(lambda x: x.fillna(x.mean()))\n```"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame with sales data structured as follows:\n   Product   Month   Sales\n0  Product A  Jan      200\n1  Product B  Jan      150\n2  Product A  Feb      0\n3  Product B  Feb      300\n4  Product A  Mar      400\n5  Product B  Mar      NaN\n6  Product A  Apr      500\n7  Product B  Apr      100\n\nI want to compute the total sales for each product, but in the case of months where sales are zero or NaN, those months should be ignored in the sum. The final aggregated output should have two columns: 'Product' and 'Total_Sales', containing the respective total sales for each product.\n\nHow can I achieve this? \n\nHere is how I set up the DataFrame:\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['Product A', 'Product B', 'Product A', 'Product B', \n                'Product A', 'Product B', 'Product A', 'Product B'],\n    'Month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar', 'Apr', 'Apr'],\n    'Sales': [200, 150, 0, 300, 400, np.nan, 500, 100]\n}\ndf = pd.DataFrame(data)\n```\nfill in the blank below:\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df[df['Sales'].notna() & (df['Sales'] > 0)].groupby('Product')['Sales'].sum().reset_index(name='Total_Sales')\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames representing sales data from different regions over a time period:\n\ndf1:\n   store_id   region      date  sales\n0        1   North  2023/01/01     100\n1        2   North  2023/01/02     150\n2        3   East   2023/01/01     200\n3        4   East   2023/01/01     250\n4        5   South  2023/01/01     300\n\ndf2:\n   store_id   date  sales\n0        3  2023/01/02     150\n1        4  2023/01/03     350\n2        5  2023/01/02     400\n3        6  2023/01/01     500\n4        7  2023/01/02     600\n\nI want to merge these DataFrames based on the store_id and retain all records. If a store_id exists in df2 but not in df1, I want to fill the region column with NaN. After merging, I also want to replace any NaN values in the sales column with 0 for easier analysis. Finally, I expect the DataFrame to be sorted by store_id and date, and here's what it should look like after processing:\n\n   store_id   region      date  sales\n0        1   North  2023/01/01     100\n1        2   North  2023/01/02     150\n2        3    East   2023/01/01     200\n3        3    East   2023/01/02     150\n4        4    East   2023/01/01     250\n5        4    East   2023/01/03     350\n6        5   South  2023/01/01     300\n7        5   South  2023/01/02     400\n8        6     NaN  2023/01/01     500\n9        7     NaN  2023/01/02     600\n\nCurrently, I have merged the DataFrames using:\n```python\nresult = pd.concat([df1, df2], axis=0)\n```\n\nWhat should I add to process the DataFrame as described, ensuring it has the right structure and changes? \n\nA:\n<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({'store_id': [1, 2, 3, 4, 5],\n                    'region': ['North', 'North', 'East', 'East', 'South'],\n                    'date': ['2023/01/01', '2023/01/02', '2023/01/01', '2023/01/01', '2023/01/01'],\n                    'sales': [100, 150, 200, 250, 300]})\n\ndf2 = pd.DataFrame({'store_id': [3, 4, 5, 6, 7],\n                    'date': ['2023/01/02', '2023/01/03', '2023/01/02', '2023/01/01', '2023/01/02'],\n                    'sales': [150, 350, 400, 500, 600]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = pd.merge(df1, df2, on=['store_id', 'date'], how='outer').fillna({'sales': 0}).sort_values(by=['store_id', 'date'])\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing the sales records for a retail store, structured as a DataFrame with the following columns: 'date', 'store_id', 'product_id', 'units_sold', and 'revenue'. You want to analyze the monthly sales performance by aggregating the data.\n\nYour task is to create a new DataFrame that summarizes the total units sold and total revenue for each store on a monthly basis. \n\nThe DataFrame `df` looks something like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n    'store_id': [1, 2] * 50,\n    'product_id': [101, 102, 103] * 33 + [101],\n    'units_sold': [10, 20, 15, 30, 25] * 20,\n    'revenue': [100, 200, 150, 300, 250] * 20\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to complete the following code to achieve this aggregation, grouping the results by 'store_id' and 'month' (extracted from 'date'), and summing 'units_sold' and 'revenue':\n\n```python\ndf['month'] = df['date'].dt.to_period('M')\nmonthly_sales = df.groupby(['store_id', 'month'])[['units_sold', 'revenue']].sum().reset_index()\n```\n\nReset the index on the resulting DataFrame to ensure it is formatted correctly for final output. \n\nresult = ... # put solution in this variable",
    "output": "\n```python\nresult = monthly_sales.reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nI am working with PyTorch and I have two tensors A and B of the same shape. I want to create a new tensor C that contains elements from A where the corresponding elements in B are greater than or equal to a threshold value of 0.5. If an element in B is less than 0.5, I want the corresponding element in C to be set to 0. I\u2019m not sure how to achieve this boolean masking operation effectively.\n\nA:\n<code>\nimport torch\n\nA = torch.tensor([[0.1, 0.6, 0.3], [0.7, 0.2, 0.9]])\nB = torch.tensor([[0.4, 0.8, 0.1], [0.6, 0.3, 0.5]])\n</code>\nC = ... # put solution in this variable",
    "output": "\n<code>\nC = torch.where(B >= 0.5, A, torch.tensor(0.0))\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame that records sales data across different stores and products, and it contains some missing values. The DataFrame looks like this:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'store': ['Store A', 'Store A', 'Store B', 'Store B', 'Store C'],\n    'product': ['Apples', 'Bananas', 'Apples', 'Bananas', 'Apples'],\n    'sales': [100, np.nan, 200, 150, 300],\n    'returns': [10, 5, np.nan, 20, 25]\n})\n  \n  store   product  sales  returns\n0 Store A   Apples  100.0      10.0\n1 Store A  Bananas    NaN       5.0\n2 Store B   Apples  200.0       NaN\n3 Store B  Bananas  150.0      20.0\n4 Store C   Apples  300.0      25.0\n\nMy goal is to calculate the net sales for each store by subtracting the returns from the sales. If there are any NaN values in the sales or returns for a particular store-product combination, the net sales for that combination should also be NaN. \n\nI would like to create a new column 'net_sales' to store these results. \n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'store': ['Store A', 'Store A', 'Store B', 'Store B', 'Store C'],\n    'product': ['Apples', 'Bananas', 'Apples', 'Bananas', 'Apples'],\n    'sales': [100, np.nan, 200, 150, 300],\n    'returns': [10, 5, np.nan, 20, 25]\n})\n</code>\ndf['net_sales'] = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['net_sales'] = df['sales'] - df['returns']\n</code>"
  },
  {
    "input": "Problem:\n\nYou have two DataFrames, sales and inventory, structured as follows:\n\nsales:\n```\n   product_id   date    sales\n0           1  2023-01-01     100\n1           2  2023-01-01      50\n2           1  2023-01-02     150\n3           3  2023-01-02      80\n```\n\ninventory:\n```\n   product_id restock_date  stock\n0           1   2023-01-05     200\n1           2   2023-01-03     100\n2           3   2023-01-04     60\n3           4   2023-01-06     50\n```\n\nYou need to merge these two DataFrames such that:\n1. All rows from sales are retained.\n2. Missing inventory information (where `sales.product_id` does not match `inventory.product_id`) should be filled with `NaN`.\n3. Ensure the date formats in the final DataFrame are in the format \"DD-Mon-YYYY\".\n\nThe output DataFrame should look like this:\n\n```\n   product_id       date  sales restock_date  stock\n0           1  01-Jan-2023     100   2023-01-05   200\n1           2  01-Jan-2023      50   2023-01-03   100\n2           1  02-Jan-2023     150   2023-01-05   200\n3           3  02-Jan-2023      80   2023-01-04    60\n```\n\nYou attempt the merge but receive a DataFrame that does not format the dates correctly. Your task is to write the code that combines these DataFrames while applying the necessary transformations for the dates.\n\nHere is your starting point:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3],\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'sales': [100, 50, 150, 80]\n})\n\ninventory = pd.DataFrame({\n    'product_id': [1, 2, 3, 4],\n    'restock_date': ['2023-01-05', '2023-01-03', '2023-01-04', '2023-01-06'],\n    'stock': [200, 100, 60, 50]\n})\n```\n\nYour solution should be assigned to `result`:\n```python\nresult = ...  # put your solution in this variable\n```",
    "output": "\n```python\nresult = pd.merge(sales, inventory, on='product_id', how='left')\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult['restock_date'] = pd.to_datetime(result['restock_date']).dt.strftime('%Y-%m-%d')\n```"
  },
  {
    "input": "Problem:\n\nI am working with a dataset of customer transactions from an online store. The dataset contains the following columns: 'customer_id', 'transaction_amount', and 'transaction_date'. I want to calculate the average transaction amount for each customer and identify customers who have an average transaction amount greater than a set threshold of $100.\n\nHere is my initial code setup:\n\n```python\nimport pandas as pd\n\ndata = {'customer_id': [1, 2, 1, 2, 3, 1, 3], \n        'transaction_amount': [120, 80, 200, 150, 90, 60, 300],\n        'transaction_date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-05', '2023-01-06', '2023-01-09', '2023-01-10', '2023-01-15'])}\n\ndf = pd.DataFrame(data)\n```\n\nI would like to create a new DataFrame that contains only the 'customer_id' and their average transaction amount, filtered to include only those customers whose average amount is greater than $100.\n\nThe code returns the result in this variable:\n\n```python\nresult = ... # put solution in this variable\n```\n\n### BEGIN SOLUTION\n```python\n```",
    "output": "\n```python\nresult = df.groupby('customer_id')['transaction_amount'].mean().reset_index()\nresult = result[result['transaction_amount'] > 100]\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames containing sales data for different products over time.\n\n`sales_df`:\n```\n   product_id    date  quantity\n0          A  2022-01-01       10\n1          A  2022-01-02       15\n2          B  2022-01-01       5\n3          B  2022-01-03       10\n4          C  2022-01-02       20\n```\n\n`promo_df`:\n```\n   product_id    date  discount\n0          A  2022-01-01         1\n1          B  2022-01-02         2\n2          B  2022-01-03         2\n3          C  2022-01-02         3\n4          D  2022-01-01         4\n```\n\nThe goal is to merge these two DataFrames such that each sale in `sales_df` has the corresponding discount from `promo_df` based on `product_id` and `date`. If there is no corresponding promotion for a sale date, fill the `discount` column with `0`. You also need to ensure the results are sorted by `product_id` and `date`.\n\nThe current merge attempt looks like this:\n```python\nmerged_df = pd.merge(sales_df, promo_df, on=['product_id', 'date'], how='outer')\n```\n\nHowever, you want to replace NaN values in the `discount` column with `0` afterwards.\n\nWhat is the correct line of code to replace the NaN values in the `discount` column of `merged_df` with `0`? \n\nProvide your answer as follows:\n```python\nmerged_df['discount'] = \n```\n",
    "output": "\n```python\nmerged_df['discount'] = merged_df['discount'].fillna(0)\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing the sales information of various products along with their sales regions. The dataset is structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product', 'Name'): ['A', 'B', 'C', 'A', 'B'],\n    ('Product', 'Category'): ['Electronics', 'Furniture', 'Electronics', 'Electronics', 'Furniture'],\n    ('Region', 'West'): [100, 200, 150, 300, None],\n    ('Region', 'East'): [None, 250, 100, 400, 150],\n    ('Region', 'North'): [50, 75, None, None, 50]\n}\ndf = pd.DataFrame(data)\n```\n\nThe DataFrame `df` has a multi-level column structure with two levels: `'Product'` and `'Region'`. \n\nYour goal is to normalize the sales data such that you extract a DataFrame containing only the total sales for each product across all regions (summing up the sales values). The resulting DataFrame should have the following format:\n\n```\n    Product   Total_Sales\n0      A          ?\n1      B          ?\n2      C          ?\n```\n\nTo accomplish this, you need to fill in the blank below with the appropriate code:\n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby(('Product', 'Name')).sum().reset_index()\nresult.columns = ['Product', 'Total_Sales']\n```"
  },
  {
    "input": "### Problem\n\nYou have been given a 3D tensor representing image data in the shape (batch_size, channels, height, width). You want to apply a boolean mask to filter out specific pixels across all images based on a condition. The condition is that pixel values should be greater than a certain threshold. The remaining values in the tensor should be set to zero. Write a function that takes the tensor and the threshold as inputs, applies the mask, and returns the modified tensor.\n\nExample input:\n```python\nimport torch\n\ntensor = torch.tensor([[[0.1, 0.5, 0.3],\n                         [0.7, 0.2, 0.6]],\n                        \n                        [[0.4, 0.9, 0.1],\n                         [0.8, 0.3, 0.5]]])\nthreshold = 0.5\n```\nThe desired output for the input above, after applying the threshold mask, should look like this:\n```python\ntensor([[ [0.0, 0.0, 0.0],\n          [0.7, 0.0, 0.6]],\n          \n         [ [0.0, 0.9, 0.0],\n          [0.8, 0.0, 0.5]]])\n```\n\nA:\n\n<code>\nimport torch\n\ntensor, threshold = load_data()\n</code>\nresult_tensor = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult_tensor = torch.where(tensor > threshold, tensor, torch.tensor(0.0))\n</code>"
  },
  {
    "input": "Problem:  \nYou are tasked with comparing two sets of multi-dimensional tensor embeddings to identify which embeddings are similar based on a cosine similarity threshold. You have two sets of embeddings, `embeddings_a` and `embeddings_b`, both represented as 2D tensors (where each row represents an embedding). Your goal is to create a boolean mask indicating which rows in `embeddings_a` are similar to any row in `embeddings_b` based on a cosine similarity above a specified threshold (let's say `0.8`). \n\nHere's the initial setup for your code:\n\n```python\nimport torch\n\nembeddings_a = torch.rand(5, 128)  # 5 embeddings of dimension 128\nembeddings_b = torch.rand(7, 128)  # 7 embeddings of dimension 128\nthreshold = 0.8\n```\n\nYou need to fill in the blank to compute the boolean mask indicating which embeddings from `embeddings_a` are similar to those in `embeddings_b`:\n\n```python\ncosine_similarity = torch.nn.functional.cosine_similarity(embeddings_a.unsqueeze(1), embeddings_b.unsqueeze(0), dim=2)\nmask = (cosine_similarity > threshold)\n```\n\nWhat should you assign to `mask` so that it indicates whether each embedding in `embeddings_a` is similar to at least one embedding in `embeddings_b`? \n\nA:\n```python\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nmask = mask.any(dim=1)\n```"
  },
  {
    "input": "## Problem\nSuppose you have a dataset containing customer orders with relevant fields such as 'customer_id', 'order_amount', and 'order_date'. Your goal is to analyze customer spending patterns and segment the customers into two distinct groups based on their total spending using K-Means clustering. \n\nYou need to compute the total amount spent by each customer and form an appropriate feature matrix for clustering. After fitting the K-Means model, you want to predict the segment for each customer.\n\nHere is an example code snippet to get you started:\n\n```python\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\n# Example dataset\ndata = {\n    'customer_id': [1, 2, 1, 2, 3],\n    'order_amount': [100, 150, 200, 50, 300],\n    'order_date': ['2021-01-01', '2021-01-02', '2021-02-01', '2021-02-02', '2021-03-01']\n}\ndf = pd.DataFrame(data)\n\n# Group by customer_id and sum the order_amount\ncustomer_spending = df.groupby('customer_id')['order_amount'].sum().reset_index()\n\n# Prepare the feature matrix\nX = customer_spending[['order_amount']]\n\n# Fit the KMeans model\nkmeans = KMeans(n_clusters=2, n_init=10)\nkmeans.fit(X)\n\n# Predict the segment for each customer\nsegments = ...\n```\nFill in the blank to obtain the segments for each customer.",
    "output": "\n```python\nsegments = kmeans.predict(X)\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing embeddings represented as a 3D tensor, which denotes a batch of sequences. Your task is to perform a similarity computation between two sets of embeddings via cosine similarity. The embeddings of two sequences are given in the following tensors:\n\n```python\nimport torch\n\nembeddings_a = torch.randn(50, 30, 128)  # 50 sequences, 30 time steps, 128 features\nembeddings_b = torch.randn(50, 30, 128)  # Another set of embeddings\n```\n\nYou need to compute the cosine similarity between the corresponding sequences from `embeddings_a` and `embeddings_b` across the last dimension (feature dimension). The result should be a tensor of shape `[50, 30]` where each element at position `(i, j)` represents the cosine similarity between the embeddings of sequence `i` at time step `j` from both tensors.\n\nYou can use the following commands to calculate the similarities. Fill in the blank for the `similarity` variable.\n\nA:\n\n```python\nimport torch\n\nembeddings_a = torch.randn(50, 30, 128)  # 50 sequences, 30 time steps, 128 features\nembeddings_b = torch.randn(50, 30, 128)  # Another set of embeddings\n\n# Normalize the embeddings\nnorm_a = embeddings_a / embeddings_a.norm(dim=-1, keepdim=True)\nnorm_b = embeddings_b / embeddings_b.norm(dim=-1, keepdim=True)\n\nsimilarity = ...  # put solution in this variable\n```\n\nBEGIN SOLUTION\n```python\nsimilarity = torch.bmm(norm_a, norm_b.transpose(1, 2)).squeeze(-1)\n```",
    "output": "\n```python\nsimilarity = torch.bmm(norm_a, norm_b.transpose(1, 2)).squeeze(-1)\n```"
  },
  {
    "input": "Problem:\nI am working with two DataFrames, `sales_data` and `returns_data`, which contain sales and return information, respectively. Below are the structures of both DataFrames:\n\n`sales_data`\n```\n   OrderID     Date   Amount\n0        1 2023-01-01  150.00\n1        2 2023-01-01  200.00\n2        3 2023-01-02  250.00\n3        4 2023-01-03  300.00\n```\n\n`returns_data`\n```\n   OrderID     Date   Amount\n0        2 2023-01-01   50.00\n1        3 2023-01-02  100.00\n2        5 2023-01-04  200.00\n```\n\nI want to create a new DataFrame, `result`, where I merge `sales_data` with `returns_data` on `OrderID`. The `Amount` in the `returns_data` should subtract from the corresponding `Amount` in the `sales_data` where `OrderID` matches. If there is no matching `OrderID` in `returns_data`, the `Amount` from `sales_data` should remain unchanged. \n\nAdditionally, the `result` DataFrame should have the following structure, including an indicator column called `Returned` to specify if the item was returned:\n\n```\n   OrderID     Date   Amount  Returned\n0        1 2023-01-01  150.00     False\n1        2 2023-01-01  150.00      True\n2        3 2023-01-02  150.00      True\n3        4 2023-01-03  300.00     False\n```\n\nCan you help me fill in the code to perform this operation?\n\nA:\n<code>\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'OrderID': [1, 2, 3, 4],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03']),\n    'Amount': [150.00, 200.00, 250.00, 300.00]\n})\n\nreturns_data = pd.DataFrame({\n    'OrderID': [2, 3, 5],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-04']),\n    'Amount': [50.00, 100.00, 200.00]\n})\n\nresult = ... # fill in your solution here\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = sales_data.merge(returns_data, on='OrderID', how='left', suffixes=('', '_return'))\nresult['Amount'] = result['Amount'] - result['Amount_return'].fillna(0)\nresult['Returned'] = result['Amount_return'].notna()\nresult = result[['OrderID', 'Date', 'Amount', 'Returned']]\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame structured as follows:\n```\n   employee_id   department   salary   bonus\n0           1          HR    55000     NaN\n1           2          IT    75000    5000\n2           3          HR    62000    3000\n3           4          IT    82000    NaN\n4           5          HR      NaN    2000\n5           6          IT    70000    4000\n```\n\nI want to calculate the total compensation for each employee, which is the sum of their salary and bonus. However, if the salary or bonus is null, it should be treated as 0 for the calculation. After that, I would like to get the average total compensation for each department.\n\nWhat code should I use to accomplish this task?\n\nA:\n<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'employee_id': [1, 2, 3, 4, 5, 6],\n                   'department': ['HR', 'IT', 'HR', 'IT', 'HR', 'IT'],\n                   'salary': [55000, 75000, 62000, 82000, None, 70000],\n                   'bonus': [None, 5000, 3000, None, 2000, 4000]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df.fillna(0).assign(total_compensation=lambda x: x['salary'] + x['bonus']).groupby('department')['total_compensation'].mean()"
  },
  {
    "input": "Problem:\n\nIn a machine learning project, I have a tensor representing the scores of various predictions from a model, and I need to rank them to identify the top N predictions per batch sample. The tensor `scores` has a shape of (batch_size, num_predictions) and contains floating-point numbers representing the predicted scores. I want to create a new tensor that contains the indices of the top N predictions for each batch sample.\n\nHere's an example structure of the `scores` tensor:\n\n```\n[[0.1, 0.4, 0.2],\n [0.5, 0.3, 0.9],\n [0.6, 0.2, 0.8]]\n```\nIf `N=2`, the desired output should look like:\n\n```\n[[1, 2],\n [2, 0],\n [0, 2]]\n```\n\nPlease fill in the blank in the code snippet below to achieve this using PyTorch:\n\n```python\nimport torch\n\nscores = torch.tensor([[0.1, 0.4, 0.2],\n                       [0.5, 0.3, 0.9],\n                       [0.6, 0.2, 0.8]])\n\nN = 2\ntop_indices = ... # put solution in this variable\n```",
    "output": "\n```python\ntop_indices = torch.topk(scores, N, dim=1).indices\n```"
  },
  {
    "input": "Problem:\nGiven a pandas DataFrame with sales data for different products, where each product's sales across multiple quarters may contain missing values, your DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'B', 'A', 'C'],\n    'Quarter': ['Q1', 'Q1', 'Q2', 'Q2', 'Q3', 'Q3'],\n    'Sales': [100, np.nan, 200, 150, np.nan, 300]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want a new DataFrame that includes the total sales for each product, but only including quarters where sales are not null. If all sales entries for a product are null, that product's total sales should also be null.\n\nThe desired output should look like this:\n\n```\nProduct  Total_Sales\nA             300.0\nB             150.0\nC             300.0\n```\n\nFill in the blank in the following line of code to achieve this:\n\n```python\nresult = df.groupby('Product')['Sales'].apply(...).reset_index(name='Total_Sales')\n```",
    "output": "\n```python\nresult = df.groupby('Product')['Sales'].sum().reset_index(name='Total_Sales')\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame containing sales data with some missing values and I want to compute a new column that indicates whether each sale is above or below the average for that month, excluding any null sales. The DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Month': ['January', 'February', 'March', 'April', 'May', 'June'],\n    'Sales': [200, None, 150, 300, 250, None]\n}\ndf = pd.DataFrame(data)\n```\n\nI have already attempted to compute the average sales with `df['Sales'].mean()`, but I'm struggling to integrate this into a new column with the desired output as follows:\n\n```\n      Month  Sales               Status\n0   January  200.0                Above\n1  February    NaN       Not Available\n2     March  150.0             Below Average\n3     April  300.0                Above\n4       May  250.0                Above\n5      June    NaN       Not Available\n```\n\nHow can I get the correct output?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'Month': ['January', 'February', 'March', 'April', 'May', 'June'],\n    'Sales': [200, None, 150, 300, 250, None]\n}\ndf = pd.DataFrame(data)\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf['Average'] = df['Sales'].mean()\ndf['Status'] = df['Sales'].apply(lambda x: 'Not Available' if pd.isna(x) else ('Above' if x > df['Average'] else 'Below Average'))\ndf = df.drop(columns=['Average'])\n</code>"
  },
  {
    "input": "Problem:\n\nI am working with a dataset containing customer transactions, and I'd like to perform feature engineering by creating a new feature that indicates the total transaction amount for each customer. My dataset is in a pandas DataFrame called `transactions` with the following structure:\n\n| customer_id | transaction_amount |\n|-------------|--------------------|\n| 1           | 100                |\n| 2           | 200                |\n| 1           | 150                |\n| 3           | 300                |\n| 2           | 50                 |\n\nI want to group the data by `customer_id` and calculate the total transaction amount for each customer, and then save this new feature into a variable called `total_amount`.\n\nA:\n\n<code>\nimport pandas as pd\ntransactions = pd.DataFrame({\n    'customer_id': [1, 2, 1, 3, 2],\n    'transaction_amount': [100, 200, 150, 300, 50]\n})\n</code>\ntotal_amount = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ntotal_amount = transactions.groupby('customer_id')['transaction_amount'].sum().reset_index()"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame containing sales data for different products across various regions, structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'product': ['A', 'B', 'A', 'B', 'C', 'A', 'C', np.nan, 'B'],\n    'region': ['North', 'North', 'South', 'South', np.nan, 'North', 'South', 'East', 'East'],\n    'sales': [100, 150, 200, 120, 90, np.nan, 80, 60, 110]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to calculate the average sales of products grouped by region while excluding any rows with null values in either the 'product' or 'region' columns. Afterward, you need to add a new column that indicates if the average sales are above or below 100.\n\nHow can you manipulate and aggregate the DataFrame to achieve this goal?\n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'product': ['A', 'B', 'A', 'B', 'C', 'A', 'C', np.nan, 'B'],\n    'region': ['North', 'North', 'South', 'South', np.nan, 'North', 'South', 'East', 'East'],\n    'sales': [100, 150, 200, 120, 90, np.nan, 80, 60, 110]\n}\ndf = pd.DataFrame(data)\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```python\n\n",
    "output": "\n```python\nresult = df.dropna(subset=['product', 'region']).groupby(['region', 'product'])['sales'].mean().reset_index()\nresult['average_sales'] = result['sales']\nresult['sales_status'] = np.where(result['average_sales'] > 100, 'Above 100', 'Below 100')\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing information about house prices, including various features such as the number of bedrooms, size in square feet, and condition of the house. The dataset may contain missing values and categorical variables that need to be encoded before passing it to a machine learning model. The goal is to build a preprocessing pipeline using sklearn that:\n\n1. Fills missing values for numerical features with the median of the column.\n2. Encodes categorical features using one-hot encoding.\n3. Scales numerical features to have a mean of 0 and standard deviation of 1.\n\nGiven the following DataFrame:\n\n```python\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'bedrooms': [3, 4, None, 2, 5],\n    'size_sqft': [1500, 2000, 1800, None, 2500],\n    'condition': ['good', 'excellent', None, 'fair', 'good']\n})\n```\n\nHow would you implement a pipeline that achieves this?\n\nA:\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Define the numerical and categorical columns\nnumerical_cols = ['bedrooms', 'size_sqft']\ncategorical_cols = ['condition']\n\n# Create the pipeline\npreprocessing_pipeline = Pipeline(steps=[\n    ('preprocessor', ColumnTransformer(\n        transformers=[\n            ('num', Pipeline(steps=[\n                ('imputer', SimpleImputer(strategy='median')),\n                ('scaler', StandardScaler())\n            ]), numerical_cols),\n            ('cat', OneHotEncoder(), categorical_cols)\n        ]\n    ))\n])\n\n# Fit and transform the data\npreprocessed_data = preprocessing_pipeline.fit_transform(data)\n```\n\nFill in the blank for `preprocessed_data = ...` to achieve the above result. \n\n### BEGIN SOLUTION\n",
    "output": "\n```python\npreprocessed_data = preprocessing_pipeline.fit_transform(data)\n```"
  },
  {
    "input": "Problem:\nContext  \nI have daily sales data for two different stores in a city. The first DataFrame contains the sales data, including dates and amounts for Store A, and the second DataFrame contains similar data for Store B, with some duplicate rows and missing values.\n\nDataFrame A (Store A):\n```\n   Date         Sales\n0  2023-01-01   200\n1  2023-01-01   150\n2  2023-01-02   180\n3  2023-01-02   180\n4  2023-01-03   220\n```\n\nDataFrame B (Store B):\n```\n   Date         Sales\n0  2023-01-01   190\n1  2023-01-01   190\n2  2023-01-02   NaN\n3  2023-01-03   240\n4  2023-01-03   240\n```\n\nI want to concatenate these DataFrames by date, clean up the duplicates so that each date includes unique sales figures, and fill in the missing sales values for Store B with the corresponding values from Store A if they exist. The expected format should preserve the order and include a column indicating which store the sale came from.\n\nHow can I achieve this using pandas?\n```python\nimport pandas as pd\n\nstore_a = pd.DataFrame({\n    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n    'Sales': [200, 150, 180, 180, 220]\n})\n\nstore_b = pd.DataFrame({\n    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03', '2023-01-03'],\n    'Sales': [190, 190, None, 240, 240]\n})\n```\nresult = ... # put solution in this variable  \nBEGIN SOLUTION  \n<code>",
    "output": "\n```python\nstore_a_unique = store_a.drop_duplicates().groupby('Date', as_index=False).agg({'Sales': 'sum'})\nstore_b_unique = store_b.drop_duplicates().groupby('Date', as_index=False).agg({'Sales': 'sum'})\n\nstore_b_filled = store_b_unique.copy()\nstore_b_filled['Sales'] = store_b_filled['Sales'].fillna(store_a_unique.set_index('Date')['Sales'])\n\nresult = pd.concat([\n    store_a_unique.assign(Store='A'),\n    store_b_filled.assign(Store='B')\n]).sort_values(by='Date').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset containing customer transactions made through an online store. The dataset is stored as a Pandas DataFrame with the following structure:\n\n```\n  CustomerID  ProductID  Quantity  Price\n0          1         101        2   15.0\n1          1         102        1   45.0\n2          2         101        3   15.0\n3          2         103        1   20.0\n4          3         101        1   15.0\n5          3         102        2   45.0\n```\n\nTo analyze customer spending habits, you want to create a new DataFrame where each customer is represented by their total spending on each product type. The resulting DataFrame should have a multi-index with CustomerID as the first level and ProductID as the second level, and the total spending as values. \n\nThe expected output structure should look like this:\n\n```\n                     Total_Spending\nCustomerID ProductID               \n1          101           30.0\n1          102           45.0\n2          101           45.0\n2          103           20.0\n3          101           15.0\n3          102           90.0\n```\n\nTo compute this, you need to use appropriate Pandas methods to group by CustomerID and ProductID, and then calculate the total spending. Write the code that will store this result in the variable `result`.\n\n```python\nimport pandas as pd\n\ndata = {\n    'CustomerID': [1, 1, 2, 2, 3, 3],\n    'ProductID': [101, 102, 101, 103, 101, 102],\n    'Quantity': [2, 1, 3, 1, 1, 2],\n    'Price': [15.0, 45.0, 15.0, 20.0, 15.0, 45.0]\n}\n\ndf = pd.DataFrame(data)\n```\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df.groupby(['CustomerID', 'ProductID']).apply(lambda x: (x['Quantity'] * x['Price']).sum()).reset_index(name='Total_Spending').set_index(['CustomerID', 'ProductID'])\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames, sales and returns, as follows:\n\nsales:\n```\n    OrderID    OrderDate   ProductID   Quantity\n0      1001  2023-01-01       A123         10\n1      1002  2023-01-02       B234          5\n2      1003  2023-01-03       A123          3\n3      1004  2023-01-04       C345         20\n4      1005  2023-01-05       B234          2\n```\n\nreturns:\n```\n   ReturnID  ReturnDate   ProductID   Quantity\n0       R001  2023-01-02       A123          2\n1       R002  2023-01-05       B234          1\n2       R003  2023-01-06       C345          5\n```\n\nI need to create a new DataFrame combining both sales and returns, maintaining the integrity of the sales data while adjusting the quantity based on the returns. Specifically, if a product was returned, the quantity of that product in the sales DataFrame should be decreased accordingly. The resulting DataFrame should contain the original sales data with an additional column indicating the adjusted quantity.\n\nThe desired output should look like this:\n\n```\n    OrderID    OrderDate   ProductID   Quantity  Adjusted_Quantity\n0      1001  2023-01-01       A123         10                  8\n1      1002  2023-01-02       B234          5                  4\n2      1003  2023-01-03       A123          3                  3\n3      1004  2023-01-04       C345         20                 15\n4      1005  2023-01-05       B234          2                  1\n```\n\nI'm currently unsure how to adjust the quantities in the sales DataFrame according to the returns without losing data integrity. \n\nA:\n<code>\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'OrderID': [1001, 1002, 1003, 1004, 1005],\n    'OrderDate': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']),\n    'ProductID': ['A123', 'B234', 'A123', 'C345', 'B234'],\n    'Quantity': [10, 5, 3, 20, 2]\n})\n\nreturns = pd.DataFrame({\n    'ReturnID': ['R001', 'R002', 'R003'],\n    'ReturnDate': pd.to_datetime(['2023-01-02', '2023-01-05', '2023-01-06']),\n    'ProductID': ['A123', 'B234', 'C345'],\n    'Quantity': [2, 1, 5]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\n# Group returns by ProductID and sum the quantities\nreturns_grouped = returns.groupby('ProductID')['Quantity'].sum().reset_index()\n\n# Merge sales with the grouped returns\nmerged = pd.merge(sales, returns_grouped, on='ProductID', how='left', suffixes=('', '_Returned'))\n\n# Fill NaN values with 0 for returned quantities\nmerged['Quantity_Returned'] = merged['Quantity_Returned'].fillna(0)\n\n# Calculate adjusted quantity\nmerged['Adjusted_Quantity'] = merged['Quantity'] - merged['Quantity_Returned']\n\n# Select relevant columns for the final result\nresult = merged[['OrderID', 'OrderDate', 'ProductID', 'Quantity', 'Adjusted_Quantity']]\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame with sales data structured like this:\n\n| Product  | Q1   | Q2   | Q3   | Q4   |\n|----------|------|------|------|------|\n| Widget A | 150  | 200  | NaN  | 250  |\n| Widget B | 300  | 400  | 350  | NaN  |\n| Widget C | NaN  | NaN  | 600  | 700  |\n| Widget D | 100  | NaN  | 150  | NaN  |\n\nThe DataFrame has been created from a larger dataset that contains quarterly sales figures but has missing values represented by NaN. I want to calculate the total annual sales for each product while ignoring the NaN values for the quarter calculations. \n\nFurthermore, if a product has a total annual sales figure that is below 600, I want to mark that product as 'Low Performer'. Otherwise, it should be marked as 'High Performer'.\n\nThe expected output should look like this:\n\n| Product  | Total  | Performance     |\n|----------|--------|------------------|\n| Widget A | 600    | High Performer    |\n| Widget B | 1050   | High Performer    |\n| Widget C | 1300   | High Performer    |\n| Widget D | 250    | Low Performer     |\n\nHow can I achieve this using pandas? \n\nThe DataFrame creation looks like this:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Product': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n    'Q1': [150, 300, None, 100],\n    'Q2': [200, 400, None, None],\n    'Q3': [None, 350, 600, 150],\n    'Q4': [250, None, 700, None]\n})\n```\n\nPlease fill in the blanks in the following line of code to get the expected result:\n\nresult = ...\n",
    "output": "\n```python\nresult = df.assign(Total=df.iloc[:, 1:].sum(axis=1, skipna=True)).assign(Performance=lambda x: x['Total'].apply(lambda y: 'Low Performer' if y < 600 else 'High Performer'))[['Product', 'Total', 'Performance']]\n```"
  },
  {
    "input": "Problem:\nYou are working with a sales dataset in a pandas DataFrame that includes sales records for different products across multiple regions over a period of time. The DataFrame `df` is structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],\n    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n    'Sales': [150, 200, 100, 250, 300, 150, 400, 100],\n    'Date': pd.date_range(start='2023-01-01', periods=8, freq='W')\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou want to pivot this DataFrame to analyze total sales by each product and region, while simultaneously normalizing the sales values using Min-Max normalization within each region. The final DataFrame should include the total sales for each product-region combination, and an additional column for the normalized sales value.\n\nWhat would be the most efficient way to achieve this in Pandas? \n\nFill in the blank below with the appropriate code:\n\n```python\npivoted_df = df.pivot_table(index='Product', columns='Region', values='Sales', aggfunc='sum').fillna(0)\nnormalized_sales = (pivoted_df - pivoted_df.min()) / (pivoted_df.max() - pivoted_df.min())\nfinal_result = pd.concat([pivoted_df, normalized_sales.add_suffix('_Normalized')], axis=1)\n```\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n\n\n",
    "output": "\n```python\nresult = final_result\n</code>"
  },
  {
    "input": "Problem:\nYou are working with a dataset containing sales data for multiple products across different stores, structured as follows:\n\n```python\ndata = {\n    ('Store1', 'ProductA'): [100, 150, 200],\n    ('Store1', 'ProductB'): [200, 250, 300],\n    ('Store2', 'ProductA'): [150, 250, 350],\n    ('Store2', 'ProductB'): [300, 400, 500],\n}\ndf = pd.DataFrame(data, index=[2021, 2022, 2023])\n```\n\nYou must normalize the values in the DataFrame on a per-store basis, transforming them into a range between 0 and 1. Your goal is to output a new DataFrame with normalized values, keeping the multi-index structure intact. \n\nHow would you implement this normalization step?\n\nPlease provide your solution in the variable `normalized_df`.",
    "output": "\n```python\n<code>\nnormalized_df = df.groupby(level=0).apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>\n```"
  },
  {
    "input": "Problem:\nYou are given a dataset containing customer transaction records stored in a Pandas DataFrame. Each record includes a customer ID, purchase date, product category, and transaction amount. Your goal is to normalize the transaction amounts across each product category to understand how individual transactions compare within their category.\n\nThe DataFrame is structured as follows:\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': [1, 2, 1, 3, 2, 3],\n    'purchase_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06'],\n    'product_category': ['A', 'A', 'B', 'B', 'A', 'B'],\n    'transaction_amount': [100, 200, 150, 300, 250, 130]\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to calculate the normalized transaction amount for each transaction, where normalization is done using the mean and standard deviation of the transaction amounts within each product category.\n\nFill in the blank in the following code to achieve this normalization:\n```python\ndf['normalized_amount'] = (df['transaction_amount'] - df.groupby('product_category')['transaction_amount'].transform('mean')) / df.groupby('product_category')['transaction_amount'].transform('std')\n```\nHow would you write the assignment statement to correctly compute the `normalized_amount` column?\n\nA:\n<code>\ndf['normalized_amount'] = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\ndf['normalized_amount'] = (df['transaction_amount'] - df.groupby('product_category')['transaction_amount'].transform('mean')) / df.groupby('product_category')['transaction_amount'].transform('std')\n</code>"
  },
  {
    "input": "Problem:\nGiven a dataset of sales data, you have the following DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\ndata = {\n    'Store': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'Month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02'],\n    'Sales': [200, 220, 150, 180, 300, 280],\n    'Discount': [0.1, 0.15, 0.05, 0.1, 0.2, 0.15]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to calculate normalized sales for each store across the months and present the final DataFrame in a multi-index format, where the first index is the 'Store' and the second index is 'Metric', containing 'Sales' and 'Normalized Sales'. The normalized sales should be calculated by taking the z-score of sales for each store.\n\nThe expected result should look like this:\n\n```\n              Sales  Normalized Sales\nStore\nA             200               ...\nA             220               ...\nB             150               ...\nB             180               ...\nC             300               ...\nC             280               ...\n```\n\nFill in the blank for the final DataFrame preparation:\n\n```python\ndf.set_index(['Store', 'Month'])\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n```python\nresult = df.set_index(['Store', 'Month']).assign(Normalized_Sales=df.groupby('Store')['Sales'].transform(lambda x: (x - x.mean()) / x.std())).reset_index().set_index(['Store', 'Month'])\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a data science project where you want to analyze customer purchasing behavior from a dataset containing purchase amounts over time. You have a pandas DataFrame called `transactions` with two columns: `purchase_amount` and `purchase_date`. Your goal is to create a new column called `rolling_average` that calculates the rolling average of the `purchase_amount` over the last 7 days for each transaction.\n\nHere is the initial setup:\n\n```python\nimport pandas as pd\nfrom datetime import datetime\n\ndata = {\n    'purchase_amount': [100, 150, 200, 130, 170, 60, 90, 120, 180, 250],\n    'purchase_date': [\n        '2023-10-01', '2023-10-02', '2023-10-03', \n        '2023-10-05', '2023-10-06', '2023-10-07', \n        '2023-10-08', '2023-10-09', '2023-10-11', '2023-10-12'\n    ]\n}\n\ntransactions = pd.DataFrame(data)\ntransactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\ntransactions = transactions.set_index('purchase_date')\n```\n\nYou need to fill in the blank dot notation to create the `rolling_average` column using a rolling window of 7 days.\n\n```python\ntransactions['rolling_average'] = transactions['purchase_amount'].____(window='7D').mean()\n```\n\nFill in the blank with the appropriate function call.",
    "output": "\n```python\ntransactions['rolling_average'] = transactions['purchase_amount'].rolling(window='7D').mean()\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for various products over a week, including some records with missing values. The structure of the DataFrame is as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'B', 'C', 'A', 'C', 'B', 'C', 'A'],\n    'Sales': [150, np.nan, 200, 250, np.nan, 300, 350, np.nan, 400, 450],\n    'Date': pd.date_range(start='2023-01-01', periods=10)\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to compute the total sales for each product, but you need to ensure that if any sales figures for a product are missing (NaN), the total sales for that product also becomes NaN. You start with the following code for summing the sales:\n\n```python\ntotal_sales = df.groupby('Product')['Sales'].sum()\n```\n\nHowever, this does not yield the desired result with NaN handling. How can you modify your code to get the correct total sales that meet this requirement?\n\nHere is what I want to achieve:\n\n```\nProduct\nA      NaN\nB      NaN\nC      NaN\nName: Sales, dtype: float64\n```\n\nFill in the blank with the correct code:\n```python\ntotal_sales = ...\n```",
    "output": "\n```python\ntotal_sales = df.groupby('Product')['Sales'].apply(lambda x: x.sum() if x.isnull().sum() == 0 else np.nan)\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame containing sales data with some missing values. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'product': ['A', 'B', 'A', 'C', 'B', 'C'],\n    'sales': [200, np.nan, 150, 300, np.nan, 500],\n    'region': ['North', 'South', 'North', 'East', 'South', 'East']\n}\ndf = pd.DataFrame(data)\n```\n\nI want to calculate the total sales for each product, but if any sales value is missing (NaN), the total sales for that product should also be reported as NaN. \n\nI\u2019m trying to group by 'product' and sum the 'sales' column, but I cannot get the NaN handling to reflect properly. Here's what I initially attempted:\n\n```python\ndf.groupby('product')['sales'].sum()\n```\n\nWhat should I include to ensure that the presence of NaN values leads to a NaN result for that group's total sales?\n\nA:\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby('product')['sales'].sum(min_count=1)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames with records of sales transactions. The first DataFrame (`sales`) contains the following information:\n\n```python\nimport pandas as pd\n\nsales_data = {\n    'transaction_id': [1001, 1002, 1003, 1004, 1005],\n    'product_id': [501, 502, 503, 501, 504],\n    'quantity': [2, 3, 1, 1, 4],\n    'date': ['2022-01-01', '2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02']\n}\nsales = pd.DataFrame(sales_data)\n```\n\nThe second DataFrame (`products`) contains information about the products:\n\n```python\nproducts_data = {\n    'product_id': [501, 502, 503, 504],\n    'product_name': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n    'category': ['Gadgets', 'Gadgets', 'Gizmos', 'Gizmos']\n}\nproducts = pd.DataFrame(products_data)\n```\n\nI need to create a combined DataFrame that merges `sales` with `products` based on `product_id`. The final DataFrame should include `transaction_id`, `product_name`, `quantity`, and `date`. Also, entries with the same `transaction_id` should be sorted by `date` in ascending order.\n\nThe combined DataFrame should look like this:\n\n```\n   transaction_id product_name  quantity        date\n0             1001     Widget A         2  2022-01-01\n1             1002     Widget B         3  2022-01-01\n2             1003     Widget C         1  2022-01-01\n3             1004     Widget A         1  2022-01-02\n4             1005     Widget D         4  2022-01-02\n```\n\nI have initially attempted to merge the DataFrames as follows:\n\n```python\ncombined = pd.merge(sales, products, on='product_id')\n```\n\nHowever, I need the final DataFrame to reflect the specified columns and sorting criteria. \n\nWhat code should I use to achieve that?\n```python\nresult = ...  # put solution in this variable\n```",
    "output": "\n```python\nresult = combined[['transaction_id', 'product_name', 'quantity', 'date']].sort_values(by=['date', 'transaction_id']).reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a project that involves processing a set of multi-dimensional feature vectors using PyTorch. Each feature vector is structured as a tensor of shape (batch_size, channels, height, width). Your task is to implement a method that applies a specific boolean mask tensor to the feature tensor to zero out certain elements conditionally, based on whether they meet a threshold.\n\nYou have the following tensors:\n\n- `features`: a tensor of shape (100, 3, 32, 32) representing a batch of 100 images with 3 channels and 32x32 pixels.\n- `mask`: a boolean tensor of the same shape, indicating which elements should be kept (True) or zeroed out (False).\n\nThe output should be a tensor of the same shape where elements of `features` are multiplied by the corresponding elements of `mask`. \n\nA:\n\n<code>\nimport numpy as np\nimport torch\nfeatures, mask = load_data()\n</code>\noutput = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\noutput = features * mask.float()\n</code>"
  },
  {
    "input": "Problem:\nYou are tasked with normalizing a dataset that contains multiple features with varying ranges. You want to apply Min-Max scaling to transform all the features into the range [0, 1]. The dataset is provided as a Pandas DataFrame. After scaling, you want to add the scaled features back into the DataFrame, keeping the original features intact. Given the DataFrame `df`, how do you achieve this with the MinMaxScaler from sklearn?\n\nA:\n<code>\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = {\n    'feature1': [10, 20, 30, 40],\n    'feature2': [200, 300, 400, 500],\n    'feature3': [5, 10, 15, 20]\n}\ndf = pd.DataFrame(data)\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(df)\n\n</code>\ndf_scaled = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf_scaled = pd.DataFrame(scaled_features, columns=df.columns)"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame containing sales data for different products across several months. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'product': ['A', 'A', 'B', 'B', 'C', 'C', 'C'],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02', '2023-03'],\n    'sales': [200, np.nan, 150, 170, 300, np.nan, 250],\n    'returns': [5, 3, 0, np.nan, 10, 5, np.nan]\n})\n```\n\nThis DataFrame contains some null values in the 'sales' and 'returns' columns, and I would like to calculate the net sales for each product by subtracting the total returns from the total sales for each product, treating any NaN values as zero. The expected output should be a DataFrame similar to:\n\n```\n  product  net_sales\n0       A      392.0\n1       B      320.0\n2       C      300.0\n```\n\nHow can I achieve this? \n\n```python\ndf = ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\ndf = df.groupby('product').agg(net_sales=('sales', lambda x: x.sum(skipna=True) - df.loc[x.index, 'returns'].sum(skipna=True)).fillna(0)).reset_index()\n```"
  },
  {
    "input": "### Problem:\nContext  \nI have two datasets with information about user transactions and their details. The first dataset contains transactions with timestamps, while the second contains transaction details that I want to merge based on the timestamp. However, some transactions in the first dataset might not have a corresponding entry in the second dataset.\n\nDataset1 (transactions):\n```\n    UserID       Timestamp         Amount\n0      1     2023-10-01 12:00  100\n1      2     2023-10-01 12:05  150\n2      1     2023-10-01 12:10  200\n3      3     2023-10-01 12:15  300\n```\n\nDataset2 (transaction_details):\n```\n    TransactionID     Timestamp          Status\n0             A1  2023-10-01 12:00        Success\n1             A2  2023-10-01 12:05        Failed\n2             A3  2023-10-01 12:10        Success\n```\n\nI want to merge these datasets such that I keep all the transactions from Dataset1, and where possible, include the corresponding Status from Dataset2 based on the Timestamp. The final DataFrame should also sort the entries by Timestamp.\n\nThe expected result should look like this:\n```\n    UserID       Timestamp         Amount TransactionID      Status\n0      1     2023-10-01 12:00  100       A1            Success\n1      2     2023-10-01 12:05  150       A2             Failed\n2      1     2023-10-01 12:10  200       A3            Success\n3      3     2023-10-01 12:15  300         NaN          NaN\n```\n\nHow can I achieve this using Pandas? Fill in the blank with your code solution below:\n\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'UserID': [1, 2, 1, 3],\n    'Timestamp': pd.to_datetime(['2023-10-01 12:00', '2023-10-01 12:05', '2023-10-01 12:10', '2023-10-01 12:15']),\n    'Amount': [100, 150, 200, 300]\n})\n\ndf2 = pd.DataFrame({\n    'TransactionID': ['A1', 'A2', 'A3'],\n    'Timestamp': pd.to_datetime(['2023-10-01 12:00', '2023-10-01 12:05', '2023-10-01 12:10']),\n    'Status': ['Success', 'Failed', 'Success']\n})\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2, on='Timestamp', how='left').sort_values(by='Timestamp')\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames representing sales records for two months:\n\ndf_january:\n```\n   id  product quantity    date  revenue\n0   1  apples       30  2023-01-01       300\n1   2  bananas      50  2023-01-01       200\n2   3  cherries     20  2023-01-01       400\n```\n\ndf_february:\n```\n   id  product quantity    date  revenue\n0   3  cherries     15  2023-02-01       300\n1   4  dates        10  2023-02-01       150\n2   5  elderberries 25  2023-02-01       600\n```\n\nAfter merging these two DataFrames on the 'id' column, you want to ensure that for products sold in January but not in February, the quantity and revenue reflect the January values. For products sold in February that weren't in January, you want to keep the February values. Additionally, any duplicate entries (same id for January and February) should show the February 'quantity' and 'revenue', while aligning the dates properly in 'dd-MMM-yyyy' format.\n\nThe expected merged result should look like this:\n```\n   id       product quantity    date      revenue\n0   1       apples      30  01-Jan-2023       300\n1   2      bananas      50  01-Jan-2023       200\n2   3     cherries      15  01-Feb-2023       300\n3   4         dates      10  01-Feb-2023       150\n4   5 elderberries      25  01-Feb-2023       600\n```\n\nYou initially concatenated the two DataFrames, resulting in duplicates and misaligned dates due to direct concatenation. How can you do this correctly? \n\nFill in the code below to achieve this:\n```\nimport pandas as pd\n\ndf_january = pd.DataFrame({'id': [1, 2, 3],\n                            'product': ['apples', 'bananas', 'cherries'],\n                            'quantity': [30, 50, 20],\n                            'date': ['2023-01-01', '2023-01-01', '2023-01-01'],\n                            'revenue': [300, 200, 400]})\n\ndf_february = pd.DataFrame({'id': [3, 4, 5],\n                             'product': ['cherries', 'dates', 'elderberries'],\n                             'quantity': [15, 10, 25],\n                             'date': ['2023-02-01', '2023-02-01', '2023-02-01'],\n                             'revenue': [300, 150, 600]})\n\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```\nresult = pd.merge(df_january, df_february, on='id', how='outer', suffixes=('_jan', '_feb'))\nresult['quantity'] = result['quantity_feb'].combine_first(result['quantity_jan'])\nresult['revenue'] = result['revenue_feb'].combine_first(result['revenue_jan'])\nresult['date'] = result['date_feb'].combine_first(result['date_jan'])\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result[['id', 'product_feb', 'quantity', 'date', 'revenue']]\nresult.rename(columns={'product_feb': 'product'}, inplace=True)\n```"
  },
  {
    "input": "Problem:\nI have a dataset encapsulated in a pandas DataFrame that contains user activity logs. The DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'user_id': [1, 2, 1, 3, 2, 3, 1, None],\n    'activity': ['login', 'logout', 'logout', 'login', 'login', 'logout', 'logout', 'login'],\n    'duration': [20, None, 15, 25, 30, 10, None, 5]\n}\ndf = pd.DataFrame(data)\n```\n\nI want to calculate the total time spent by each user based on their 'login' and 'logout' activities. However, the calculation should only include rows where 'user_id' is not null, and for 'logout' activities, if the 'duration' is also not null.\n\nMy desired output should have the following format:\n\n```\nuser_id  total_time\n1        (sum of durations for user_id 1 where activity is 'logout')\n2        (sum of durations for user_id 2 where activity is 'logout')\n3        (sum of durations for user_id 3 where activity is 'logout')\n```\n\nWhile I understand how to group and sum, I'm not sure how to include the conditions and manage null values effectively with a custom function. What code can I use to achieve this?\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df[df['user_id'].notnull() & (df['activity'] == 'logout') & df['duration'].notnull()] \\\n    .groupby('user_id', as_index=False)['duration'].sum() \\\n    .rename(columns={'duration': 'total_time'})\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset represented as a 3D Tensor in PyTorch with dimensions (batch_size, channels, height). I want to compute a similarity score between each pair of channels within each batch. Specifically, I want to calculate the cosine similarity of each channel with every other channel. The output should be a 2D Tensor containing the similarity scores for each pair of channels per batch.\n\nHere is what I have so far:\n\n```python\nimport torch\n\n# Assume the following is our input tensor\ninput_tensor = torch.randn(4, 3, 5)  # 4 batches, 3 channels, height 5\n```\n\nWhat should I put in place of `...` to compute the cosine similarity across the channels?\n\nA:\n\n```python\nsimilarity_scores = ...  # put solution in this variable\n```",
    "output": "\n```python\nsimilarity_scores = torch.nn.functional.cosine_similarity(input_tensor.unsqueeze(2), input_tensor.unsqueeze(1), dim=3)\n```"
  },
  {
    "input": "Problem:\n\nI have a tensor `a` that holds some values and I want to apply a thresholding operation to it where values above a certain threshold are retained as-is, and values below that threshold should be set to zero. I am also required to keep track of how many values were zeroed out during this operation for further analysis. Given the tensor `a`, here is what I have so far:\n\n```python\nimport torch\na = torch.tensor([[0.5, 1.2, 3.0], [4.4, 0.1, 2.2]])\n\nthreshold = 1.0\nzeroed_out_count = 0\n# Your code should go here\n```\n\nA:\n\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nzeroed_out_count = (a < threshold).sum().item()\nresult = torch.where(a < threshold, torch.tensor(0.0), a)\n</code>"
  },
  {
    "input": "Problem:\nI have two pandas DataFrames representing sales data from two different regions, each containing a timestamp for when the data was recorded. The first DataFrame `sales_a` contains sales data from Region A, and the second DataFrame `sales_b` contains sales data from Region B. I want to merge these DataFrames such that for each timestamp in `sales_b`, I include the total sales from `sales_a` that occurred within a 5-minute window before that timestamp. If no sales are found in that time window, I want to record a `None` in the sales column.\n\nHere is the setup of the two DataFrames:\n\n```python\nimport pandas as pd\n\nsales_a = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:00:00', '2023-10-01 10:02:00', '2023-10-01 10:05:00', '2023-10-01 10:10:00'],\n    'TotalSales': [100, 200, 150, 300]\n})\n\nsales_b = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:01:00', '2023-10-01 10:06:00', '2023-10-01 10:11:00']\n})\n```\n\nNow, after merging, I want the resulting DataFrame to look like this:\n\n```\n+---------------------+------------+\n|      Timestamp      | TotalSales |\n+---------------------+------------+\n| 2023-10-01 10:01:00 |     100    |\n| 2023-10-01 10:06:00 |     200    |\n| 2023-10-01 10:11:00 |     150    |\n+---------------------+------------+\n```\n\nIf no sales were recorded in the 5-minute window, `TotalSales` should be `None`.\n\nTo achieve this, write the code that fills in the blank for the variable `result` below, ensuring that the merged DataFrame keeps the order of `sales_b` and aligns the sales data from `sales_a` correctly.\n\n```python\nimport pandas as pd\n\nsales_a = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:00:00', '2023-10-01 10:02:00', '2023-10-01 10:05:00', '2023-10-01 10:10:00'],\n    'TotalSales': [100, 200, 150, 300]\n})\n\nsales_b = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:01:00', '2023-10-01 10:06:00', '2023-10-01 10:11:00']\n})\n\n# Convert Timestamp columns to datetime\nsales_a['Timestamp'] = pd.to_datetime(sales_a['Timestamp'])\nsales_b['Timestamp'] = pd.to_datetime(sales_b['Timestamp'])\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = sales_b.copy()\nresult['TotalSales'] = result['Timestamp'].apply(\n    lambda x: sales_a[sales_a['Timestamp'] <= x].loc[\n        sales_a['Timestamp'] > (x - pd.Timedelta(minutes=5)), 'TotalSales'\n    ].sum() or None\n)\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing various attributes of houses, including square footage, number of bedrooms, and sale price. You want to preprocess the data by normalizing the continuous variables (square footage and sale price) using `StandardScaler` from `sklearn.preprocessing`. The standardized data should then be combined with the categorical variable (number of bedrooms) encoded as one-hot vectors. The final output should be a NumPy array containing the combined data.\n\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Simulated dataset\ndata = pd.DataFrame({\n    'square_footage': [1500, 2400, 3000, 1800],\n    'bedrooms': [3, 4, 5, 3],\n    'sale_price': [300000, 450000, 600000, 350000]\n})\n\n# Begin preprocessing\nscaler = StandardScaler()\nencoder = OneHotEncoder(sparse=False)\n\n# Normalize continuous variables\nnormalized_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>normalized_data = np.hstack((scaler.fit_transform(data[['square_footage', 'sale_price']]),\nencoder.fit_transform(data[['bedrooms']])))\n</code>",
    "output": "\n<code>normalized_data = np.hstack((scaler.fit_transform(data[['square_footage', 'sale_price']]),\nencoder.fit_transform(data[['bedrooms']])))\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for a company over a period of time. The DataFrame includes the columns 'Date', 'Region', 'Sales', and 'Returns'. Some sales entries are missing (NaN) and should be considered zero for calculations. You want to compute the net sales for each region, which is defined as the sum of sales minus the sum of returns. Additionally, you want to create a new column that categorizes the net sales as 'High', 'Medium', or 'Low' based on the following thresholds: High if net sales are above 1000, Medium if between 500 and 1000, and Low if below 500.\n\nGiven the DataFrame as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Date': ['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02'],\n    'Region': ['North', 'South', 'North', 'South'],\n    'Sales': [np.nan, 200, 300, 400],\n    'Returns': [50, np.nan, 20, 30]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to complete the following line of code that will produce a new DataFrame (`result`) which summarizes the net sales and their categories:\n\n```python\nresult = df.groupby('Region').apply(lambda x: pd.Series({\n    'Net_Sales': (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()),\n    'Sales_Category': 'High' if ... else 'Medium' if ... else 'Low'\n}))\n```\n\nFill in the blanks to correctly define the net sales calculations and category assignments.",
    "output": "\n```python\nresult = df.groupby('Region').apply(lambda x: pd.Series({\n    'Net_Sales': (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()),\n    'Sales_Category': 'High' if (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()) > 1000 else 'Medium' if (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()) >= 500 else 'Low'\n}))\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames containing sales data. The first DataFrame `df_sales` looks like this:\n\n```\n    StoreID      Date  Sales\n0      S001 2023-01-01    500\n1      S002 2023-01-01    300\n2      S001 2023-01-02    550\n3      S002 2023-01-02    600\n```\n\nThe second DataFrame `df_returns` is structured as follows:\n\n```\n    StoreID      Date  Returns\n0      S001 2023-01-02    50\n1      S002 2023-01-01    30\n2      S002 2023-01-02    70\n3      S001 2023-01-03    20\n```\n\nI want to create a new DataFrame that contains all the sales data along with the returns for each store on the appropriate dates. If there are no returns for a given day, that value should be filled with 0. The result should look like this:\n\n```\n    StoreID      Date  Sales  Returns\n0      S001 2023-01-01    500        0\n1      S002 2023-01-01    300       30\n2      S001 2023-01-02    550       50\n3      S002 2023-01-02    600       70\n4      S001 2023-01-03    0       20\n```\n\nHow do I accomplish this merging while ensuring that any missing return values are replaced with zero? \n\nA:\n<code>\nimport pandas as pd\n\n\ndf_sales = pd.DataFrame({\n    'StoreID': ['S001', 'S002', 'S001', 'S002'],\n    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'Sales': [500, 300, 550, 600]\n})\n\ndf_returns = pd.DataFrame({\n    'StoreID': ['S001', 'S002', 'S002', 'S001'],\n    'Date': ['2023-01-02', '2023-01-01', '2023-01-02', '2023-01-03'],\n    'Returns': [50, 30, 70, 20]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.merge(df_sales, df_returns, on=['StoreID', 'Date'], how='left').fillna(0)\nresult['Returns'] = result['Returns'].astype(int)\nresult = result.sort_values(by=['StoreID', 'Date']).reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\n\nI am working with a PyTorch model that takes a tensor of shape (batch_size, sequence_length, feature_dimension) as input. I need to compute the cosine similarity between the first and second entries across the batch, and I want to return a tensor of shape (batch_size, 1) that contains the cosine similarity for each pair.\n\nThe formula for cosine similarity is given by:\n\\[ \\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]\n\nFor instance, if my tensor looks like this:\n```python\ninput_tensor = torch.tensor([\n    [[1, 2, 3], [4, 5, 6]],\n    [[7, 8, 9], [1, 0, 0]]\n])\n```\nThen I want a tensor that contains the cosine similarity of the first and second elements of the sequences from each batch.\n\nHow can I achieve this in PyTorch?\n\nA:\n<code>\nimport torch\n\ninput_tensor = torch.tensor([\n    [[1, 2, 3], [4, 5, 6]],\n    [[7, 8, 9], [1, 0, 0]]\n])\n\ndef compute_cosine_similarity(tensor):\n    # return the cosine similarity in this function\n    # similarity = compute_cosine_similarity(tensor)\n    ### BEGIN SOLUTION\n",
    "output": "\n<code>\n    A = tensor[:, 0, :]  # First entry across the batch\n    B = tensor[:, 1, :]  # Second entry across the batch\n\n    dot_product = (A * B).sum(dim=1)  # Compute dot product\n    norm_A = A.norm(dim=1)  # Compute norm of A\n    norm_B = B.norm(dim=1)  # Compute norm of B\n\n    cosine_similarity = dot_product / (norm_A * norm_B)  # Compute cosine similarity\n    return cosine_similarity.unsqueeze(1)  # Return shape (batch_size, 1)\n\nsimilarity = compute_cosine_similarity(input_tensor)\n</code>"
  },
  {
    "input": "## Problem\nYou have a dataset containing sales information from a retail company stored in a pandas DataFrame. The DataFrame includes information on the `store_id`, `category`, and `sales` for each transaction, like so:\n\n```python\nimport pandas as pd\n\ndata = {\n    'store_id': ['store_1', 'store_1', 'store_2', 'store_2', 'store_1', 'store_3'],\n    'category': ['A', 'B', 'A', 'B', 'A', 'C'],\n    'sales': [100, 150, 200, 300, 250, 400]\n}\ndf = pd.DataFrame(data)\n```\n\nNow, you want to normalize the `sales` values within each `store_id` across `category`, turning the `sales` values into their corresponding z-scores. Finally, you would like the DataFrame structured such that it retains the `store_id` and `category`, while also displaying a new row for the z-scores below the original sales data. The resulting DataFrame should look like:\n\n```\n          store_id category  sales\n0         store_1       A    100\n1         store_1       B    150\n2         store_2       A    200\n3         store_2       B    300\n4         store_1       A    250\n5         store_3       C    400\n          zscore\n0         ...   \n1         ...\n2         ...\n3         ...\n4         ...\n5         ...\n```\n\nWrite the code to generate the `result` variable where you will perform the normalization and structure the DataFrame as described. \n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nfrom scipy.stats import zscore\n\ndf['zscore'] = df.groupby('store_id')['sales'].transform(zscore)\nresult = pd.concat([df, df[['store_id', 'category', 'zscore']].rename(columns={'zscore': 'sales'})], ignore_index=True)\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data with multi-level indexing for products sold across different regions and quarters of the year. Here\u2019s a sample of the data:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples(\n    [('North', 'Q1'), ('North', 'Q2'), ('North', 'Q3'), ('North', 'Q4'),\n     ('South', 'Q1'), ('South', 'Q2'), ('South', 'Q3'), ('South', 'Q4')],\n    names=['Region', 'Quarter']\n)\ndata = {\n    'Sales': [25000, 27000, 24000, 30000, 15000, 18000, 16000, 20000],\n    'Profit': [5000, 6000, 4000, 7000, 3000, 3500, 2500, 4000]\n}\ndf = pd.DataFrame(data, index=index)\n```\n\nYou want to normalize the \"Sales\" column for each region while keeping the DataFrame structure intact. The final DataFrame should keep the multi-index but include the normalized sales values alongside the existing data.\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.copy()\nresult['Normalized_Sales'] = df.groupby(level='Region')['Sales'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset of image embeddings represented as a tensor using PyTorch. The tensor `embeddings` has a shape of (100, 128), where 100 is the number of images and 128 represents the feature dimensions of those embeddings. I want to compute a similarity score between these embeddings and a query embedding vector `query_embedding` of shape (128). Specifically, I want to calculate the cosine similarity for each embedding with respect to the query_embedding and return the top 5 most similar embeddings.\n\nHere is the code snippet I have started with:\n\n```python\nimport torch\n\nembeddings = torch.randn(100, 128)  # Random tensor to represent image embeddings\nquery_embedding = torch.randn(128)   # Random tensor for the query embedding\n\n# Your task is to compute the cosine similarity and get the indices of the top 5 most similar embeddings.\nsimilarity_scores = ...  # Fill in this line\n\n# Get the indices of the top 5 most similar embeddings\ntop_5_indices = torch.topk(similarity_scores, 5).indices\n```\n\nWhat should I put in place of the ellipsis (...)?",
    "output": "\n```python\nsimilarity_scores = torch.nn.functional.cosine_similarity(embeddings, query_embedding.unsqueeze(0), dim=1)\n```"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame that contains sales data structured like this:\n```\n   region       sales      \n0  North   2000.00          \n1  South   1500.00          \n2  East    3000.00          \n3  West    NaN              \n4  North   2500.00          \n5  South   NaN              \n6  East    3200.00          \n```\nThis is just an example; the actual DataFrame is larger but follows the same schema. The sample DataFrame can be created using the following lines of code:\n```python\nimport pandas as pd\ndf = pd.DataFrame({\n    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East'],\n    'sales': [2000.00, 1500.00, 3000.00, None, 2500.00, None, 3200.00]\n})\n```\n\nI would like to calculate the average sales for each region, but I want to handle the null values by ignoring them in the calculations. Furthermore, I would like to replace any region with less than two valid sales entries with 'Insufficient Data'. \n\nHow can I achieve that? \nI know that I can use groupby() to get averages, but I'm not sure how to handle the conditional replacements afterward.\n\nA:\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby('region')['sales'].mean().reset_index()\nresult['sales'] = result['sales'].where(result['sales'].notnull(), 'Insufficient Data')\nresult = result[result['sales'] != 'Insufficient Data'].groupby('region').filter(lambda x: len(x) >= 2)\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing information about different products in a retail store. Each product has several attributes like category, price, and sales. You want to create a MultiIndex DataFrame using the Pandas library, where the first level of the index is the product category and the second level is the product name. You also want to normalize the sales data for each product within its category to ensure comparability.\n\nAssume you have a dictionary of data as follows:\n```python\ndata = {\n    'category': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Grocery'],\n    'product': ['Smartphone', 'Laptop', 'Sofa', 'Table', 'Rice'],\n    'price': [699.99, 1299.99, 499.99, 199.99, 29.99],\n    'sales': [150, 80, 20, 50, 200]\n}\n```\n\nYour task is to create a MultiIndex DataFrame from this dictionary, normalize the 'sales' column per category, and then return the resulting DataFrame. \n\nThe expected structure should look like this, where sales are normalized within their respective categories:\n```\n                     price  sales\ncategory    product             \nElectronics Smartphone  699.99  1.000\n            Laptop     1299.99  0.533\nFurniture   Sofa        499.99  0.400\n            Table      199.99  1.000\nGrocery     Rice        29.99  1.000\n```\n\nHow can you achieve this in Python?\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'category': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Grocery'],\n    'product': ['Smartphone', 'Laptop', 'Sofa', 'Table', 'Rice'],\n    'price': [699.99, 1299.99, 499.99, 199.99, 29.99],\n    'sales': [150, 80, 20, 50, 200]\n}\n\ndf = pd.DataFrame(data)\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df.set_index(['category', 'product']).groupby(level=0).apply(lambda x: x.assign(sales=x['sales'] / x['sales'].max()))</code>"
  },
  {
    "input": "Problem:\n\nYou are working on a recommendation system using PyTorch. You have a user-item interaction tensor `interactions` of shape (n_users, n_items) where each element represents a score indicating the user's preference for a particular item. You want to filter this tensor to get only the scores of items that have been positively rated (greater than zero). After filtering, you want to compute the mean score per user from the remaining items. \n\nHere\u2019s the code stub you have:\n\n```python\nimport torch\n\n# Sample tensor representing user-item interactions\ninteractions = torch.tensor([[5.0, 0.0, 3.0],\n                             [0.0, 2.0, 0.0],\n                             [4.0, 0.0, 1.0]])\n\n# Create a mask for positive ratings\nmask = interactions > 0\n\n# Filter the interactions tensor\nfiltered_interactions = interactions[mask] # This line needs correction\n\n# Compute the mean score per user\nmean_scores = ...\n\n```\n\nFill in the blank in the `filtered_interactions` assignment to correctly apply the mask. The result should maintain the original structure of filtering while considering the user-item relations.",
    "output": "\n```python\nfiltered_interactions = interactions * mask\nmean_scores = filtered_interactions.sum(dim=1) / mask.sum(dim=1)\n```"
  },
  {
    "input": "Problem:\n\nI'm working with a PyTorch model that outputs a tensor of shape (batch_size, features). I want to find the cosine similarity between each pair of feature vectors within the same batch. However, I need to ignore any features that correspond to a masked value that I've defined. The mask is a boolean tensor of the same shape, where `True` indicates valid entries to be considered in the similarity computation, and `False` indicates masked values.\n\nGiven a tensor `X` and its corresponding boolean mask `mask`, how can I compute the cosine similarity for the valid entries while ignoring the masked values? The resulting tensor should have a shape of (batch_size, batch_size) representing the cosine similarity between valid feature vectors.\n\nExample input:\n```python\nX = torch.tensor([[1.0, 0.0, 2.0],\n                  [2.0, 3.0, 1.0],\n                  [0.0, 3.0, 0.0]])\nmask = torch.tensor([[True, False, True],\n                     [True, True, True],\n                     [False, True, False]])\n```\n\nA:\n\n<code>\nimport torch\n\nX = torch.tensor([[1.0, 0.0, 2.0],\n                  [2.0, 3.0, 1.0],\n                  [0.0, 3.0, 0.0]])\nmask = torch.tensor([[True, False, True],\n                     [True, True, True],\n                     [False, True, False]])\n</code>\nsimilarity = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nimport torch.nn.functional as F\n\ndef masked_cosine_similarity(X, mask):\n    # Apply the mask to X\n    masked_X = X * mask.float()\n    \n    # Normalize the masked feature vectors\n    norms = torch.norm(masked_X, dim=1, keepdim=True)\n    normalized_X = masked_X / norms\n    \n    # Compute cosine similarity\n    similarity = torch.mm(normalized_X, normalized_X.t())\n    \n    # Create a mask for the similarity matrix\n    similarity_mask = mask.float() @ mask.float().t()\n    \n    # Zero out the masked entries in the similarity matrix\n    similarity *= similarity_mask\n    \n    return similarity\n\nsimilarity = masked_cosine_similarity(X, mask)\n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset containing customer information with various features such as age, income, and purchase history. Some of these features have missing values, and I want to preprocess them before feeding them into a machine learning model. Specifically, I need to apply the following transformations:\n\n1. Fill missing values in the 'age' column with the median age.\n2. Normalize the 'income' column using Min-Max scaling.\n3. One-hot encode the 'purchase_history' categorical column.\n\nHow can I achieve this using Pandas and Scikit-Learn?\n\nA:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = load_data()\n```\npipeline = ...  # put solution in this variable\nBEGIN SOLUTION\n```python\n\n",
    "output": "\n```python\npipeline = Pipeline(steps=[\n    ('preprocessor', ColumnTransformer(\n        transformers=[\n            ('age_imputer', 'passthrough', ['age']),\n            ('income_scaler', MinMaxScaler(), ['income']),\n            ('purchase_encoder', OneHotEncoder(), ['purchase_history'])\n        ],\n        remainder='drop'\n    )),\n    ('age_median_imputer', SimpleImputer(strategy='median'))\n])\n```"
  },
  {
    "input": "## Problem\n\nI am working with a dataset that contains various features about houses, including their prices. I want to preprocess the data to better prepare it for a machine learning model. Specifically, I need to handle missing values in the 'price' column, standardize the 'size' column, and encode the categorical 'location' column.\n\nI have the following code snippet to start with:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Load dataset\ndf = pd.read_csv(\"houses.csv\")\n\n# Define columns\nnumeric_features = ['size']\ncategorical_features = ['location']\ntarget_column = 'price'\n\n# Imputer for numerical data\nnum_imputer = SimpleImputer(strategy='mean')\n# Encoder for categorical data\ncat_encoder = OneHotEncoder()\n\n# Create a preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_imputer, numeric_features),\n        ('cat', cat_encoder, categorical_features)\n    ]\n)\n\n# Apply preprocessing\nX = df[numeric_features + categorical_features]\ny = df[target_column]  # target variable\n\n# I need to fit and transform the features using preprocessor and also use StandardScaler on 'size'.\nX_transformed = ... # put solution in this variable\n\n```\nWhat code should I place where the comment indicates to preprocess `X` correctly?\n\nA:\n```python\nX_transformed = ... # put solution in this variable\n```",
    "output": "\n```python\nX_transformed = preprocessor.fit_transform(X)\nX_transformed[:, 0] = StandardScaler().fit_transform(X_transformed[:, 0].reshape(-1, 1)).flatten()\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame consisting of user transaction data that looks like this:\n\n| user_id | transaction_date | amount  | status  |\n|---------|------------------|---------|---------|\n| 1       | 2023-01-01       | 100.00  | Success |\n| 1       | 2023-01-02       | 50.00   | Failed  |\n| 2       | 2023-01-01       | 200.00  | Success |\n| 2       | 2023-01-03       | NaN     | Success |\n| 1       | 2023-01-04       | NaN     | Failed  |\n\nI want to compute the total successful transaction amount for each user while ensuring that any failed transactions or null values in the amount column do not affect the calculations. I also want to create a new column that provides a summary of the status of each user, where the user is classified as 'Active' if they have a successful transaction and 'Inactive' otherwise.\n\nMy code starts like this:\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'user_id': [1, 1, 2, 2, 1],\n    'transaction_date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-04'],\n    'amount': [100.0, 50.0, 200.0, np.nan, np.nan],\n    'status': ['Success', 'Failed', 'Success', 'Success', 'Failed']\n}\n\ndf = pd.DataFrame(data)\n\nI have already grouped the DataFrame by user_id and am using apply but couldn't finalize the summary and aggregation.\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df.groupby('user_id').agg(\n    total_successful_amount=('amount', lambda x: x[df['status'] == 'Success'].sum()),\n    status_summary=('status', lambda x: 'Active' if 'Success' in x.values else 'Inactive')\n).reset_index()\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames representing sales data from different sources as follows:\n\nsales_df1:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:00:00 | Widget A |   10     |\n |  2     | 2023-01-01 11:00:00 | Widget B |   15     |\n |  3     | 2023-01-01 12:00:00 | Widget A |   5      |\n +--------+----------+----------+----------+\n```\n\nsales_df2:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:05:00 | Widget A |   7      |\n |  3     | 2023-01-01 12:05:00 | Widget A |   3      |\n |  4     | 2023-01-01 12:10:00 | Widget C |   8      |\n +--------+----------+----------+----------+\n```\n\nYour task is to merge these two DataFrames based on the `Product` column and ensure that in the final DataFrame, you include all the original records. If there are overlapping timestamps for the same product, sum the quantities from both DataFrames. Finally, sort the resulting DataFrame by `Timestamp`.\n\nAfter merging them efficiently, the resulting DataFrame should look like this:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:00:00 | Widget A |   17     |\n |  2     | 2023-01-01 11:00:00 | Widget B |   15     |\n |  3     | 2023-01-01 12:00:00 | Widget A |   8      |\n |  4     | 2023-01-01 12:10:00 | Widget C |   8      |\n +--------+----------+----------+----------+\n```\n\nUse the following placeholders in your solution:\n```python\nimport pandas as pd\n\nsales_df1 = pd.DataFrame({\n    'OrderID': [1, 2, 3],\n    'Timestamp': pd.to_datetime(['2023-01-01 10:00:00', '2023-01-01 11:00:00', '2023-01-01 12:00:00']),\n    'Product': ['Widget A', 'Widget B', 'Widget A'],\n    'Quantity': [10, 15, 5]\n})\n\nsales_df2 = pd.DataFrame({\n    'OrderID': [1, 3, 4],\n    'Timestamp': pd.to_datetime(['2023-01-01 10:05:00', '2023-01-01 12:05:00', '2023-01-01 12:10:00']),\n    'Product': ['Widget A', 'Widget A', 'Widget C'],\n    'Quantity': [7, 3, 8]\n})\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = pd.concat([sales_df1, sales_df2]).groupby(['Product', 'Timestamp'], as_index=False).agg({'Quantity': 'sum'}).reset_index()\nresult['OrderID'] = range(1, len(result) + 1)\nresult = result[['OrderID', 'Timestamp', 'Product', 'Quantity']].sort_values(by='Timestamp').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames, `sales` and `returns`, that need to be merged and processed for analysis. The `sales` DataFrame looks like this:\n\n```\n    OrderID     Date     Amount\n0      1001 2023-01-01     250.0\n1      1002 2023-01-02     150.0\n2      1003 2023-01-02     300.0\n3      1004 2023-01-03     200.0\n```\n\nAnd the `returns` DataFrame looks like this:\n\n```\n    OrderID     Date    Amount\n0      1002 2023-01-02      50.0\n1      1003 2023-01-03     100.0\n2      1005 2023-01-03     20.0\n```\n\nYour task is to combine these two DataFrames, ensuring that you include all sales records and the respective return amounts, such that if a sale has a corresponding return, the return amount should be deducted from the total sale amount. Furthermore, ensure that the `Date` format in the resulting DataFrame is simplified to \"DD-MMM-YYYY\". \n\nThe expected output should look like this:\n```\n    OrderID     Date     NetAmount\n0      1001 01-Jan-2023     250.0\n1      1002 02-Jan-2023     100.0\n2      1003 02-Jan-2023     200.0\n3      1004 03-Jan-2023     200.0\n4      1005 03-Jan-2023       NaN\n```\n\nCurrently, you have tried merging them like this:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({'OrderID': [1001, 1002, 1003, 1004],\n                      'Date': ['2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n                      'Amount': [250.0, 150.0, 300.0, 200.0]})\n\nreturns = pd.DataFrame({'OrderID': [1002, 1003, 1005],\n                        'Date': ['2023-01-02', '2023-01-03', '2023-01-03'],\n                        'Amount': [50.0, 100.0, 20.0]})\n```\n\nAnd you are unsure how to calculate the `NetAmount` and format the dates correctly.\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nmerged = pd.merge(sales, returns, on='OrderID', how='left', suffixes=('_sale', '_return'))\nmerged['NetAmount'] = merged['Amount_sale'] - merged['Amount_return'].fillna(0)\nmerged['Date'] = pd.to_datetime(merged['Date_sale']).dt.strftime('%d-%b-%Y')\nresult = merged[['OrderID', 'Date', 'NetAmount']]\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame representing sales data for a retail store, structured like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['A', 'B', 'A', 'B', 'C'],\n    'store': ['X', 'X', 'Y', 'Y', 'X'],\n    'sales': [100, 200, 150, 300, 250],\n    'date': pd.to_datetime(['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02', '2021-01-01'])\n}\ndf = pd.DataFrame(data)\n```\n\nI need to calculate the total sales for each product in each store, and then normalize these totals across all products for each store. The expected output should have a multi-index with levels for 'store' and 'product', along with the normalized sales.\n\nHow can I achieve this? \n\nThe result should look like this:\n\n```\n                sales  normalized_sales\nstore product                         \nX     A      100           normalized_value_A\n      B      200           normalized_value_B\n      C      250           normalized_value_C\nY     A      150           normalized_value_A\n      B      300           normalized_value_B\n```\n\nWhere `normalized_value_X` are the normalized sales values for each product. \n\nPlease fill in the missing code to accomplish this normalization.\n\nresult = ... # put solution in this variable",
    "output": "\n```python\nresult = df.groupby(['store', 'product'])['sales'].sum().reset_index()\nresult['normalized_sales'] = result.groupby('store')['sales'].transform(lambda x: x / x.sum())\nresult = result.set_index(['store', 'product'])\n```"
  },
  {
    "input": "Problem:\nI have a dataset of customer orders represented as a pandas DataFrame with a MultiIndex consisting of 'customer_id' and 'order_date'. Each order has multiple items with 'product_id' and 'quantity'. I want to normalize the 'quantity' columns for each customer across their orders. Here is a snippet of the DataFrame:\n\n```\n                    product_id  quantity\ncustomer_id order_date                      \n1           2021-01-01        A        5\n            2021-01-01        B        2\n            2021-01-02        A        3\n2           2021-01-01        A        4\n            2021-01-02        B        6\n            2021-01-02        C        1\n```\n\nThe goal is to create a new DataFrame where the 'quantity' is normalized per customer, meaning the quantities for each customer should sum to 1. The resulting DataFrame should keep the same MultiIndex structure.\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'product_id': ['A', 'B', 'A', 'A', 'B', 'C'],\n    'quantity': [5, 2, 3, 4, 6, 1]\n}\nindex = pd.MultiIndex.from_tuples(\n    [(1, '2021-01-01'), (1, '2021-01-01'), (1, '2021-01-02'), \n     (2, '2021-01-01'), (2, '2021-01-02'), (2, '2021-01-02')],\n    names=['customer_id', 'order_date']\n)\ndf = pd.DataFrame(data, index=index)\n</code>\nnormalized_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nnormalized_df = df.groupby(level='customer_id').apply(lambda x: x['quantity'] / x['quantity'].sum()).reset_index(level=0, drop=True)  \nnormalized_df = df[['product_id']].join(normalized_df.rename('quantity'))  \n</code>"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'client_id': [1, 1, 2, 2, 3, 3, 3],\n    'purchase_amount': [100, np.nan, 200, 250, np.nan, 300, 400],\n    'purchase_date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-02', '2023-01-01', '2023-01-04'])\n})\n```\n\nThe DataFrame includes some missing values in the `purchase_amount` column. You want to fill these missing values with the average purchase amount for each client. Additionally, you need to create a new column that contains the cumulative sum of `purchase_amount` for each client, but only considering the non-null values.\n\nHow can you achieve the following modified DataFrame?\n\n```python\n   client_id  purchase_amount purchase_date  avg_purchase client_cumsum\n0          1            100.0    2023-01-01           100.0         100.0\n1          1            100.0    2023-01-02           100.0         200.0\n2          2            200.0    2023-01-01           225.0         200.0\n3          2            250.0    2023-01-03           225.0         450.0\n4          3            333.3    2023-01-02           333.3         333.3\n5          3            300.0    2023-01-01           333.3         633.3\n6          3            400.0    2023-01-04           333.3        1033.3\n```\n\nYou can start your code with:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'client_id': [1, 1, 2, 2, 3, 3, 3],\n    'purchase_amount': [100, np.nan, 200, 250, np.nan, 300, 400],\n    'purchase_date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-02', '2023-01-01', '2023-01-04'])\n})\n```\n\nresult = ... # put solution in this variable",
    "output": "\n```python\ndf['avg_purchase'] = df.groupby('client_id')['purchase_amount'].transform(lambda x: x.fillna(x.mean()))\ndf['purchase_amount'] = df['avg_purchase']\ndf['client_cumsum'] = df.groupby('client_id')['purchase_amount'].cumsum()\nresult = df\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame of users and their associated scores in multiple categories, and you want to compute the maximum score for each user while excluding any scores that are null (NaN). The DataFrame looks like this:\n\n```python\nimport pandas as pd\ndata = {\n    'user': ['Alice', 'Bob', 'Alice', 'Bob', 'Charlie'],\n    'category': ['A', 'A', 'B', 'B', 'A'],\n    'score': [95, None, 80, 100, None]\n}\ndf = pd.DataFrame(data)\n```\n\nThe desired output for each user should show the maximum score across all categories while discarding any null scores:\n\n```\n     user  max_score\n0   Alice        95.0\n1     Bob       100.0\n2 Charlie         0.0\n```\n\nHowever, your current attempt is not yielding the correct results. You want to replace any user's `max_score` with `0.0` if all their scores are null. Here\u2019s the code to modify:\n\n```python\nresult = df.groupby('user')['score'].apply(...).reset_index()\n```\n\nFill in the blank to derive the correct max score with the specified conditions.",
    "output": "\n```python\nresult = df.groupby('user')['score'].apply(lambda x: x.max() if x.notnull().any() else 0.0).reset_index(name='max_score')\n```"
  },
  {
    "input": "Problem:\nI have two dataframes representing sales data from two different months:\n\ndf1:\n```\n   product_id  product_name     date  sales\n0           1    Product A  2023-01-01    100\n1           2    Product B  2023-01-01    150\n2           3    Product C  2023-01-01    200\n3           4    Product D  2023-01-01    250\n```\n\ndf2:\n```\n   product_id     date  sales\n0           2 2023-02-01    180\n1           3 2023-02-01    220\n2           4 2023-02-01    275\n3           5 2023-02-01    300\n```\n\nI need to concatenate these two dataframes into a single dataframe based on `product_id`, filling in the `product_name` from `df1` into `df2` for matching `product_id`s. The resulting dataframe should have all the sales data from both months, with the same `product_id` rows grouped together, and the rows sorted by date. If there\u2019s no match for `product_id` in `df1`, the `product_name` should appear as NaN.\n\nHere is the initial concatenated result that needs adjustment:\n```\n   product_id product_name     date  sales\n0           1   Product A  2023-01-01    100\n1           2   Product B  2023-01-01    150\n2           2        NaN  2023-02-01    180\n3           3   Product C  2023-01-01    200\n4           3        NaN  2023-02-01    220\n5           4   Product D  2023-01-01    250\n6           4        NaN  2023-02-01    275\n7           5        NaN  2023-02-01    300\n```\n\nYou should use `pd.concat()` along with a merge to achieve the desired format. \n\nA:\n<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({'product_id': [1, 2, 3, 4],\n                    'product_name': ['Product A', 'Product B', 'Product C', 'Product D'],\n                    'date': ['2023-01-01', '2023-01-01', '2023-01-01', '2023-01-01'],\n                    'sales': [100, 150, 200, 250]})\n\ndf2 = pd.DataFrame({'product_id': [2, 3, 4, 5],\n                    'date': ['2023-02-01', '2023-02-01', '2023-02-01', '2023-02-01'],\n                    'sales': [180, 220, 275, 300]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.concat([df1, df2]).merge(df1[['product_id', 'product_name']], on='product_id', how='left').sort_values(by='date').reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\nYou are working on a classification problem with a dataset that includes various features, some of which are categorical. You need to preprocess the dataset before applying machine learning algorithms. Specifically, you want to:\n\n1. Encode the categorical variables using one-hot encoding.\n2. Impute missing values in the numeric features with the median of each feature.\n3. Scale the numeric features to a range of [0, 1].\n\nGiven a pandas DataFrame `df` with both numerical and categorical features, fill in the following code to perform these preprocessing steps. \n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Sample data creation (omit this part in actual code)\n# df = pd.DataFrame({\n#     'Age': [25, 30, 35, None],\n#     'Salary': [50000, 60000, None, 80000],\n#     'City': ['New York', 'Los Angeles', None, 'Chicago']\n# })\n\n# Step 1: Encode categorical variables\nencoder = OneHotEncoder(sparse=False, drop='first')\nencoded_features = encoder.fit_transform(df[['City']])  # Adjust column name accordingly\n\n# Step 2: Impute missing values for numerical features\nimputer = SimpleImputer(strategy='median')\nnumeric_features = imputer.fit_transform(df[['Age', 'Salary']])  # Adjust column names accordingly\n\n# Step 3: Scale the numeric features\nscaler = MinMaxScaler()\nscaled_numeric_features = scaler.fit_transform(numeric_features)\n\n# Combine the processed features back into a single DataFrame\nprocessed_df = pd.DataFrame(np.hstack([scaled_numeric_features, encoded_features]))\n\n# What should be the final processed DataFrame?\nfinal_processed_df = ... # put solution in this variable\n```",
    "output": "\n```python\nfinal_processed_df = pd.DataFrame(np.hstack([scaled_numeric_features, encoded_features]), \n                                   columns=[*df[['Age', 'Salary']].columns, *encoder.get_feature_names_out(['City'])])\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame representing customer orders at a cafe as shown below. The orders may have missing values (NaN). You want to create a new column that calculates the total cost for each order based on the quantity of items ordered and their respective prices, but you should treat NaN as zero for the calculations. The DataFrame is defined as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Customer': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Coffee': [2, 1, np.nan, 3],\n    'Tea': [1, np.nan, 2, 1],\n    'Pastries': [np.nan, 5, 3, np.nan],\n    'Price_Coffee': [2.50, 2.50, 2.50, 2.50],\n    'Price_Tea': [2.00, 2.00, 2.00, 2.00],\n    'Price_Pastries': [3.00, 3.00, 3.00, 3.00]\n})\n```\n\nNext, you need to calculate the total cost as follows:\n- `Total_Cost` should be computed as:\n  \n  Total_Cost = (Coffee Quantity \u00d7 Price_Coffee) + (Tea Quantity \u00d7 Price_Tea) + (Pastries Quantity \u00d7 Price_Pastries)\n\nYour goal is to fill in the blank with the appropriate code to achieve this.\n\n```python\ndf['Total_Cost'] = df.apply(lambda row: row['Coffee'] * row['Price_Coffee'] + row['Tea'] * row['Price_Tea'] + row['Pastries'] * row['Price_Pastries'], axis=1)\n```\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Customer': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Coffee': [2, 1, np.nan, 3],\n    'Tea': [1, np.nan, 2, 1],\n    'Pastries': [np.nan, 5, 3, np.nan],\n    'Price_Coffee': [2.50, 2.50, 2.50, 2.50],\n    'Price_Tea': [2.00, 2.00, 2.00, 2.00],\n    'Price_Pastries': [3.00, 3.00, 3.00, 3.00]\n})\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\ndf['Total_Cost'] = df.apply(lambda row: row['Coffee'] * row['Price_Coffee'] + row['Tea'] * row['Price_Tea'] + row['Pastries'] * row['Price_Pastries'] if pd.notna(row['Coffee']) and pd.notna(row['Tea']) and pd.notna(row['Pastries']) else 0, axis=1)\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing customer information and their purchasing patterns, which includes the columns 'Age', 'Annual_Income', and 'Spending_Score'. You want to segment the customers into clusters based on these features using KMeans clustering from scikit-learn. However, before applying the clustering, you need to standardize the feature values to ensure the clustering algorithm works effectively.\n\nHere is a sample of your data:\n\n```python\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'Age': [25, 45, 35, 50, 23, 40, 28, 55],\n    'Annual_Income': [50000, 80000, 62000, 70000, 53000, 91000, 68000, 43000],\n    'Spending_Score': [60, 30, 40, 70, 50, 90, 80, 20]\n})\n```\n\nYour task is to write one line of code that fits a KMeans model with 3 clusters after standardizing the features. Assume that `StandardScaler` and `KMeans` are already imported.\n\nA:\n```python\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n... # put solution in this variable\n```",
    "output": "\n```python\nkmeans_model = KMeans(n_clusters=3).fit(StandardScaler().fit_transform(data))\n```"
  },
  {
    "input": "Problem:\nYou are given a pandas DataFrame that contains sales data for different products in different regions. The DataFrame `df` has the following structure:\n\n```\n   product    region     sales\n0     A       North      200\n1     A       South      150\n2     B       North      300\n3     B       South      NaN\n4     C       North      250\n5     C       South      NaN\n6     A       North      NaN\n7     B       South      100\n8     C       North      400\n```\n\nYou need to calculate the total sales for each product by region, while applying the following conditions:\n- If the sales value is `NaN`, treat it as zero for the purpose of summation.\n- After summing, create a new DataFrame that shows `total_sales` for each combination of `product` and `region`.\n\nYour desired output should look like this:\n\n```\n   product    region  total_sales\n0     A       North        200\n1     A       South        150\n2     B       North        300\n3     B       South        100\n4     C       North        650\n5     C       South          0\n```\n\nHere is the starting code:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'B', 'C'],\n    'region': ['North', 'South', 'North', 'South', 'North', 'South', 'North', 'South', 'North'],\n    'sales': [200, 150, 300, None, 250, None, None, 100, 400]\n})\n```\n\nFill in the blank to compute the desired result:\n\n```python\ntotal_sales_df = ...\n```",
    "output": "\n```python\ntotal_sales_df = df.groupby(['product', 'region'], as_index=False)['sales'].sum().fillna(0).rename(columns={'sales': 'total_sales'})\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products and regions, structured as follows:\n\n```plaintext\n    Product    Region  Sales\n0  Widget A      East   150\n1  Widget B      West   200\n2  Widget C      North  250\n3  Widget A      South  300\n4  Widget B      East   400\n5  Widget C      West   NaN\n6  Widget A      East   NaN\n7  Widget B      North  450\n```\n\nYou want to calculate the average sales per product while disregarding any NaN values, and you also want to append this average to the DataFrame as a new column called 'Avg_Sales'. The result should still allow for the regions and products to be displayed.\n\nYour desired output should look like this:\n\n```plaintext\n    Product    Region  Sales  Avg_Sales\n0  Widget A      East   150       225.0\n1  Widget B      West   200       283.33\n2  Widget C      North  250       250.0\n3  Widget A      South  300       225.0\n4  Widget B      East   400       283.33\n5  Widget C      West   NaN       250.0\n6  Widget A      East   NaN       225.0\n7  Widget B      North  450       283.33\n```\n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Product': ['Widget A', 'Widget B', 'Widget C', 'Widget A', 'Widget B', 'Widget C', 'Widget A', 'Widget B'],\n    'Region': ['East', 'West', 'North', 'South', 'East', 'West', 'East', 'North'],\n    'Sales': [150, 200, 250, 300, 400, np.nan, np.nan, 450]\n})\n```\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n```python\n\n",
    "output": "\n```python\navg_sales = df.groupby('Product')['Sales'].mean().reset_index()\navg_sales.rename(columns={'Sales': 'Avg_Sales'}, inplace=True)\ndf = df.merge(avg_sales, on='Product', how='left')\n```"
  },
  {
    "input": "Problem:\n\nI have a tensor `data` shaped (100, 5) representing different features for 100 samples. I want to create a new tensor that only contains the entries where the second feature is greater than 0.5. Each entry should include all features of the filtered samples. The final tensor should be shaped according to the number of samples that meet this condition.\n\nA:\n\n<code>\nimport torch\ndata = load_data()\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = data[data[:, 1] > 0.5]\n</code>"
  },
  {
    "input": "## Problem\n\nYou are working with a PyTorch tensor that represents the similarity scores of items after applying a neural network model in a recommendation system. The tensor looks like this:\n\n```python\nscores = torch.tensor([[0.1, 0.4, 0.3],\n                       [0.2, 0.5, 0.1],\n                       [0.4, 0.1, 0.2]])\n```\n\nYou also have a boolean mask representing which of these scores are considered valid or relevant:\n\n```python\nmask = torch.tensor([[True, False, True],\n                     [False, True, False],\n                     [True, True, False]])\n```\n\nYour task is to use this mask to extract the valid scores from the `scores` tensor while simultaneously calculating their mean. The resulting value should be stored in a variable named `result`.\n\nFill in the blank below:\n\n```python\nimport torch\n\nscores = torch.tensor([[0.1, 0.4, 0.3],\n                       [0.2, 0.5, 0.1],\n                       [0.4, 0.1, 0.2]])\nmask = torch.tensor([[True, False, True],\n                     [False, True, False],\n                     [True, True, False]])\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = scores[mask].mean()\n```"
  },
  {
    "input": "Problem:\nI am working with two DataFrames, X and Y, that represent sales data over time. The structure of the DataFrames is as follows:\n\nX:\n```\n   Product  Sales        Date\n0       A    200  2023-01-01\n1       B    150  2023-01-02\n2       C    300  2023-01-03\n3       A    250  2023-01-01\n```\n\nY:\n```\n   Product  Sales        Date\n0       A    180  2023-01-02\n1       D    330  2023-01-04\n2       B    160  2023-01-03\n```\n\nI need to merge these DataFrames to create a consolidated view of sales data, ensuring that any duplicate entries for a Product on the same Date are merged such that their Sales values are summed. The resulting DataFrame should have the unique combinations of Product and Date, while maintaining the original data integrity and sorting by Date first and then by Product. The expected structure of the output DataFrame is:\n\n```\n   Product  Sales        Date\n0       A    200  2023-01-01\n1       A    180  2023-01-02\n2       B    150  2023-01-02\n3       B    320  2023-01-03 \n4       C    300  2023-01-03\n5       D    330  2023-01-04\n```\n\nCurrently, I am trying to use `pd.concat` but I am unsure how to handle the duplicates and sum their Sales properly.\n\nA:\n<code>\nimport pandas as pd\n\nX = pd.DataFrame({\n    'Product': ['A', 'B', 'C', 'A'],\n    'Sales': [200, 150, 300, 250],\n    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-01']\n})\n\nY = pd.DataFrame({\n    'Product': ['A', 'D', 'B'],\n    'Sales': [180, 330, 160],\n    'Date': ['2023-01-02', '2023-01-04', '2023-01-03']\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.concat([X, Y]).groupby(['Product', 'Date'], as_index=False).sum().sort_values(by=['Date', 'Product'])\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset containing information about customer transactions, including customer ID, transaction amount, and transaction date. Some of the transaction amounts are missing. You want to fill in these missing amounts using interpolation based on the mean of the amounts in the same month. Then, you plan to apply a logarithmic transformation to normalize the distribution of transaction amounts for further analysis using machine learning models.\n\nHere's a sample dataset:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'customer_id': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n    'transaction_amount': [100, np.nan, 200, 150, np.nan, np.nan, 300, 400, np.nan],\n    'transaction_date': pd.to_datetime(['2023-01-01', '2023-01-15', '2023-01-20', \n                                        '2023-02-01', '2023-02-10', '2023-02-15', \n                                        '2023-03-01', '2023-03-05', '2023-03-10'])\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to fill in the missing `transaction_amount` values with the mean of the available transaction amounts for each respective month, and then apply the logarithmic transformation to the `transaction_amount` column. \n\nComplete the statement below to achieve this, storing the result in the `df` DataFrame:\n\n```python\n# Fill missing values by group and apply logarithmic transformation\ndf['transaction_amount'] = df.groupby(df['transaction_date'].dt.to_period('M'))['transaction_amount'].transform(lambda x: x.fillna(x.mean()).apply(np.log))\n```\n\nBEGIN SOLUTION\n```python\n\n```",
    "output": "\n```python\n# Fill missing values by group and apply logarithmic transformation\ndf['transaction_amount'] = df.groupby(df['transaction_date'].dt.to_period('M'))['transaction_amount'].transform(lambda x: x.fillna(x.mean()).apply(np.log))\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames that record daily sales and returns of products at a retail store. The sales DataFrame (`sales_df`) tracks information about each sale, while the returns DataFrame (`returns_df`) captures the details of returns made by customers.\n\n`sales_df`:\n```\n   product_id    date  sales_amount\n0          101  2023-01-01         150\n1          102  2023-01-01          80\n2          101  2023-01-02         200\n3          103  2023-01-02         100\n4          102  2023-01-03         120\n```\n\n`returns_df`:\n```\n   product_id    date  return_amount\n0          101  2023-01-02          30\n1          102  2023-01-03          20\n2          101  2023-01-01          10\n3          104  2023-01-02          50\n```\n\nYou need to combine these two DataFrames into a single DataFrame (`combined_df`) that maintains the integrity of the sales and returns data. For each transaction date, ensure that you calculate the net sales for each product (sales minus returns) and preserve all products even if they have not been returned.\n\nThe combined DataFrame should look something like:\n```\n   product_id    date  net_sales\n0          101  2023-01-01         140   # 150 - 10\n1          101  2023-01-02         170   # 200 - 30\n2          102  2023-01-01          80    # only in sales \n3          102  2023-01-03         100   # 120 - 20\n4          103  2023-01-02         100   # only in sales\n5          104  2023-01-02        -50    # only in returns\n```\n\nYou have already read the data into pandas DataFrames. Now, fill in the blank below to create the `combined_df` with the required calculations.\n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'product_id': [101, 102, 101, 103, 102],\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n    'sales_amount': [150, 80, 200, 100, 120]\n})\n\nreturns_df = pd.DataFrame({\n    'product_id': [101, 102, 101, 104],\n    'date': ['2023-01-02', '2023-01-03', '2023-01-01', '2023-01-02'],\n    'return_amount': [30, 20, 10, 50]\n})\n\ncombined_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\ncombined_df = sales_df.merge(returns_df, on=['product_id', 'date'], how='outer').fillna(0)\ncombined_df['net_sales'] = combined_df['sales_amount'] - combined_df['return_amount']\ncombined_df = combined_df[['product_id', 'date', 'net_sales']]\ncombined_df = combined_df.sort_values(by=['product_id', 'date']).reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nContext\nI have two DataFrames that need to be combined while also ensuring data integrity and managing duplicates effectively. \n\nThe first DataFrame is as follows:\n```python\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n```\n\nAnd the second DataFrame is:\n```python\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\n\nI want to merge these two DataFrames on the `product_id` column, but I need to handle the case where `product_id` values in `df2` may result in duplicate rows in the final merged DataFrame, and keep only the first occurrence. The output should include all products and their corresponding stores, with an added column for availability.\n\nFinally, I would like to make sure the final DataFrame is sorted by `product_id`, in ascending order. The expected output should resemble:\n```python\n   product_id        products  price    store  availability\n0         101          Widget  20.99      NaN           NaN\n1         102          Gadget  25.50      NaN           NaN\n2         103          Doodad  15.75  Store A          True\n3         104  Thingamajig  22.10  Store B         False\n4         105           NaN     NaN  Store D          True\n```\n\nYou can find below the beginning of my implementation:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2.drop_duplicates(subset='product_id'), on='product_id', how='outer').sort_values(by='product_id').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset that contains the sales information of a retail store over different months for various products. The data is structured in a multi-index DataFrame, with the first level of indexing as 'Product' and the second level as 'Month'. Here is the structure of the DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples(\n    [('A', '2023-01'), ('A', '2023-02'), ('B', '2023-01'), ('B', '2023-02')],\n    names=['Product', 'Month']\n)\ndata = np.random.randint(1, 100, size=(4,))\ndf = pd.DataFrame(data, index=index, columns=['Sales'])\n```\n\nTo prepare the data for analysis, you need to normalize the sales figures across each product so that each product's sales amount is expressed as a percentage of that product's total sales over the monitored months. You want the normalized sales to be stored back in the DataFrame with the same multi-index structure.\n\nComplete the code below to achieve this normalization, storing the result in the variable `normalized_sales`.\n\n```python\n# Calculate total sales per product\ntotal_sales = df.groupby(level='Product').sum()\n\n# Normalize sales\nnormalized_sales = df.div(total_sales).reset_index(level='Product', drop=True)\nnormalized_sales.columns = ['Normalized_Sales']\n```\n\nFill in the blank where you need to reassign the normalized sales to the correct index structure. \n\n```python\nfinal_df = df.copy()\nfinal_df['Normalized_Sales'] = ...\n```",
    "output": "\n```python\nfinal_df = df.copy()\nfinal_df['Normalized_Sales'] = normalized_sales\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset containing information about customers' purchases, including purchase amounts, customer demographics, and timestamps of the transactions. I need to perform preprocessing on this dataset to prepare it for a machine learning model. Specifically, I want to:\n\n1. Normalize the 'purchase_amount' column using MinMax scaling.\n2. Convert the 'timestamp' column to a datetime object and extract the year as a new feature.\n3. One-hot encode the 'gender' column.\n\nGiven the following DataFrame named `df`:\n\n```python\nimport pandas as pd\n\ndata = {\n    'purchase_amount': [100.0, 150.5, 200.75, 130.0],\n    'gender': ['M', 'F', 'F', 'M'],\n    'timestamp': ['2021-05-01', '2021-05-02', '2021-05-03', '2021-05-04']\n}\n\ndf = pd.DataFrame(data)\n```\n\nI want to use MinMaxScaler for normalization and then create the new column and one-hot encode the gender. I have already imported `MinMaxScaler` from `sklearn.preprocessing`. Write the code to apply MinMax scaling to the 'purchase_amount' column and create the new one-hot encoded DataFrame.\n\nFill in the blank for the following code:\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf['normalized_purchase'] = scaler.fit_transform(df[['purchase_amount']])\n\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['year'] = df['timestamp'].dt.year\n\none_hot = pd.get_dummies(df['gender'], prefix='gender')\nfinal_df = pd.concat([df, one_hot], axis=1)\n```\n\nThe missing piece of information is the correct way to adjust `final_df` such that it contains the new features without the original categorical column. \n\nHow do I update `final_df` to drop the original 'gender' column?\n \nA:\n\n<code>\nfinal_df = final_df.drop(columns=['gender'])  # put solution in this variable\n</code>",
    "output": "\n<code>\nfinal_df = final_df.drop(columns=['gender'])  # put solution in this variable\n</code>"
  },
  {
    "input": "Problem:\nContext  \nI have two DataFrames representing sales data and returns data. The sales DataFrame contains transactions, while the returns DataFrame contains records of returned items.\n\nSales DataFrame (sales_df):\n```\n   order_id  product_id  quantity  sale_date\n0        1         101         2  2023-01-01\n1        2         102         1  2023-01-02\n2        3         101         1  2023-01-03\n3        4         103         4  2023-01-04\n4        5         104         3  2023-01-05\n```\n\nReturns DataFrame (returns_df):\n```\n   return_id  product_id  quantity  return_date\n0         1         101         1  2023-01-03\n1         2         102         1  2023-01-05\n2         3         104         2  2023-01-06\n```\n\nI want to create a new DataFrame that combines both sales and returns, with the quantity of each product adjusted based on the returns. If a product is returned, its sold quantity should be decreased accordingly in the final DataFrame. The new DataFrame should look like this:\n\nFinal DataFrame:\n```\n   product_id  sold_quantity  return_quantity\n0         101              1                1\n1         102              0                1\n2         103              4                0\n3         104              1                2\n```\n\nThe `sold_quantity` is calculated as the total quantity sold minus the total returned quantity for each product.\n\nHow can I achieve this using Pandas?  \nHere's the setup:\n```python\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'order_id': [1, 2, 3, 4, 5],\n    'product_id': [101, 102, 101, 103, 104],\n    'quantity': [2, 1, 1, 4, 3],\n    'sale_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']\n})\n\nreturns_df = pd.DataFrame({\n    'return_id': [1, 2, 3],\n    'product_id': [101, 102, 104],\n    'quantity': [1, 1, 2],\n    'return_date': ['2023-01-03', '2023-01-05', '2023-01-06']\n})\n```\n\nresult = ... # put solution in this variable  \nBEGIN SOLUTION  \n```",
    "output": "\n```python\nsales_summary = sales_df.groupby('product_id')['quantity'].sum().reset_index()\nreturns_summary = returns_df.groupby('product_id')['quantity'].sum().reset_index()\n\nmerged_df = pd.merge(sales_summary, returns_summary, on='product_id', how='outer', suffixes=('_sold', '_returned')).fillna(0)\nmerged_df['sold_quantity'] = merged_df['quantity_sold'] - merged_df['quantity_returned']\nmerged_df = merged_df[['product_id', 'sold_quantity', 'quantity_returned']]\nmerged_df.columns = ['product_id', 'sold_quantity', 'return_quantity']\n\nresult = merged_df\n```"
  },
  {
    "input": "Problem:\nYou are provided with two DataFrames: `df1` and `df2`.\n\n`df1`:\n```\n   product_id  category     date  sales\n0           1      A 2023/01/01    100\n1           2      B 2023/01/01    150\n2           3      A 2023/01/02    200\n3           4      C 2023/01/01    50\n4           5      B 2023/01/03    300\n```\n\n`df2`:\n```\n   product_id     date  sales\n0           1 2023/01/01     80\n1           2 2023/01/03    120\n2           3 2023/01/02    210\n3           6 2023/01/04    90\n4           7 2023/01/02     60\n```\n\nYou need to concatenate `df1` and `df2` based on `product_id`, filling in the missing `category` information from `df1` into `df2`. Ensure the results are sorted by `product_id` and the dates are formatted as \"DD-Mon-YYYY\". Duplicate entries should be kept in the final DataFrame.\n\nThe final DataFrame should look like this:\n```\n   product_id category         date  sales\n0           1        A  01-Jan-2023    100\n1           1        A  01-Jan-2023     80\n2           2        B  01-Jan-2023    150\n3           2        B  01-Jan-2023    120\n4           3        A  02-Jan-2023    200\n5           3        A  02-Jan-2023    210\n6           4        C  01-Jan-2023     50\n7           5        B  03-Jan-2023    300\n8           6      NaN  04-Jan-2023     90\n9           7      NaN  02-Jan-2023     60\n```\n\nSo far, the result generated with `pd.concat([df1, df2], axis=0)` is like this:\n```\n   product_id     date category  sales\n0           1 2023/01/01        A    100\n1           2 2023/01/01        B    150\n2           3 2023/01/02        A    200\n3           4 2023/01/01        C     50\n4           5 2023/01/03        B    300\n0           1 2023/01/01      NaN     80\n1           2 2023/01/03      NaN    120\n2           3 2023/01/02      NaN    210\n3           6 2023/01/04      NaN     90\n4           7 2023/01/02      NaN     60\n```\n\nYour task is to complete the following code to achieve the desired transformation and formatting:\n\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'product_id': [1, 2, 3, 4, 5],\n    'category': ['A', 'B', 'A', 'C', 'B'],\n    'date': ['2023/01/01', '2023/01/01', '2023/01/02', '2023/01/01', '2023/01/03'],\n    'sales': [100, 150, 200, 50, 300]\n})\n\ndf2 = pd.DataFrame({\n    'product_id': [1, 2, 3, 6, 7],\n    'date': ['2023/01/01', '2023/01/03', '2023/01/02', '2023/01/04', '2023/01/02'],\n    'sales': [80, 120, 210, 90, 60]\n})\n\n# Your solution should populate this variable\nresult = ...\n```",
    "output": "\n```python\nresult = pd.concat([df1, df2.assign(category=df2['product_id'].map(df1.set_index('product_id')['category']))], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(by='product_id').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nGiven a multi-index DataFrame that represents sales data for different products over various regions, structured as follows:\n\n```python\ndata = {\n    ('North', 'Product A'): [100, 150, 200],\n    ('North', 'Product B'): [80, 120, 160],\n    ('South', 'Product A'): [130, 170, 210],\n    ('South', 'Product B'): [90, 130, 190]\n}\ndf = pd.DataFrame(data, index=pd.date_range(start='2023-01-01', periods=3, freq='M'))\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Region', 'Product'])\n```\n\nYour task is to normalize the sales figures by dividing each sales value by the maximum sales value in its corresponding region, and return a DataFrame that maintains the same multi-index structure, but with normalized values. The resulting DataFrame's structure should look like this:\n\n```\n                  Region     Product A   Product B\n2023-01-31        North    0.5          0.5\n2023-02-28        North    0.75         0.75\n2023-03-31        North    1.0          1.0\n2023-01-31        South    0.6190       0.4737\n2023-02-28        South    0.7960       0.6842\n2023-03-31        South    1.0          1.0\n```\n\nPlease fill in the blank to complete this task:\n\n```python\nnormalized_df = ...  # put solution in this variable\n```",
    "output": "\n```python\nnormalized_df = df.div(df.groupby(level='Region', axis=1).transform('max'), axis=1)\n```"
  },
  {
    "input": "Problem:\n\nYou are tasked with managing a dataset of high-dimensional embeddings, represented as a tensor in PyTorch with shape (batch_size, num_features). You need to filter this tensor to keep only the embeddings that meet a certain condition defined by a mask tensor. \n\nSuppose you have the following setup:\n\n```python\nimport torch\n\nbatch_size = 5\nnum_features = 10\nembeddings = torch.randn(batch_size, num_features)\nmask = torch.tensor([True, False, True, False, True])  # This mask indicates which rows to keep\n\n# Fill in the code below to filter the embeddings tensor using the mask\n```\n\nHow can you achieve this filtering and return the resulting tensor of embeddings that pass the mask condition?\n\nA:\n<code>\nfiltered_embeddings = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfiltered_embeddings = embeddings[mask]\n</code>"
  },
  {
    "input": "### Problem:\n\nI am working on a recommendation system using PyTorch. I have a tensor representing user-item interactions, where the rows correspond to users and the columns to items, with values indicating the interaction scores (higher scores indicate stronger interactions). Given this tensor, I'd like to apply a boolean mask to filter out items that have a score below a certain threshold, and then calculate the mean interaction score for the remaining items for each user.\n\nHere is the tensor and the boolean mask I have:\n\n```python\nimport torch\n\n# Tensor representing user-item interactions\ninteraction_scores = torch.tensor([[5.0, 2.0, 0.0, 4.0],\n                                    [3.0, 0.0, 1.0, 6.0],\n                                    [0.0, 8.0, 7.0, 0.0]])\n\n# Define a threshold\nthreshold = 2.0\n\n# Create a boolean mask\nmask = interaction_scores > threshold\n```\n\nI want to compute the mean of the filtered interaction scores for each user. Note that if all items for a user are filtered out, the mean should return `0.0`.\n\nComplete the following line of code to achieve this:\n\n```python\nmean_scores = torch.mean(interaction_scores[mask], dim=1)  # Calculate mean scores\nmean_scores = ...\n```\n\nA:\n\n```python\nmean_scores = ... # put solution in this variable\n```",
    "output": "\n```python\nmean_scores = torch.where(mask.sum(dim=1) > 0, mean_scores, torch.tensor(0.0))  # Handle cases with no valid items\n```"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame that contains the sales data of different products across multiple months. The DataFrame has columns 'product_id', 'month', 'sales', and 'returns', where 'returns' can contain null values. Your goal is to calculate the net sales by subtracting the returns from the sales and also to categorize the net sales into performance tiers: \"poor\" for negative or zero net sales, \"average\" for net sales between 1 and 100, and \"excellent\" for net sales above 100. \n\nThe initial DataFrame is as follows:\n```python\ndf = pd.DataFrame({\n    'product_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02'],\n    'sales': [150, 200, 50, 70, 300, 0],\n    'returns': [10, pd.NA, 5, 20, pd.NA, 5]\n})\n```\n\nYou attempted to calculate the net sales and categorize them but encountered issues when handling the null values in the returns column. Here is an incomplete approach you tried:\n```python\ndf['net_sales'] = df['sales'] - df['returns']\ndf['performance'] = df['net_sales'].apply(lambda x: 'excellent' if x > 100 else ('average' if x > 0 else 'poor'))\n```\n\nHowever, this code resulted in errors due to null values interfering with the calculations. \n\nA:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02'],\n    'sales': [150, 200, 50, 70, 300, 0],\n    'returns': [10, pd.NA, 5, 20, pd.NA, 5]\n})\n```\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf['returns'] = df['returns'].fillna(0)\ndf['net_sales'] = df['sales'] - df['returns']\ndf['performance'] = df['net_sales'].apply(lambda x: 'excellent' if x > 100 else ('average' if x > 0 else 'poor'))\n```"
  },
  {
    "input": "Problem:\nI'm working with PyTorch and I have a tensor `A` that contains embeddings for a set of documents. Each embedding is a 2D tensor with dimensions (batch_size, embedding_dim). I want to compute the pairwise cosine similarity for these document embeddings while ignoring any NaN values present in the tensor. The resulting similarity matrix should not contain these NaN contributions.\n\nExample input:\n```python\nimport torch\n\nA = torch.tensor([[0.5, 0.3, float('nan')],\n                  [0.1, 0.2, 0.9],\n                  [0.0, float('nan'), 1.0],\n                  [0.4, 0.5, 0.6]])\n```\n\nThe desired output should be a tensor of shape (4, 4) where each element (i, j) is the cosine similarity between the embeddings of the i-th and j-th documents, with NaNs appropriately handled.\n\nA:\n```python\nimport torch\n\nA = torch.tensor([[0.5, 0.3, float('nan')],\n                  [0.1, 0.2, 0.9],\n                  [0.0, float('nan'), 1.0],\n                  [0.4, 0.5, 0.6]])\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nimport torch\nimport torch.nn.functional as F\n\ndef cosine_similarity_with_nan(A):\n    # Replace NaNs with zeros for the purpose of similarity calculation\n    A_nan_fixed = torch.nan_to_num(A)\n    \n    # Compute the pairwise cosine similarity\n    similarity_matrix = F.cosine_similarity(A_nan_fixed.unsqueeze(1), A_nan_fixed.unsqueeze(0), dim=2)\n    \n    # Create a mask for NaN values\n    mask = ~torch.isnan(A).any(dim=1).unsqueeze(1) & ~torch.isnan(A).any(dim=1).unsqueeze(0)\n    \n    # Apply the mask to the similarity matrix\n    result = similarity_matrix * mask.float()\n    \n    return result\n\nresult = cosine_similarity_with_nan(A)\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a machine learning project and have a dataset contained within a pandas DataFrame that includes categorical features. Your task is to apply one-hot encoding to these categorical features while also ensuring to drop the original columns from the DataFrame afterwards. \n\nGiven the following DataFrame:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'city': ['New York', 'San Francisco', 'Los Angeles', 'New York', 'Los Angeles'],\n    'weather': ['Sunny', 'Foggy', 'Sunny', 'Rainy', 'Foggy'],\n    'temperature': [85, 72, 78, 80, 75]\n})\n```\n\nYou want to transform the 'city' and 'weather' categorical fields into one-hot encoded format and store the result in a variable.\n\nFill in the code to achieve this transformation:\n\n```python\ndf_encoded = ... # put solution in this variable\n```",
    "output": "\n```python\ndf_encoded = pd.get_dummies(df, columns=['city', 'weather'], drop_first=True)\n```"
  },
  {
    "input": "Problem:\nI have two pandas DataFrames that represent sales data and returns data. The sales DataFrame contains the following columns: 'order_id', 'customer_id', 'date', 'amount'. The returns DataFrame contains 'order_id', 'return_date', and 'amount_returned'. I need to merge these two DataFrames on 'order_id', ensuring that if an order has been returned, the 'amount' in the sales DataFrame is updated to reflect the return. Additionally, I want to keep the original columns intact while adding a new 'final_amount' column that shows the amount after returns have been deducted.\n\nHere are my DataFrames:\n\nsales_df:\n   order_id  customer_id       date  amount\n0        1           A 2023-01-01     100\n1        2           B 2023-01-02     150\n2        3           A 2023-01-03     200\n3        4           C 2023-01-04     300\n\nreturns_df:\n   order_id return_date  amount_returned\n0        2  2023-01-05               50\n1        3  2023-01-06              100\n\nI want the final DataFrame to look like this:\n   order_id  customer_id       date  amount  final_amount\n0        1           A 2023-01-01     100          100\n1        2           B 2023-01-02     150          100\n2        3           A 2023-01-03     200          100\n3        4           C 2023-01-04     300          300\n\nHow can I accomplish this with pandas? \n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'order_id': [1, 2, 3, 4],\n    'customer_id': ['A', 'B', 'A', 'C'],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']),\n    'amount': [100, 150, 200, 300]\n})\n\nreturns_df = pd.DataFrame({\n    'order_id': [2, 3],\n    'return_date': pd.to_datetime(['2023-01-05', '2023-01-06']),\n    'amount_returned': [50, 100]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmerged_df = sales_df.merge(returns_df, on='order_id', how='left')\nmerged_df['final_amount'] = merged_df['amount'] - merged_df['amount_returned'].fillna(0)\nresult = merged_df[['order_id', 'customer_id', 'date', 'amount', 'final_amount']]\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset that contains the daily average temperatures in degrees Celsius for a city over a year, represented as a Pandas DataFrame. You want to preprocess the temperature data by first filling missing values using the forward fill method, then normalizing the data using Min-Max scaling so that the values range between 0 and 1. Finally, you need to calculate the average temperature for each month after the preprocessing step.\n\nA:\n<code>\nimport pandas as pd\n\n# Assuming `df` is your DataFrame and the column containing temperatures is named 'Temperature'\n# Fill missing values and normalize the temperature data\ndf['Temperature'] = df['Temperature'].fillna(method='ffill')\nnormalized_temperatures = (df['Temperature'] - df['Temperature'].min()) / (df['Temperature'].max() - df['Temperature'].min())\n\n# Now, you need to calculate the average temperature for each month\naverage_monthly_temp = normalized_temperatures.groupby(df['Date'].dt.month).mean()\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = average_monthly_temp.reset_index(name='Average_Temperature')</code>"
  },
  {
    "input": "Problem:\n\nI have a DataFrame containing sales data with the following structure:\n\n```\n| Region   | Product    | Date       | Sales |\n|----------|------------|------------|-------|\n| North    | A          | 2022-01-01 | 100   |\n| South    | A          | 2022-01-02 | 150   |\n| East     | B          | 2022-01-01 | 200   |\n| West     | B          | 2022-01-03 | 250   |\n```\n\nI want to pivot this DataFrame to analyze sales by product across regions. However, I also want to normalize the sales values so that they are scaled between 0 and 1 for better comparative analysis in machine learning. \n\nGiven the DataFrame `df`, I want to fill in the missing line of code that performs the normalization before pivoting. How can I accomplish this using the `MinMaxScaler` from `sklearn.preprocessing`?\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = {\n    'Region': ['North', 'South', 'East', 'West'],\n    'Product': ['A', 'A', 'B', 'B'],\n    'Date': ['2022-01-01', '2022-01-02', '2022-01-01', '2022-01-03'],\n    'Sales': [100, 150, 200, 250]\n}\n\ndf = pd.DataFrame(data)\n\n# Fill in the missing line\nscaler = MinMaxScaler()\ndf['Sales'] = ... # put solution in this variable\ndf_pivot = df.pivot_table(index='Region', columns='Product', values='Sales', aggfunc='sum')\n\nprint(df_pivot)\n```\n\nA:\n<code>\ndf['Sales'] = scaler.fit_transform(df[['Sales']])\n</code>",
    "output": "\n<code>\ndf['Sales'] = scaler.fit_transform(df[['Sales']])\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames E and F as follows:\nE\n    Timestamp       Temperature\n0  2023-01-01 10:00:00       22.5\n1  2023-01-01 11:00:00       23.0\n2  2023-01-01 12:00:00       21.5\n\nF\n    Timestamp       Temperature\n0  2023-01-01 10:30:00       24.0\n1  2023-01-01 11:30:00       23.5\n\nI need to combine these two DataFrames based on the 'Timestamp', ensuring that if there are duplicate timestamps, the value from DataFrame F replaces the value from DataFrame E. Additionally, I want to add a new column 'Source' indicating whether the temperature came from DataFrame E or F. If the timestamp is original to DataFrame E, it should read \"E\", otherwise \"F\". The resulting DataFrame should be sorted by 'Timestamp'.\n\nOutput\n    Timestamp          Temperature Source\n0  2023-01-01 10:00:00       22.5      E\n1  2023-01-01 10:30:00       24.0      F\n2  2023-01-01 11:00:00       23.0      E\n3  2023-01-01 11:30:00       23.5      F\n4  2023-01-01 12:00:00       21.5      E\n\nA:\n<code>\nimport pandas as pd\n\n\nE = pd.DataFrame({\n    \"Timestamp\": pd.to_datetime([\"2023-01-01 10:00:00\", \"2023-01-01 11:00:00\", \"2023-01-01 12:00:00\"]),\n    \"Temperature\": [22.5, 23.0, 21.5]\n})\nF = pd.DataFrame({\n    \"Timestamp\": pd.to_datetime([\"2023-01-01 10:30:00\", \"2023-01-01 11:30:00\"]),\n    \"Temperature\": [24.0, 23.5]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.concat([E.assign(Source='E'), F.assign(Source='F')]).drop_duplicates(subset='Timestamp', keep='last').sort_values(by='Timestamp').reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\n\nYou are given a dataset containing customer information and their purchasing behavior in a CSV file. The dataset contains columns for customer_id, age, annual_income, and spending_score. Before building a machine learning model to predict customer segments based on their spending scores, you need to preprocess the data.\n\nYour tasks are:\n1. Load the dataset and handle any missing values by filling them with the mean of their respective columns.\n2. Normalize the annual_income and spending_score using MinMax scaling, so that the values fall between 0 and 1.\n3. Finally, convert the processed data into a 2D NumPy array where the first two columns represent 'annual_income' and 'spending_score' and the rest is filled with customer IDs.\n\nThe code snippet provided is as follows:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = pd.read_csv(\"customer_data.csv\")\n# Fill missing values\ndata.fillna(data.mean(), inplace=True)\n\nscaler = MinMaxScaler()\ndata[['annual_income', 'spending_score']] = scaler.fit_transform(data[['annual_income', 'spending_score']])\n\nprocessed_data = ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>",
    "output": "\nprocessed_data = data[['customer_id', 'annual_income', 'spending_score']].to_numpy()"
  },
  {
    "input": "Problem:\nYou have a dataset containing sales information of products over several regions and years. The dataset is structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product_A', 'North'): [100, 150, 200],\n    ('Product_A', 'South'): [120, 180, 220],\n    ('Product_B', 'North'): [130, 170, 210],\n    ('Product_B', 'South'): [140, 190, 230]\n}\n\nindex = pd.MultiIndex.from_product([['2021', '2022', '2023'], ['Q1', 'Q2', 'Q3']], names=['Year', 'Quarter'])\ndf = pd.DataFrame(data, index=index)\n```\n\nYour task is to normalize the sales values in the DataFrame using Min-Max scaling along the 'Product' level of the MultiIndex. The output should be a new DataFrame with the same structure, where the sales values are transformed to a 0-1 range for each product.\n\nPlease fill in the blank variable with the correct solution to achieve this normalization:\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\n# Apply normalization here\nnormalized_df = ...\n```",
    "output": "\n```python\nnormalized_df = df.groupby(level=0).apply(lambda x: scaler.fit_transform(x)).reset_index(level=0, drop=True)\n```"
  },
  {
    "input": "Problem:\n\nI have a Pandas DataFrame containing sales data that includes a hierarchical index representing year, quarter, and product category, along with the sales figures. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples([\n    (2021, 'Q1', 'Electronics'),\n    (2021, 'Q1', 'Furniture'),\n    (2021, 'Q2', 'Electronics'),\n    (2021, 'Q2', 'Furniture'),\n    (2022, 'Q1', 'Electronics'),\n    (2022, 'Q1', 'Furniture')\n], names=['Year', 'Quarter', 'Category'])\n\ndata = {\n    'Sales': [2000, 1500, 3000, 2500, 3200, 2600]\n}\n\ndf = pd.DataFrame(data, index=index)\n```\n\nNow, I want to normalize the sales figures across the different product categories within each quarter. How can I achieve that using Pandas? Specifically, I want to modify the DataFrame such that each sales figure is expressed as a fraction of the total sales for that quarter, i.e. for each `(Year, Quarter)` combination, the sales for each category should be divided by the total sales for that quarter.\n\nI would do this in a generalizable way using the following code:\n\n```python\nnormalized_sales = ...\n```\n",
    "output": "\n```python\nnormalized_sales = df.groupby(level=['Year', 'Quarter']).transform(lambda x: x / x.sum())\n```"
  },
  {
    "input": "### Problem:\n\nYou are working with a PyTorch tensor that represents the embedding of words in a vocabulary, with a shape of (1000, 128), where 1000 is the number of words and 128 is the dimensionality of their embeddings. You want to find the cosine similarity between a given query embedding and all the embeddings in the vocabulary, then you wish to retrieve the indices of the top 5 most similar words.\n\nThe cosine similarity can be calculated using the formula:\n\n\\[ \\text{similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]\n\nIn your case, you already have the query embedding in a tensor `query_embedding` with shape (128,).\n\nHere\u2019s what you have tried:\n\n```python\nimport torch\n\n# Example tensors\nembeddings = torch.randn(1000, 128)  # Word embeddings\nquery_embedding = torch.randn(128)    # Query embedding\n\n# Normalize the embeddings\nnormalized_embeddings = ...\n```\n\nFill in the blank to compute the cosine similarity, then get the indices of the top 5 most similar words:\n\nA:\n```python\ntop_indices = ...  # put solution in this variable\n```",
    "output": "\n```python\nnormalized_embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\nnormalized_query = query_embedding / query_embedding.norm()\ncosine_similarities = torch.matmul(normalized_embeddings, normalized_query)\ntop_indices = torch.topk(cosine_similarities, 5).indices\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames representing product sales data. The first DataFrame `df_sales` contains information on various products sold, along with their sales dates and quantities, while the second DataFrame `df_inventory` contains the current stock levels of the same products.\n\nHere are the DataFrames:\n\ndf_sales:\n```\n   product_id         sale_date  quantity\n0         A1     2023/01/10        10\n1         A2     2023/01/10        15\n2         A1     2023/01/12         5\n3         A3     2023/01/13         3\n4         A2     2023/01/15        20\n```\n\ndf_inventory:\n```\n   product_id  current_stock\n0         A1             20\n1         A2             10\n2         A3             5\n3         A4             0\n```\n\nI want to create a new DataFrame that lists the total quantity sold for each product (from `df_sales`) alongside the remaining stock from `df_inventory`. If there are duplicate sales for a product, sum those quantities correctly. The DataFrame should include all products from `df_inventory`, and if there are no sales for a product, its sold quantity should be marked as 0. The 'sale_date' should not be part of the output.\n\nThe result should look like this:\n```\n   product_id  total_sold  current_stock\n0         A1          15             20\n1         A2          35             10\n2         A3           3              5\n3         A4           0              0\n```\n\nCurrently, the result generated from my attempts using pd.merge() is not giving me the output I need because the total sold is not being computed. I want to know how to achieve the expected result.\n\nA:\n<code>\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': ['A1', 'A2', 'A1', 'A3', 'A2'],\n    'sale_date': ['2023/01/10', '2023/01/10', '2023/01/12', '2023/01/13', '2023/01/15'],\n    'quantity': [10, 15, 5, 3, 20]\n})\n\ndf_inventory = pd.DataFrame({\n    'product_id': ['A1', 'A2', 'A3', 'A4'],\n    'current_stock': [20, 10, 5, 0]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ntotal_sold = df_sales.groupby('product_id')['quantity'].sum().reset_index()\ntotal_sold.columns = ['product_id', 'total_sold']\nresult = pd.merge(df_inventory, total_sold, on='product_id', how='left').fillna(0)\nresult['total_sold'] = result['total_sold'].astype(int)\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset of customer transactions and want to derive meaningful features from the timestamps of these transactions. Your goal is to extract features such as the hour of the day, day of the week, and whether the transaction occurred on a weekend. You have a dataframe called `transactions` with a column named `timestamp` in datetime format. \n\nYou need to create a new dataframe that contains the original timestamps along with these three new features. Fill in the blank with a function that achieves this transformation using pandas.\n\nA:\n<code>\nimport pandas as pd\n\n# Sample transaction data\ndata = {'timestamp': pd.to_datetime(['2022-01-01 08:23', '2022-01-02 14:45', '2022-01-03 18:12'])}\ntransactions = pd.DataFrame(data)\n\ndef extract_features(df):\n    # Add the hour of the day, day of the week, and weekend flag\n    df['hour'] = df['timestamp'].dt.hour\n    df['day_of_week'] = df['timestamp'].dt.day_name()\n    df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5\n    return df\n\n# Fill in the blank\nnew_transactions = extract_features(transactions)\n</code>\nnew_transactions = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nnew_transactions = extract_features(transactions)  \n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset containing information about various houses, and I need to encode a categorical feature called 'Neighborhood' into numerical values to use in a linear regression model. The dataset looks like this:\n\n```\n   Size  Bedrooms  Neighborhood\n0   1500        3        Suburban\n1   2000        4        Urban\n2   1200        2        Suburban\n3   1800        3        Rural\n4   2400        5        Urban\n```\n\nI want to create a one-hot encoding for the 'Neighborhood' feature while keeping the original columns intact. After encoding, the DataFrame should include new columns for each neighborhood without dropping the original 'Neighborhood' column.\n\nA:\n\n<code>\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Size': [1500, 2000, 1200, 1800, 2400],\n    'Bedrooms': [3, 4, 2, 3, 5],\n    'Neighborhood': ['Suburban', 'Urban', 'Suburban', 'Rural', 'Urban']\n}\ndf = pd.DataFrame(data)\n\n# One-hot encoding of the 'Neighborhood' column\nencoded_df = df.copy()\n</code>\nencoded_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nencoded_df = pd.get_dummies(df, columns=['Neighborhood'], drop_first=False)\n</code>"
  },
  {
    "input": "Problem:\n\nYou have a multi-index Pandas DataFrame representing sales data for different products across various regions and years. The DataFrame, named `sales_df`, looks like this:\n\n```\nsales_df = pd.DataFrame({\n    'ProductA': [100, 150, 200],\n    'ProductB': [80, 120, 160],\n    'ProductC': [90, 110, 140]\n}, index=pd.MultiIndex.from_tuples([\n    ('North', 2021),\n    ('North', 2022),\n    ('South', 2021)\n], names=['Region', 'Year']))\n```\n\nYour task is to normalize the sales data, so each product's sales figures are transformed to be between 0 and 1 based on the max value within its product category across all regions and years. You want to store the normalized DataFrame into a variable called `normalized_sales`.\n\nHow can you achieve this?\n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'ProductA': [100, 150, 200],\n    'ProductB': [80, 120, 160],\n    'ProductC': [90, 110, 140]\n}, index=pd.MultiIndex.from_tuples([\n    ('North', 2021),\n    ('North', 2022),\n    ('South', 2021)\n], names=['Region', 'Year']))\n</code>\nnormalized_sales = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nnormalized_sales = sales_df / sales_df.max()"
  },
  {
    "input": "Problem:\nI have a DataFrame containing sales data for different products across multiple stores. Here is the DataFrame:\n\n```\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'store': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'B', 'C'],\n    'product': ['Apple', 'Banana', 'Apple', 'Banana', 'Banana', 'Apple', 'Apple', 'Apple', 'Banana'],\n    'sales': [10, np.nan, 15, 12, 5, np.nan, 8, 20, np.nan]\n}\ndf = pd.DataFrame(data)\n```\n\nI want to compute the total sales for each product, but if any of the sales values for a product in a store are NaN, I want the total for that product to also be NaN. I can achieve this using groupby and an aggregate function, but I'm not sure how to handle the NaNs correctly.\n\nThe expected result should look like this:\n\n```\nproduct\nApple    NaN\nBanana   NaN\nName: sales, dtype: float64\n```\n\nHow can I write the code to achieve this?\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby('product')['sales'].agg(lambda x: x.sum() if x.isnull().any() == False else np.nan)\n</code>"
  },
  {
    "input": "Problem:\nYou are given a DataFrame containing sales data for different products over various months, structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product_A', 'Q1'): [200, 250, 300],\n    ('Product_A', 'Q2'): [220, 240, 320],\n    ('Product_B', 'Q1'): [150, 175, 200],\n    ('Product_B', 'Q2'): [180, 190, 210],\n}\n\nindex = ['January', 'February', 'March']\ndf = pd.DataFrame(data, index=index)\n```\n\nYou need to normalize the sales data for each product (column-wise normalization) and then create a new DataFrame that retains the original multi-index structure while also appending a new row at the bottom with the normalized sums for each product across all months. How can you accomplish this?\n\nA:\n<code>\ndf_normalized = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf_normalized = df.div(df.sum(axis=0), axis=1)\nnormalized_sums = df_normalized.sum(axis=0).to_frame().T\nnormalized_sums.index = ['Normalized_Sum']\ndf_normalized = pd.concat([df_normalized, normalized_sums])\n</code>"
  },
  {
    "input": "Problem:\nYou are working on a machine learning project using PyTorch. You have two tensors representing feature vectors and predicted classes from your model. The feature tensor is `features` of shape `(32, 128)` representing 32 samples with 128 features each, and the predicted class tensor is `predictions` of shape `(32,)`. You want to create a tensor `mask` that contains `True` for each instance where the predicted class belongs to a specific target class defined as `target_class = 5`. \n\nYour task is to fill in the code that creates this boolean mask.\n\nA:\n<code>\nimport torch\n\nfeatures = torch.rand((32, 128))\npredictions = torch.randint(0, 10, (32,))\ntarget_class = 5\n</code>\nmask = ... # put solution in this variable",
    "output": "\n<code>\nmask = predictions == target_class\n</code>"
  },
  {
    "input": "## Problem\nYou have the following Pandas DataFrame representing sales data for different products over various months:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'product': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B'],\n    'month': ['2023-01', '2023-01', '2023-02', '2023-01', '2023-02', '2023-03', '2023-03', '2023-03'],\n    'sales': [200, 150, np.nan, 300, 200, np.nan, 250, np.nan]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to calculate the total sales for each product while treating the months with `NaN` sales values as a zero. Additionally, if a product has no sales recorded for any month, it should be included in the result with a total sales of zero.\n\nYou aim to achieve a DataFrame that shows total sales per product like this:\n\n```\n  product  total_sales\n0       A         450.0\n1       B         350.0\n2       C         300.0\n```\n\nTo do this, you need to group by the 'product' and sum the sales, replacing `NaN` with zero. \n\nYour solution should be placed in the following blank variable:\n\n```python\nresult = ...  # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby('product')['sales'].sum().fillna(0).reset_index(name='total_sales')\n```"
  },
  {
    "input": "Problem:\n\nI have a PyTorch tensor representing the embeddings of a batch of images, with a shape of [64, 128], where 64 is the batch size and 128 is the embedding dimension. I also have a tensor of boolean masks of shape [64] indicating whether each embedding is valid or not. I want to calculate the mean of only the valid embeddings (i.e., where the mask is True). How can I do this efficiently while ignoring the invalid ones?\n\nA:\n<code>\nimport torch\n\nembeddings = torch.randn(64, 128)  # Randomly generated embeddings\nmask = torch.tensor([True, False, True, True, False, True, False, True, True, True, \n                     False, True, True, False, True, False, True, True, False, \n                     True, True, False, False, True, True, False, True, \n                     True, False, False, True, True, False, True, \n                     True, False, False, True, True, False, \n                     True, True, True, True, False, True, \n                     False, True, True, False, True, False, \n                     False, True, True, True, False, True, \n                     True, True, True, False, True, True], dtype=torch.bool)\n\n# Compute the mean of valid embeddings \nmean_valid_embeddings = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmean_valid_embeddings = embeddings[mask].mean(dim=0)\n</code>"
  },
  {
    "input": "### Problem:\nI have a dataset containing information about movies with features such as title, genre, and user ratings from different platforms. I'd like to preprocess this dataset to prepare it for machine learning. Specifically, I want to one-hot encode the genre column and scale the user ratings between 0 and 1. \n\nHere's a sample of my DataFrame:\n\n+-----------------------+----------------+--------------+\n| title                 | genre          | user_rating  |\n+-----------------------+----------------+--------------+\n| The Shawshank Redemption | Drama       | 9.3          |\n| The Godfather         | Crime, Drama   | 9.2          |\n| The Dark Knight       | Action, Crime   | 9.0          |\n| Schindler's List      | Biography, Drama | 8.9         |\n| Pulp Fiction          | Crime, Drama   | 8.9          |\n+-----------------------+----------------+--------------+\n\nI want to transform the DataFrame to include one-hot encoded columns for each genre and scale the user ratings. The resulting DataFrame should look like this:\n\n+-----------------------+--------------+--------------+----------------+-----------------+\n| title                 | user_rating  | Action       | Biography       | Crime           | Drama          |\n+-----------------------+--------------+--------------+----------------+-----------------+\n| The Shawshank Redemption | 0.9       | 0            | 0              | 0               | 1              |\n| The Godfather         | 0.8          | 0            | 0              | 1               | 1              |\n| The Dark Knight       | 0.7          | 1            | 0              | 1               | 0              |\n| Schindler's List      | 0.6          | 0            | 1              | 0               | 1              |\n| Pulp Fiction          | 0.6          | 0            | 0              | 1               | 1              |\n+-----------------------+--------------+--------------+----------------+-----------------+\n\nYou can assume you have already imported necessary libraries and have the DataFrame `df` loaded with the movie data. Fill in the blank for the transformation step:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n\n# Sample DataFrame\ndata = {\n    'title': ['The Shawshank Redemption', 'The Godfather', 'The Dark Knight', 'Schindler\\'s List', 'Pulp Fiction'],\n    'genre': ['Drama', 'Crime, Drama', 'Action, Crime', 'Biography, Drama', 'Crime, Drama'],\n    'user_rating': [9.3, 9.2, 9.0, 8.9, 8.9]\n}\ndf = pd.DataFrame(data)\n\n# Begin transformation\nencoder = OneHotEncoder(sparse=False)\nscaler = MinMaxScaler()\n\n# _______ (Fill in the blank with the correct transformation code)\n\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\n# One-hot encode the genre column\ngenre_encoded = encoder.fit_transform(df['genre'].str.get_dummies(sep=', '))\ngenre_df = pd.DataFrame(genre_encoded, columns=encoder.get_feature_names_out(['genre']))\n\n# Scale the user ratings\ndf['user_rating'] = scaler.fit_transform(df[['user_rating']])\n\n# Concatenate the original DataFrame with the one-hot encoded genres\ndf = pd.concat([df[['title', 'user_rating']], genre_df], axis=1)\n</code>\n```"
  },
  {
    "input": "Problem:\n\nYou have a PyTorch tensor representing the scores from a recommendation engine with shape [100, 5], where each score indicates the predicted affinity of a user for a specific item. You also have a boolean mask tensor of the same shape, indicating which scores should be considered for determining the maximum score. Your goal is to obtain a tensor containing the indices of the maximum scores, but only for elements where the corresponding boolean mask is True.\n\nGiven the following tensor for scores and a boolean mask:\n\nA:\n<code>\nimport torch\n\nscores = torch.tensor([[0.1, 0.5, 0.3, 0.9, 0.2],\n                       [0.2, 0.8, 0.4, 0.7, 0.5],\n                       [0.9, 0.4, 0.1, 0.2, 0.3]] * 33 + [[0.2, 0.1, 0.9, 0.4, 0.5]])\nmask = torch.tensor([[False, True, False, True, False],\n                     [True, True, False, False, False],\n                     [True, False, False, False, True]] * 33 + [[False, False, True, False, True]])\n</code>\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = torch.argmax(scores * mask.float(), dim=1)"
  },
  {
    "input": "Problem:\nYou are working with a multi-indexed DataFrame that represents sales data across different regions and products, structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Region A', 'Product 1'): [100, 120, 130],\n    ('Region A', 'Product 2'): [80, 90, 110],\n    ('Region B', 'Product 1'): [150, 160, 170],\n    ('Region B', 'Product 2'): [110, 100, 120],\n}\nindex = pd.date_range('2023-01-01', periods=3, freq='D')\ndf = pd.DataFrame(data, index=index)\n```\n\nNow, you would like to normalize the sales data for each product across all regions to a range between 0 and 1. Your goal is to create a new DataFrame where each sales value is replaced by its respective normalized value according to its product.\n\nHow can you use Pandas to achieve this normalization?\n\nA:\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assume df has been created as given above\nscaler = MinMaxScaler()\nnormalized_data = ...  # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nnormalized_data = pd.DataFrame(scaler.fit_transform(df.T), index=df.columns, columns=df.index).T\nEND SOLUTION\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset of housing prices with features such as square footage, number of bedrooms, and age of the house. Before fitting a linear regression model, you need to handle missing values by filling them with the median of each respective feature. Also, you must scale the features to a standard normal distribution (mean=0, standard deviation=1). Given the following DataFrame `df`, which already contains missing values:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndata = {\n    'square_footage': [1500, 1600, None, 1800, 2000, None],\n    'num_bedrooms': [3, 3, 4, None, 4, 5],\n    'age': [10, 15, 20, 25, None, 30]\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to fill in the missing values and scale the features. Complete the following code snippet to achieve that:\n\n```python\n# Fill missing values with the median of each column\ndf.fillna(df.median(), inplace=True)\n\n# Scale the features\nscaler = StandardScaler()\nscaled_features = __________\n```\n\nA: \n<code>\nscaled_features = scaler.fit_transform(df)\n</code>",
    "output": "\n<code>\nscaled_features = scaler.fit_transform(df)\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame representing sales data for different products in different regions. The DataFrame looks like this:\n\n```python\nimport pandas as pd\ndata = {\n    'Product': ['A', 'B', 'A', 'B', 'C', 'C', 'A'],\n    'Region': ['North', 'South', 'East', 'West', 'North', 'South', 'West'],\n    'Sales': [100, None, 200, 300, None, 400, 150]\n}\ndf = pd.DataFrame(data)\n```\n\nI want to calculate the total sales for each product, but I need to replace any missing sales values (NaN) with the average sales for that product before computing the total. \n\nThe final result should be a new DataFrame that shows the total sales for each product:\n\n```python\n  Product  Total_Sales\n0       A         450.0\n1       B         300.0\n2       C         400.0\n```\n\nHow can I fill in the following line to achieve this?\n\n```python\nresult = df.groupby('Product').agg({'Sales': lambda x: x.fillna(x.mean()).sum()}).reset_index()\n```\nFill in the blank:\n```python\ndf['Sales'] = df['Sales'] ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>df['Sales'] = df['Sales'].fillna(df.groupby('Product')['Sales'].transform('mean'))</code>",
    "output": "\n<code>df['Sales'] = df['Sales'].fillna(df.groupby('Product')['Sales'].transform('mean'))</code>"
  }
]