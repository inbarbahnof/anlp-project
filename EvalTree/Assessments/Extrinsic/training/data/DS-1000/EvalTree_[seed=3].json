[
  {
    "input": "Problem:\nI have a dataset of customer orders represented as a pandas DataFrame with a MultiIndex consisting of 'customer_id' and 'order_date'. Each order has multiple items with 'product_id' and 'quantity'. I want to normalize the 'quantity' columns for each customer across their orders. Here is a snippet of the DataFrame:\n\n```\n                    product_id  quantity\ncustomer_id order_date                      \n1           2021-01-01        A        5\n            2021-01-01        B        2\n            2021-01-02        A        3\n2           2021-01-01        A        4\n            2021-01-02        B        6\n            2021-01-02        C        1\n```\n\nThe goal is to create a new DataFrame where the 'quantity' is normalized per customer, meaning the quantities for each customer should sum to 1. The resulting DataFrame should keep the same MultiIndex structure.\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'product_id': ['A', 'B', 'A', 'A', 'B', 'C'],\n    'quantity': [5, 2, 3, 4, 6, 1]\n}\nindex = pd.MultiIndex.from_tuples(\n    [(1, '2021-01-01'), (1, '2021-01-01'), (1, '2021-01-02'), \n     (2, '2021-01-01'), (2, '2021-01-02'), (2, '2021-01-02')],\n    names=['customer_id', 'order_date']\n)\ndf = pd.DataFrame(data, index=index)\n</code>\nnormalized_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nnormalized_df = df.groupby(level='customer_id').apply(lambda x: x['quantity'] / x['quantity'].sum()).reset_index(level=0, drop=True)  \nnormalized_df = df[['product_id']].join(normalized_df.rename('quantity'))  \n</code>"
  },
  {
    "input": "Problem:\n\nI'm working with a PyTorch model that outputs a tensor of shape (batch_size, features). I want to find the cosine similarity between each pair of feature vectors within the same batch. However, I need to ignore any features that correspond to a masked value that I've defined. The mask is a boolean tensor of the same shape, where `True` indicates valid entries to be considered in the similarity computation, and `False` indicates masked values.\n\nGiven a tensor `X` and its corresponding boolean mask `mask`, how can I compute the cosine similarity for the valid entries while ignoring the masked values? The resulting tensor should have a shape of (batch_size, batch_size) representing the cosine similarity between valid feature vectors.\n\nExample input:\n```python\nX = torch.tensor([[1.0, 0.0, 2.0],\n                  [2.0, 3.0, 1.0],\n                  [0.0, 3.0, 0.0]])\nmask = torch.tensor([[True, False, True],\n                     [True, True, True],\n                     [False, True, False]])\n```\n\nA:\n\n<code>\nimport torch\n\nX = torch.tensor([[1.0, 0.0, 2.0],\n                  [2.0, 3.0, 1.0],\n                  [0.0, 3.0, 0.0]])\nmask = torch.tensor([[True, False, True],\n                     [True, True, True],\n                     [False, True, False]])\n</code>\nsimilarity = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nimport torch.nn.functional as F\n\ndef masked_cosine_similarity(X, mask):\n    # Apply the mask to X\n    masked_X = X * mask.float()\n    \n    # Normalize the masked feature vectors\n    norms = torch.norm(masked_X, dim=1, keepdim=True)\n    normalized_X = masked_X / norms\n    \n    # Compute cosine similarity\n    similarity = torch.mm(normalized_X, normalized_X.t())\n    \n    # Create a mask for the similarity matrix\n    similarity_mask = mask.float() @ mask.float().t()\n    \n    # Zero out the masked entries in the similarity matrix\n    similarity *= similarity_mask\n    \n    return similarity\n\nsimilarity = masked_cosine_similarity(X, mask)\n</code>"
  },
  {
    "input": "Problem:\n\nI have a tensor representing a set of embeddings for documents and I need to calculate the cosine similarity matrix between these document embeddings. The embeddings tensor looks like this:\n\n```python\ntorch.FloatTensor([[0.1, 0.2, 0.3],\n                   [0.4, 0.5, 0.6],\n                   [0.7, 0.8, 0.9]])\n```\n\nI'll then use a boolean mask to filter out the pairs whose similarity is below a certain threshold (e.g., 0.9). The output should include the indices of the document pairs that meet this criterion.\n\nHow can I accomplish this? \n\nA:\n<code>\nimport torch\n\nembeddings = torch.FloatTensor([[0.1, 0.2, 0.3],\n                                 [0.4, 0.5, 0.6],\n                                 [0.7, 0.8, 0.9]])\n</code>\nsimilarity_matrix = ... # put solution in this variable \nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_matrix = cosine_similarity(embeddings)\nthreshold = 0.9\nmask = similarity_matrix > threshold\nindices = torch.nonzero(mask, as_tuple=True)\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame representing sales data for a retail store, structured like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['A', 'B', 'A', 'B', 'C'],\n    'store': ['X', 'X', 'Y', 'Y', 'X'],\n    'sales': [100, 200, 150, 300, 250],\n    'date': pd.to_datetime(['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02', '2021-01-01'])\n}\ndf = pd.DataFrame(data)\n```\n\nI need to calculate the total sales for each product in each store, and then normalize these totals across all products for each store. The expected output should have a multi-index with levels for 'store' and 'product', along with the normalized sales.\n\nHow can I achieve this? \n\nThe result should look like this:\n\n```\n                sales  normalized_sales\nstore product                         \nX     A      100           normalized_value_A\n      B      200           normalized_value_B\n      C      250           normalized_value_C\nY     A      150           normalized_value_A\n      B      300           normalized_value_B\n```\n\nWhere `normalized_value_X` are the normalized sales values for each product. \n\nPlease fill in the missing code to accomplish this normalization.\n\nresult = ... # put solution in this variable",
    "output": "\n```python\nresult = df.groupby(['store', 'product'])['sales'].sum().reset_index()\nresult['normalized_sales'] = result.groupby('store')['sales'].transform(lambda x: x / x.sum())\nresult = result.set_index(['store', 'product'])\n```"
  },
  {
    "input": "Problem:\nI have two Pandas DataFrames containing sales data from different regions, and I'm trying to consolidate them while ensuring data integrity. Here are the DataFrames:\n\nDataFrame A:\n```\nProductID     Sales\n-------------------\nA001          150\nA002          200\nA003          175\nA001          100  # Duplicate for A001\n```\n\nDataFrame B:\n```\nProductID     Sales\n-------------------\nA001          120\nA002          240\nA004          300\n```\n\nI want to create a single DataFrame that:\n1. Merges both DataFrames on 'ProductID'.\n2. Keeps the highest sales value if duplicates exist in the merged result.\n3. Ensures the final DataFrame is sorted by 'ProductID'.\n\nThe final DataFrame should look like this:\n```\nProductID     Sales\n-------------------\nA001          150\nA002          240\nA003          175\nA004          300\n```\n\nHow can I achieve this with Pandas? \n\nA:\n<code>\nimport pandas as pd\n\ndf_a = pd.DataFrame({'ProductID': ['A001', 'A002', 'A003', 'A001'],\n                     'Sales': [150, 200, 175, 100]})\n\ndf_b = pd.DataFrame({'ProductID': ['A001', 'A002', 'A004'],\n                     'Sales': [120, 240, 300]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = pd.concat([df_a, df_b]).groupby('ProductID', as_index=False).max().sort_values('ProductID')"
  },
  {
    "input": "## Problem:\n\nYou have two tensors, `A` and `B`, representing feature vectors for a binary classification task. The shapes of these tensors are `(50, 128)` each, where `50` is the number of samples and `128` is the feature dimension. You want to compute a similarity score for each pair of vectors from `A` and `B` using the cosine similarity formula, and return a mask indicating which pairs have a similarity score greater than a given threshold, say `0.8`.\n\nFor the cosine similarity, you need to perform element-wise operations to compute the dot products and magnitudes. Your initial attempt using PyTorch's operations has been partially completed.\n\nHere is the beginning of your code:\n\n```python\nimport torch\n\nA = load_data_A()  # Shape: (50, 128)\nB = load_data_B()  # Shape: (50, 128)\nthreshold = 0.8\n\n# Compute the dot products\ndot_product = torch.mm(A, B.t())\n\n# Compute magnitudes\nnorm_A = torch.norm(A, dim=1).unsqueeze(1)\nnorm_B = torch.norm(B, dim=1).unsqueeze(0)\n\n# Compute cosine similarity\ncosine_similarity = dot_product / (norm_A * norm_B)\n\n# Now, you want to create a boolean mask based on the threshold\nmask = ...\n```\n\nYou need to fill in the blank to produce the `mask` tensor that has the same shape as `cosine_similarity` but contains `True` for elements greater than `0.8` and `False` otherwise.\n\nA:\n\n```python\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nmask = cosine_similarity > threshold\n```"
  },
  {
    "input": "Problem:\nI am working with a dataset containing information about houses, with features like size, number of rooms, and age. I want to preprocess this data before training a machine learning model. Specifically, I need to one-hot encode the categorical feature 'location' and normalize the 'size' feature. After applying the transformations, I want to create a new DataFrame with the transformed features ready for modeling.\n\nHere is my example DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'size': [1500, 2000, 1700, 2500, 1800],\n    'num_rooms': [3, 4, 3, 5, 4],\n    'age': [10, 20, 15, 5, 12],\n    'location': ['urban', 'suburban', 'urban', 'rural', 'suburban']\n}\ndf = pd.DataFrame(data)\n```\n\nI want to fill in the code that applies the transformations and creates a transformed DataFrame called `transformed_df`. \n\nThe transformation steps are:\n1. One-hot encode the 'location' column.\n2. Normalize the 'size' column so that it has a mean of 0 and a standard deviation of 1.\n\nA:\n<code>\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Your code here\nencoder = OneHotEncoder()\nscaler = StandardScaler()\n\n# Fit and transform the necessary columns\nlocation_encoded = encoder.fit_transform(df[['location']]).toarray()\nsize_normalized = scaler.fit_transform(df[['size']])\n\n# Combine the transformed features with the original numeric columns\ntransformed_df = pd.DataFrame(data=size_normalized, columns=['size_normalized'])\ntransformed_df = pd.concat([transformed_df, df[['num_rooms', 'age']].reset_index(drop=True), \n                            pd.DataFrame(location_encoded, columns=encoder.get_feature_names_out(['location']))], axis=1)\n\ntransformed_df\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\n\nlocation_encoded = encoder.fit_transform(df[['location']]).toarray()\nsize_normalized = scaler.fit_transform(df[['size']])\n\ntransformed_df = pd.DataFrame(data=size_normalized, columns=['size_normalized'])\ntransformed_df = pd.concat([transformed_df, df[['num_rooms', 'age']].reset_index(drop=True), \n                            pd.DataFrame(location_encoded, columns=encoder.get_feature_names_out(['location']))], axis=1)\n\ntransformed_df\n</code>"
  },
  {
    "input": "### Problem:\n\nI have a batch of images defined as a PyTorch tensor with shape `(batch_size, channels, height, width)`. Each image has been processed to obtain feature maps, and I want to apply a boolean mask to extract specific features based on a condition. The tensor is as follows:\n\n```python\nimport torch\n\nfeature_maps = torch.tensor([[[[0.1, 0.2], [0.3, 0.4]], \n                               [[0.5, 0.6], [0.7, 0.8]]], \n                              [[[1.0, 1.2], [1.3, 1.4]], \n                               [[1.5, 1.6], [1.7, 1.8]]]])\n```\n\nTo filter the feature maps, I need a mask that selects values greater than `0.5`. I already have the tensor defined, but I need help applying the boolean mask to get a tensor that only includes the values satisfying the condition. The desired output should still maintain the structure of the original tensor but have the values below `0.5` replaced by zeros.\n\nFill in the blank with the correct code needed for the masking operation:\n\n```python\nmask = feature_maps > 0.5\nmasked_feature_maps = ... # put solution in this variable\n```\n",
    "output": "\n```python\nmasked_feature_maps = feature_maps * mask\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame representing customer orders at a cafe as shown below. The orders may have missing values (NaN). You want to create a new column that calculates the total cost for each order based on the quantity of items ordered and their respective prices, but you should treat NaN as zero for the calculations. The DataFrame is defined as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Customer': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Coffee': [2, 1, np.nan, 3],\n    'Tea': [1, np.nan, 2, 1],\n    'Pastries': [np.nan, 5, 3, np.nan],\n    'Price_Coffee': [2.50, 2.50, 2.50, 2.50],\n    'Price_Tea': [2.00, 2.00, 2.00, 2.00],\n    'Price_Pastries': [3.00, 3.00, 3.00, 3.00]\n})\n```\n\nNext, you need to calculate the total cost as follows:\n- `Total_Cost` should be computed as:\n  \n  Total_Cost = (Coffee Quantity \u00d7 Price_Coffee) + (Tea Quantity \u00d7 Price_Tea) + (Pastries Quantity \u00d7 Price_Pastries)\n\nYour goal is to fill in the blank with the appropriate code to achieve this.\n\n```python\ndf['Total_Cost'] = df.apply(lambda row: row['Coffee'] * row['Price_Coffee'] + row['Tea'] * row['Price_Tea'] + row['Pastries'] * row['Price_Pastries'], axis=1)\n```\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Customer': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Coffee': [2, 1, np.nan, 3],\n    'Tea': [1, np.nan, 2, 1],\n    'Pastries': [np.nan, 5, 3, np.nan],\n    'Price_Coffee': [2.50, 2.50, 2.50, 2.50],\n    'Price_Tea': [2.00, 2.00, 2.00, 2.00],\n    'Price_Pastries': [3.00, 3.00, 3.00, 3.00]\n})\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\ndf['Total_Cost'] = df.apply(lambda row: row['Coffee'] * row['Price_Coffee'] + row['Tea'] * row['Price_Tea'] + row['Pastries'] * row['Price_Pastries'] if pd.notna(row['Coffee']) and pd.notna(row['Tea']) and pd.notna(row['Pastries']) else 0, axis=1)\n```"
  },
  {
    "input": "## Problem:\n\nYou have a batch of embeddings represented as a 2D tensor with shape `[batch_size, embedding_dim]`. Let's say the embeddings tensor is:\n\n```python\nembeddings = torch.rand((64, 128))  # 64 samples with 128-dimensional embeddings\n```\n\nYou also have a tensor of dot products with a shape of `[64, 64]` representing similarity scores between these embeddings.\n\nYour task is to apply a threshold of 0.5 to create a binary mask where values greater than or equal to the threshold are marked with 1, and those below are marked with 0. Additionally, you should ensure that there are no self-comparisons in the similarity scores (i.e., set the diagonal to 0).\n\nHow can you generate the binary mask tensor from the similarity scores tensor? \n\nPlease fill in the following variable with your solution:\n\n```python\nsimilarity_scores = torch.rand((64, 64))  # Sample similarity scores\n```\n\nA:\n\n```python\nembeddings = torch.rand((64, 128))\nsimilarity_scores = ...  # put solution in this variable\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nsimilarity_scores = torch.rand((64, 64))  # Sample similarity scores\nsimilarity_scores.fill_diagonal_(0)  # Set diagonal to 0 for no self-comparisons\nmask = (similarity_scores >= 0.5).int()  # Create binary mask with threshold\n```"
  },
  {
    "input": "Problem:\nI have a sales dataset represented in a pandas DataFrame with the following columns: 'OrderID', 'Product', 'Quantity', 'Price', 'Discount', and 'Date'. Some of the rows in the 'Discount' column contain null values. I want to compute the total sales for each product while applying a discount of 10% where the 'Discount' is null and using the given value when it's not null. Here's a snippet of my DataFrame:\n\n```\n   OrderID   Product  Quantity  Price  Discount       Date\n0        1  Widget A        2   20.0      NaN  2023-01-01\n1        2  Widget B        1   30.0      5.0  2023-01-02\n2        3  Widget A        4   20.0      NaN  2023-01-03\n3        4  Widget C        5   15.0      3.0  2023-01-04\n4        5  Widget B        3   30.0      NaN  2023-01-05\n```\n\nAfter performing the necessary calculations, I want to get the resulting DataFrame that shows the total sales for each product. How can I achieve this?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'OrderID': [1, 2, 3, 4, 5],\n    'Product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],\n    'Quantity': [2, 1, 4, 5, 3],\n    'Price': [20.0, 30.0, 20.0, 15.0, 30.0],\n    'Discount': [None, 5.0, None, 3.0, None],\n    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']\n}\ndf = pd.DataFrame(data)\n</code>\ntotal_sales = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['Discount'] = df['Discount'].fillna(0.1 * df['Price'])\ndf['TotalSales'] = df['Quantity'] * df['Price'] * (1 - df['Discount'] / 100)\ntotal_sales = df.groupby('Product')['TotalSales'].sum().reset_index()\n</code>"
  },
  {
    "input": "Problem:\nI have a dataset containing information about customers and their purchasing behavior. I want to preprocess this data to better analyze the factors influencing their purchase decisions. Specifically, I'd like to fill missing values in the 'Income' column with the median income of the age group the customer belongs to. Assuming I have a pandas DataFrame `df` that includes 'Age' and 'Income' columns, how can I implement this column-wise transformation using the 'Age' groups (e.g., 18-25, 26-35, etc.) to fill the missing 'Income' values? \n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'Age': [22, 29, 35, 22, 44, 29, 19, 38, 45, 26],\n    'Income': [30000, None, 50000, 32000, None, None, 28000, 60000, 70000, 35000]\n}\ndf = pd.DataFrame(data)\n\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nage_bins = [18, 25, 35, 45, 100]\nage_labels = ['18-25', '26-35', '36-45', '46+']\ndf['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n\nmedian_income = df.groupby('AgeGroup')['Income'].transform('median')\ndf['Income'] = df['Income'].fillna(median_income)\nresult = df\n</code>"
  },
  {
    "input": "Problem:\nI have two dataframes that contain a list of products and their sales over time. The first dataframe `df_sales` contains columns for `product_id`, `date`, and `sales_amount`, while the second dataframe `df_details` contains columns for `product_id`, `product_name`, and `category`. I need to merge them based on `product_id`, ensuring that we also handle any missing product names from `df_details`. After that, I want to create a new column `sales_status` that categorizes sales into 'High' if sales are greater than 1000, 'Medium' if sales are between 500 and 1000, and 'Low' for sales below 500. Finally, I would like the resultant dataframe to be sorted by `date` in ascending order.\n\nHere are the two dataframes:\n\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3, 2],\n    'date': ['2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03'],\n    'sales_amount': [1000, 2000, 300, 500]\n})\n\ndf_details = pd.DataFrame({\n    'product_id': [1, 2, 3],\n    'product_name': ['Product A', 'Product B', None],\n    'category': ['Category 1', 'Category 2', 'Category 1']\n})\n```\n\nAfter merging, how can I generate the final dataframe as described? Use the following placeholder for the result:\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df_sales.merge(df_details, on='product_id', how='left')\nresult['sales_status'] = pd.cut(result['sales_amount'], bins=[-1, 500, 1000, float('inf')], labels=['Low', 'Medium', 'High'])\nresult = result.sort_values(by='date')\n</code>"
  },
  {
    "input": "### Problem\n\nYou have been given a 3D tensor representing image data in the shape (batch_size, channels, height, width). You want to apply a boolean mask to filter out specific pixels across all images based on a condition. The condition is that pixel values should be greater than a certain threshold. The remaining values in the tensor should be set to zero. Write a function that takes the tensor and the threshold as inputs, applies the mask, and returns the modified tensor.\n\nExample input:\n```python\nimport torch\n\ntensor = torch.tensor([[[0.1, 0.5, 0.3],\n                         [0.7, 0.2, 0.6]],\n                        \n                        [[0.4, 0.9, 0.1],\n                         [0.8, 0.3, 0.5]]])\nthreshold = 0.5\n```\nThe desired output for the input above, after applying the threshold mask, should look like this:\n```python\ntensor([[ [0.0, 0.0, 0.0],\n          [0.7, 0.0, 0.6]],\n          \n         [ [0.0, 0.9, 0.0],\n          [0.8, 0.0, 0.5]]])\n```\n\nA:\n\n<code>\nimport torch\n\ntensor, threshold = load_data()\n</code>\nresult_tensor = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult_tensor = torch.where(tensor > threshold, tensor, torch.tensor(0.0))\n</code>"
  },
  {
    "input": "Problem:\nYou are working with a DataFrame that contains sales data for various products across different regions. The DataFrame consists of product names, regions, sales figures, and dates. Your task is to create a pivot table from this DataFrame where the index comprises the unique product names, the columns represent regions, and the values are the total sales figures for each product by region. Within the pivot table, you want to fill any missing sales figures with zero.\n\nGiven the following code snippet, how would you fill in the blank to achieve this?\n\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget B', 'Widget A'],\n    'region': ['North', 'South', 'South', 'North', 'East'],\n    'sales': [100, 150, 200, 300, 100],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'])\n}\ndf = pd.DataFrame(data)\n\npivot_table = df.pivot_table(index='product', columns='region', values='sales', aggfunc='sum').fillna(0)\n\nresult = ...\n```\n",
    "output": "\n```python\nresult = pivot_table\n```"
  },
  {
    "input": "## Problem\nYou have a dataset containing sales information from a retail company stored in a pandas DataFrame. The DataFrame includes information on the `store_id`, `category`, and `sales` for each transaction, like so:\n\n```python\nimport pandas as pd\n\ndata = {\n    'store_id': ['store_1', 'store_1', 'store_2', 'store_2', 'store_1', 'store_3'],\n    'category': ['A', 'B', 'A', 'B', 'A', 'C'],\n    'sales': [100, 150, 200, 300, 250, 400]\n}\ndf = pd.DataFrame(data)\n```\n\nNow, you want to normalize the `sales` values within each `store_id` across `category`, turning the `sales` values into their corresponding z-scores. Finally, you would like the DataFrame structured such that it retains the `store_id` and `category`, while also displaying a new row for the z-scores below the original sales data. The resulting DataFrame should look like:\n\n```\n          store_id category  sales\n0         store_1       A    100\n1         store_1       B    150\n2         store_2       A    200\n3         store_2       B    300\n4         store_1       A    250\n5         store_3       C    400\n          zscore\n0         ...   \n1         ...\n2         ...\n3         ...\n4         ...\n5         ...\n```\n\nWrite the code to generate the `result` variable where you will perform the normalization and structure the DataFrame as described. \n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nfrom scipy.stats import zscore\n\ndf['zscore'] = df.groupby('store_id')['sales'].transform(zscore)\nresult = pd.concat([df, df[['store_id', 'category', 'zscore']].rename(columns={'zscore': 'sales'})], ignore_index=True)\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a tensor representing images with a shape of (num_images, height, width, channels), where you have a batch of images and their respective confidences. You want to apply a thresholding operation to filter out images with a confidence score below a certain value. \n\nGiven the following tensors:\n\n```python\nconfidences = torch.tensor([[0.9, 0.8, 0.85], [0.5, 0.1, 0.97], [0.65, 0.88, 0.9]])\nimages = torch.rand((3, 128, 128, 3))  # Random images\nthreshold = 0.7 \n```\n\nThe goal is to create a new tensor that retains only the images with confidence scores above the provided threshold. Your solution should apply boolean masking based on the condition.\n\nHow would you fill in the code to achieve this?\n\n```python\nimport torch\n\nconfidences = torch.tensor([[0.9, 0.8, 0.85], [0.5, 0.1, 0.97], [0.65, 0.88, 0.9]])\nimages = torch.rand((3, 128, 128, 3))  # Random images\nthreshold = 0.7\n```\nfiltered_images = ... # put solution in this variable",
    "output": "\n```python\nfiltered_images = images[confidences > threshold]\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset represented as a 3D Tensor in PyTorch with dimensions (batch_size, channels, height). I want to compute a similarity score between each pair of channels within each batch. Specifically, I want to calculate the cosine similarity of each channel with every other channel. The output should be a 2D Tensor containing the similarity scores for each pair of channels per batch.\n\nHere is what I have so far:\n\n```python\nimport torch\n\n# Assume the following is our input tensor\ninput_tensor = torch.randn(4, 3, 5)  # 4 batches, 3 channels, height 5\n```\n\nWhat should I put in place of `...` to compute the cosine similarity across the channels?\n\nA:\n\n```python\nsimilarity_scores = ...  # put solution in this variable\n```",
    "output": "\n```python\nsimilarity_scores = torch.nn.functional.cosine_similarity(input_tensor.unsqueeze(2), input_tensor.unsqueeze(1), dim=3)\n```"
  },
  {
    "input": "Problem:\nI'm using PyTorch for a machine learning task. I have a predictions tensor that contains the softmax scores of shape [50, 5], which corresponds to 50 samples and 5 classes. Additionally, I have a tensor containing the actual class indices, `labels`, of shape [50]. My aim is to compute the loss for these predictions using PyTorch's cross-entropy loss function, which expects raw logits. To achieve this, I need to convert my softmax scores into logits, since cross-entropy cannot directly work with probabilities.\n\nGiven the softmax scores tensor looks like this:\n```python\nimport torch\n\nscores = torch.tensor([[0.1, 0.1, 0.6, 0.1, 0.1],\n                       [0.3, 0.3, 0.1, 0.1, 0.2],\n                       [0.5, 0.1, 0.3, 0.1, 0.0]])\n```\nThe `labels` tensor corresponds to the following actual classes:\n```python\nlabels = torch.tensor([2, 0, 1])\n```\nI want to revert the softmax scores back to logits by applying the appropriate transformation. How would I do that?\n\nA:\n<code>\nimport torch\n\nscores = torch.tensor([[0.1, 0.1, 0.6, 0.1, 0.1],\n                       [0.3, 0.3, 0.1, 0.1, 0.2],\n                       [0.5, 0.1, 0.3, 0.1, 0.0]])\nlabels = torch.tensor([2, 0, 1])\n\ndef f(scores=scores):\n    # return the solution in this function\n    # logits = f(scores)\n    ### BEGIN SOLUTION",
    "output": "\n<code>\n    logits = torch.log(scores) - torch.logsumexp(torch.log(scores), dim=1, keepdim=True)\n    return logits\n### END SOLUTION\n</code>"
  },
  {
    "input": "Problem:\nI have a dataset containing information about customer purchases in a retail store. Each purchase record includes the customer's ID, the product ID, the quantity bought, and the purchase amount. I want to aggregate that data to find the total amount spent by each customer. To prepare for this, I've imported the data into a pandas DataFrame. However, I also want to include only those customers who have spent more than a specified threshold.\n\nMy DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': [1, 2, 3, 1, 2, 3, 1],\n    'product_id': [101, 102, 103, 101, 102, 103, 104],\n    'quantity': [1, 2, 1, 1, 1, 1, 2],\n    'amount': [10.00, 20.00, 15.00, 5.00, 25.00, 15.00, 30.00]\n}\n\ndf = pd.DataFrame(data)\n```\n\nI\u2019ve tried to aggregate the total amount each customer has spent, but I\u2019m unsure how to filter out customers who spent less than a given threshold. Here\u2019s the code I have so far:\n\n```python\nthreshold = 50.00\ntotal_spent = df.groupby('customer_id')['amount'].sum()\n# filter to only include customers over the threshold\nfiltered_customers = total_spent[total_spent > threshold]\n```\n\nI'm looking to complete this code to obtain the resulting DataFrame `filtered_customers`.\n\nA:\n```python\nfiltered_customers = ... # put solution in this variable\n```",
    "output": "\n```python\nfiltered_customers = total_spent[total_spent > threshold].reset_index()\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset containing information about various houses, and I need to encode a categorical feature called 'Neighborhood' into numerical values to use in a linear regression model. The dataset looks like this:\n\n```\n   Size  Bedrooms  Neighborhood\n0   1500        3        Suburban\n1   2000        4        Urban\n2   1200        2        Suburban\n3   1800        3        Rural\n4   2400        5        Urban\n```\n\nI want to create a one-hot encoding for the 'Neighborhood' feature while keeping the original columns intact. After encoding, the DataFrame should include new columns for each neighborhood without dropping the original 'Neighborhood' column.\n\nA:\n\n<code>\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Size': [1500, 2000, 1200, 1800, 2400],\n    'Bedrooms': [3, 4, 2, 3, 5],\n    'Neighborhood': ['Suburban', 'Urban', 'Suburban', 'Rural', 'Urban']\n}\ndf = pd.DataFrame(data)\n\n# One-hot encoding of the 'Neighborhood' column\nencoded_df = df.copy()\n</code>\nencoded_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nencoded_df = pd.get_dummies(df, columns=['Neighborhood'], drop_first=False)\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products across several regions over multiple years. The DataFrame is structured with a multi-index consisting of 'Region' and 'Product', and columns for 'Year', 'Sales', and 'Returns'. Your goal is to normalize the 'Sales' column by converting it to a z-score within each product category across all regions for the year 2020.\n\nConsider the following DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('North', 'Widget A'): {2019: 100, 2020: 150, 2021: 200},\n    ('North', 'Widget B'): {2019: 120, 2020: 110, 2021: 130},\n    ('South', 'Widget A'): {2019: 90, 2020: 160, 2021: 190},\n    ('South', 'Widget B'): {2019: 160, 2020: 180, 2021: 210}\n}\n\ndf = pd.DataFrame(data).T\ndf.index.names = ['Region', 'Product']\ndf.columns.name = 'Year'\n```\n\nYou need to calculate the z-score for 'Sales' in 2020 and store the result in a new column 'Sales_Z'. The resulting DataFrame should keep the original structure with the additional 'Sales_Z' column.\n\nHow can you accomplish this by filling in the blank below?\n\nA:\n```python\ndf['Sales_Z'] = (df.xs(2020, level='Year') - df.xs(2020, level='Year').mean()) / df.xs(2020, level='Year').std()\n```",
    "output": "\n```python\ndf['Sales_Z'] = (df.xs(2020, level='Year')['Sales'] - df.xs(2020, level='Year')['Sales'].mean()) / df.xs(2020, level='Year')['Sales'].std()\n```"
  },
  {
    "input": "Problem:\n\nYou have a pandas DataFrame containing sales data for different products across various regions. The DataFrame is structured with a multi-index composed of 'Region' and 'Product', and contains sales figures for the year 2023. Here\u2019s a sample DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('North', 'A'): [200, 250, 300],\n    ('North', 'B'): [150, 175, 200],\n    ('South', 'A'): [300, 350, 400],\n    ('South', 'B'): [100, 125, 150],\n}\n\nindex = pd.date_range('2023-01-01', periods=3, freq='M')\ndf = pd.DataFrame(data, index=index)\nprint(df)\n```\n\nYour task is to normalize the sales figures within each region so that they sum up to 1 for each month. This requires applying a transformation to the DataFrame to achieve the desired normalization. After applying your transformation, the DataFrame should reflect normalized sales values.\n\nPlease fill in the following code to perform the normalization:\n\n```python\nnormalized_df = df.div(df.sum(axis=1), axis=0)\n```\n\nWhat does the `normalized_df` look like after your code executes? \n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    ('North', 'A'): [200, 250, 300],\n    ('North', 'B'): [150, 175, 200],\n    ('South', 'A'): [300, 350, 400],\n    ('South', 'B'): [100, 125, 150],\n}\n\nindex = pd.date_range('2023-01-01', periods=3, freq='M')\ndf = pd.DataFrame(data, index=index)\n```\nnormalized_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nnormalized_df = df.div(df.sum(axis=1), axis=0)\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a batch of image feature embeddings represented as a tensor in PyTorch, shaped as follows:\n\n```python\nfeatures = torch.tensor([[0.2, 0.8, 0.5],\n                         [0.9, 0.1, 0.4],\n                         [0.2, 0.5, 0.9]])\n```\n\nYou want to create a boolean mask that indicates which embeddings have a maximum value greater than 0.6. The resulting mask should have `True` for those indices and `False` where the condition is not met. How can you achieve this?\n\nA:\n```python\nimport torch\n\nfeatures = torch.tensor([[0.2, 0.8, 0.5],\n                         [0.9, 0.1, 0.4],\n                         [0.2, 0.5, 0.9]])\n```\nmask = ... # put solution in this variable\nBEGIN SOLUTION",
    "output": "\n```python\nmask = (features.max(dim=1).values > 0.6)\n```"
  },
  {
    "input": "## Problem:\n\nI have a set of images in the form of a 4D tensor where the dimensions are (batch_size, channels, height, width). My task is to apply a threshold to the pixel values to create a binary mask of the images, where pixels greater than a specific threshold are set to 1, and those less than or equal to the threshold are set to 0.\n\nGiven a tensor of images `images_tensor` and a threshold value `threshold`, I need to create the binary mask using boolean masking. \n\nThe current setup looks like this:\n\n```python\nimport torch\n\n# Assume images_tensor is your input tensor with shape (batch_size, channels, height, width)\nimages_tensor = load_data()  # example: shape (32, 3, 64, 64)\nthreshold = 0.5\n```\n\nWhat code should I use to generate the binary mask?\n\nA:\n\n```python\nbinary_mask = ... # put solution in this variable\n```",
    "output": "\n```python\nbinary_mask = (images_tensor > threshold).float()\n```"
  },
  {
    "input": "Problem:\n\nYou are tasked with preprocessing a dataset containing information about customer purchases in a retail store. The raw data is represented as a list of dictionaries, where each dictionary contains customer ID and a list of items they purchased, like so:\n\ndata = [\n    {'customer_id': 'C001', 'items': ['item1', 'item2', 'item3']},\n    {'customer_id': 'C002', 'items': ['item2', 'item4']},\n    {'customer_id': 'C003', 'items': ['item1', 'item5']},\n    {'customer_id': 'C001', 'items': ['item2', 'item3']},\n]\n\nTo prepare this data for machine learning, you need to create a one-hot encoded representation of item purchases, where each item becomes a feature. The resulting DataFrame should have customer IDs as the index and one column for each unique item, filling in with 1s for purchases and 0s for non-purchases. \n\nHow can you achieve this using pandas?\n\nA:\n\n<code>\nimport pandas as pd\ndata = load_data()\n</code>\nencoded_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Extracting items from the data\nitems = [d['items'] for d in data]\nmlb = MultiLabelBinarizer()\nencoded_items = mlb.fit_transform(items)\n\n# Creating a DataFrame with customer IDs as index\nencoded_data = pd.DataFrame(encoded_items, index=[d['customer_id'] for d in data], columns=mlb.classes_).groupby(level=0).sum()\n</code>"
  },
  {
    "input": "Problem:\nGiven a pandas DataFrame with sales data for different products, where each product's sales across multiple quarters may contain missing values, your DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'B', 'A', 'C'],\n    'Quarter': ['Q1', 'Q1', 'Q2', 'Q2', 'Q3', 'Q3'],\n    'Sales': [100, np.nan, 200, 150, np.nan, 300]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want a new DataFrame that includes the total sales for each product, but only including quarters where sales are not null. If all sales entries for a product are null, that product's total sales should also be null.\n\nThe desired output should look like this:\n\n```\nProduct  Total_Sales\nA             300.0\nB             150.0\nC             300.0\n```\n\nFill in the blank in the following line of code to achieve this:\n\n```python\nresult = df.groupby('Product')['Sales'].apply(...).reset_index(name='Total_Sales')\n```",
    "output": "\n```python\nresult = df.groupby('Product')['Sales'].sum().reset_index(name='Total_Sales')\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame with sales data structured like this:\n\n| Product  | Q1   | Q2   | Q3   | Q4   |\n|----------|------|------|------|------|\n| Widget A | 150  | 200  | NaN  | 250  |\n| Widget B | 300  | 400  | 350  | NaN  |\n| Widget C | NaN  | NaN  | 600  | 700  |\n| Widget D | 100  | NaN  | 150  | NaN  |\n\nThe DataFrame has been created from a larger dataset that contains quarterly sales figures but has missing values represented by NaN. I want to calculate the total annual sales for each product while ignoring the NaN values for the quarter calculations. \n\nFurthermore, if a product has a total annual sales figure that is below 600, I want to mark that product as 'Low Performer'. Otherwise, it should be marked as 'High Performer'.\n\nThe expected output should look like this:\n\n| Product  | Total  | Performance     |\n|----------|--------|------------------|\n| Widget A | 600    | High Performer    |\n| Widget B | 1050   | High Performer    |\n| Widget C | 1300   | High Performer    |\n| Widget D | 250    | Low Performer     |\n\nHow can I achieve this using pandas? \n\nThe DataFrame creation looks like this:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Product': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n    'Q1': [150, 300, None, 100],\n    'Q2': [200, 400, None, None],\n    'Q3': [None, 350, 600, 150],\n    'Q4': [250, None, 700, None]\n})\n```\n\nPlease fill in the blanks in the following line of code to get the expected result:\n\nresult = ...\n",
    "output": "\n```python\nresult = df.assign(Total=df.iloc[:, 1:].sum(axis=1, skipna=True)).assign(Performance=lambda x: x['Total'].apply(lambda y: 'Low Performer' if y < 600 else 'High Performer'))[['Product', 'Total', 'Performance']]\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data with multi-level indexing for products sold across different regions and quarters of the year. Here\u2019s a sample of the data:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples(\n    [('North', 'Q1'), ('North', 'Q2'), ('North', 'Q3'), ('North', 'Q4'),\n     ('South', 'Q1'), ('South', 'Q2'), ('South', 'Q3'), ('South', 'Q4')],\n    names=['Region', 'Quarter']\n)\ndata = {\n    'Sales': [25000, 27000, 24000, 30000, 15000, 18000, 16000, 20000],\n    'Profit': [5000, 6000, 4000, 7000, 3000, 3500, 2500, 4000]\n}\ndf = pd.DataFrame(data, index=index)\n```\n\nYou want to normalize the \"Sales\" column for each region while keeping the DataFrame structure intact. The final DataFrame should keep the multi-index but include the normalized sales values alongside the existing data.\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.copy()\nresult['Normalized_Sales'] = df.groupby(level='Region')['Sales'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame containing sales data for different products across multiple stores. Here is the DataFrame:\n\n```\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'store': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'B', 'C'],\n    'product': ['Apple', 'Banana', 'Apple', 'Banana', 'Banana', 'Apple', 'Apple', 'Apple', 'Banana'],\n    'sales': [10, np.nan, 15, 12, 5, np.nan, 8, 20, np.nan]\n}\ndf = pd.DataFrame(data)\n```\n\nI want to compute the total sales for each product, but if any of the sales values for a product in a store are NaN, I want the total for that product to also be NaN. I can achieve this using groupby and an aggregate function, but I'm not sure how to handle the NaNs correctly.\n\nThe expected result should look like this:\n\n```\nproduct\nApple    NaN\nBanana   NaN\nName: sales, dtype: float64\n```\n\nHow can I write the code to achieve this?\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby('product')['sales'].agg(lambda x: x.sum() if x.isnull().any() == False else np.nan)\n</code>"
  },
  {
    "input": "### Problem:\nContext  \nI have two datasets with information about user transactions and their details. The first dataset contains transactions with timestamps, while the second contains transaction details that I want to merge based on the timestamp. However, some transactions in the first dataset might not have a corresponding entry in the second dataset.\n\nDataset1 (transactions):\n```\n    UserID       Timestamp         Amount\n0      1     2023-10-01 12:00  100\n1      2     2023-10-01 12:05  150\n2      1     2023-10-01 12:10  200\n3      3     2023-10-01 12:15  300\n```\n\nDataset2 (transaction_details):\n```\n    TransactionID     Timestamp          Status\n0             A1  2023-10-01 12:00        Success\n1             A2  2023-10-01 12:05        Failed\n2             A3  2023-10-01 12:10        Success\n```\n\nI want to merge these datasets such that I keep all the transactions from Dataset1, and where possible, include the corresponding Status from Dataset2 based on the Timestamp. The final DataFrame should also sort the entries by Timestamp.\n\nThe expected result should look like this:\n```\n    UserID       Timestamp         Amount TransactionID      Status\n0      1     2023-10-01 12:00  100       A1            Success\n1      2     2023-10-01 12:05  150       A2             Failed\n2      1     2023-10-01 12:10  200       A3            Success\n3      3     2023-10-01 12:15  300         NaN          NaN\n```\n\nHow can I achieve this using Pandas? Fill in the blank with your code solution below:\n\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'UserID': [1, 2, 1, 3],\n    'Timestamp': pd.to_datetime(['2023-10-01 12:00', '2023-10-01 12:05', '2023-10-01 12:10', '2023-10-01 12:15']),\n    'Amount': [100, 150, 200, 300]\n})\n\ndf2 = pd.DataFrame({\n    'TransactionID': ['A1', 'A2', 'A3'],\n    'Timestamp': pd.to_datetime(['2023-10-01 12:00', '2023-10-01 12:05', '2023-10-01 12:10']),\n    'Status': ['Success', 'Failed', 'Success']\n})\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2, on='Timestamp', how='left').sort_values(by='Timestamp')\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing information about sales data, structured as follows:\n\n| product_id | region   | sales      |\n|------------|----------|------------|\n| 1          | North    | 100        |\n| 2          | South    | 150        |\n| 3          | East     | 90         |\n| 1          | West     | <NA>      |\n| 2          | North    | 200        |\n| 3          | South    | NaN        |\n| 1          | East     | 110        |\n| 2          | West     | 160        |\n| 3          | North    | <NA>      |\n\nYour goal is to calculate the average sales per product_id for each region and fill any missing values with the overall average sales across all products. The desired output is a DataFrame that shows the average sales with no missing values.\n\nThe current code you are working with is:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product_id': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North'],\n    'sales': [100, 150, 90, pd.NA, 200, None, 110, 160, pd.NA]\n})\n\nresult = df.groupby(['product_id', 'region']).mean()\n```\n\nHowever, you are encountering issues with handling null values and cannot directly perform operations that include those null entries. How can you modify your code to fill in the missing values correctly and achieve the desired result?\n\nA:\n<code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['sales'] = df['sales'].astype('float')  # Ensure sales are float for mean calculation\nresult = df.groupby(['product_id', 'region']).mean().fillna(df['sales'].mean())\n</code>\n"
  },
  {
    "input": "Problem:\nYou have two Pandas DataFrames as follows:\n\ndf1:\n```\n   employee_id      name   department      start_date  salary\n0           101      John     Sales          2020/01/01   50000\n1           102      Mary     HR             2019/05/12   60000\n2           103      Sam      IT             2020/08/22   55000\n3           104      Anna     IT             2021/04/01   65000\n```\n\ndf2:\n```\n   employee_id      start_date  bonus\n0           103    2020/02/15   3000\n1           104    2020/11/01   4000\n2           105    2021/01/01   5000\n```\n\nYour task is to concatenate the two DataFrames based on `employee_id`, fill the `name` and `department` for the entries in `df2` where `employee_id` matches from `df1`, and sort the resulting DataFrame first by `employee_id` and then by `start_date`. The resulting DataFrame `result` should have the following format:\n\n```\n   employee_id      name   department      start_date  salary  bonus\n0           101      John     Sales          2020/01/01   50000   NaN\n1           102      Mary     HR             2019/05/12   60000   NaN\n2           103      Sam      IT             2020/02/15   55000   3000\n3           104      Anna     IT             2020/11/01   65000   4000\n4           105      NaN      NaN            2021/01/01    NaN   5000\n```\n\nThe code you've written for concatenating and filling is as follows:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'employee_id': [101, 102, 103, 104],\n                    'name': ['John', 'Mary', 'Sam', 'Anna'],\n                    'department': ['Sales', 'HR', 'IT', 'IT'],\n                    'start_date': ['2020/01/01', '2019/05/12', '2020/08/22', '2021/04/01'],\n                    'salary': [50000, 60000, 55000, 65000]})\n\ndf2 = pd.DataFrame({'employee_id': [103, 104, 105],\n                    'start_date': ['2020/02/15', '2020/11/01', '2021/01/01'],\n                    'bonus': [3000, 4000, 5000]})\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2, on='employee_id', how='outer').sort_values(by=['employee_id', 'start_date'])\n```"
  },
  {
    "input": "### Problem:\n\nI am working on a recommendation system using PyTorch. I have a tensor representing user-item interactions, where the rows correspond to users and the columns to items, with values indicating the interaction scores (higher scores indicate stronger interactions). Given this tensor, I'd like to apply a boolean mask to filter out items that have a score below a certain threshold, and then calculate the mean interaction score for the remaining items for each user.\n\nHere is the tensor and the boolean mask I have:\n\n```python\nimport torch\n\n# Tensor representing user-item interactions\ninteraction_scores = torch.tensor([[5.0, 2.0, 0.0, 4.0],\n                                    [3.0, 0.0, 1.0, 6.0],\n                                    [0.0, 8.0, 7.0, 0.0]])\n\n# Define a threshold\nthreshold = 2.0\n\n# Create a boolean mask\nmask = interaction_scores > threshold\n```\n\nI want to compute the mean of the filtered interaction scores for each user. Note that if all items for a user are filtered out, the mean should return `0.0`.\n\nComplete the following line of code to achieve this:\n\n```python\nmean_scores = torch.mean(interaction_scores[mask], dim=1)  # Calculate mean scores\nmean_scores = ...\n```\n\nA:\n\n```python\nmean_scores = ... # put solution in this variable\n```",
    "output": "\n```python\nmean_scores = torch.where(mask.sum(dim=1) > 0, mean_scores, torch.tensor(0.0))  # Handle cases with no valid items\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset containing multiple features, and wish to apply a Min-Max scaling to normalize the features. The dataset is represented as a pandas DataFrame with some features having missing values. Before scaling, you need to handle these missing values by replacing them with the median of each feature. After dealing with the missing values, you will perform the Min-Max scaling. For example, consider the following DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'feature1': [2, np.nan, 5, 7, 9],\n    'feature2': [1, 2, np.nan, 4, 5],\n    'feature3': [np.nan, 2, 3, np.nan, 5]\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to compute the Min-Max scaled DataFrame and store it in a variable called `scaled_df`. The Min-Max scaling should transform the features to a range between 0 and 1. There is already a method to perform Min-Max scaling in sklearn.\n\nA:\n```python\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n```\nscaled_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nimputer = SimpleImputer(strategy='median')\ndf_imputed = imputer.fit_transform(df)\n\nscaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(df_imputed)\nscaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n</code>\n```"
  },
  {
    "input": "Problem:\nYou are working on a dataset containing information about housing prices and their features. You want to preprocess this dataset using the `StandardScaler` from `sklearn.preprocessing` to normalize the features before training a machine learning model. The dataset is loaded into a variable called `housing_data`, which is a pandas DataFrame.\n\nWrite a function named `preprocess_data` that accepts this DataFrame as an argument and returns the standardized version of the DataFrame. Make sure to handle any NaN values by replacing them with the mean of their respective columns before scaling.\n\nA:\n<code>\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nhousing_data = load_data()  # assuming this function loads your DataFrame\n\ndef preprocess_data(df):\n    # your solution goes here\n    ### BEGIN SOLUTION\n</code>",
    "output": "\n<code>\n    df.fillna(df.mean(), inplace=True)\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(df)\n    return pd.DataFrame(scaled_data, columns=df.columns)\n    ### END SOLUTION\n</code>"
  },
  {
    "input": "### Problem:\n\nYou are tasked with building a predictive model to analyze credit card transaction data. The dataset contains various features including transaction amount, time of transaction, merchant category, and a binary label indicating if the transaction was fraudulent. \n\nTo preprocess the data, you need to handle missing values, normalize numerical features, and one-hot encode categorical features before fitting a machine learning model.\n\nGiven the following example DataFrame `df`, implement a function `preprocess_data` that does the following:\n\n1. Fills any missing values in the 'transaction_amount' column with the median value of the column.\n2. Normalizes the 'transaction_amount' column to a scale of 0 to 1.\n3. One-hot encodes the 'merchant_category' column.\n\nHere is the example DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'transaction_amount': [20, 55, None, 30, 75],\n    'merchant_category': ['grocery', 'electronics', 'grocery', 'electronics', 'clothing'],\n    'is_fraud': [0, 1, 0, 1, 0]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYour function definition should look like this:\n\n```python\ndef preprocess_data(df):\n    # Your preprocessing code goes here\n    ...\n```\n\nAfter implementing the preprocessing steps, the function should return a new DataFrame, ready for model training.",
    "output": "\n```python\ndef preprocess_data(df):\n    df['transaction_amount'].fillna(df['transaction_amount'].median(), inplace=True)\n    df['transaction_amount'] = (df['transaction_amount'] - df['transaction_amount'].min()) / (df['transaction_amount'].max() - df['transaction_amount'].min())\n    df = pd.get_dummies(df, columns=['merchant_category'], drop_first=True)\n    return df\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing various attributes of houses, including square footage, number of bedrooms, and sale price. You want to preprocess the data by normalizing the continuous variables (square footage and sale price) using `StandardScaler` from `sklearn.preprocessing`. The standardized data should then be combined with the categorical variable (number of bedrooms) encoded as one-hot vectors. The final output should be a NumPy array containing the combined data.\n\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Simulated dataset\ndata = pd.DataFrame({\n    'square_footage': [1500, 2400, 3000, 1800],\n    'bedrooms': [3, 4, 5, 3],\n    'sale_price': [300000, 450000, 600000, 350000]\n})\n\n# Begin preprocessing\nscaler = StandardScaler()\nencoder = OneHotEncoder(sparse=False)\n\n# Normalize continuous variables\nnormalized_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>normalized_data = np.hstack((scaler.fit_transform(data[['square_footage', 'sale_price']]),\nencoder.fit_transform(data[['bedrooms']])))\n</code>",
    "output": "\n<code>normalized_data = np.hstack((scaler.fit_transform(data[['square_footage', 'sale_price']]),\nencoder.fit_transform(data[['bedrooms']])))\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame that contains customer transaction data with several columns, including customer ID, transaction date, and transaction amount. The DataFrame looks like this:\n\n```\ncustomer_id | transaction_date | amount\n------------|------------------|-------\n      101   |   2023-01-15     |  200\n      102   |   2023-01-16     |  150\n      101   |   2023-01-17     |  300\n      103   |   2023-01-18     |  400\n      102   |   2023-01-19     |  250\n```\n\nI want to transform this DataFrame to have a multi-index defined by `customer_id` and `transaction_date`, and also normalize the `amount` column such that the values are transformed to be between 0 and 1 based on the max transaction amount per customer.\n\nThe final DataFrame should look like this:\n\n```\n                     amount\ncustomer_id transaction_date      \n      101   2023-01-15   0.00\n             2023-01-17   1.00\n      102   2023-01-16   0.00\n             2023-01-19   1.00\n      103   2023-01-18   1.00\n```\n\nWhat would be the most idiomatic way to achieve this in Pandas?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'customer_id': [101, 102, 101, 103, 102],\n    'transaction_date': ['2023-01-15', '2023-01-16', '2023-01-17', '2023-01-18', '2023-01-19'],\n    'amount': [200, 150, 300, 400, 250]\n}\n\ndf = pd.DataFrame(data)\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf = (df.set_index(['customer_id', 'transaction_date'])\n        .groupby(level=0)\n        .apply(lambda x: x['amount'] / x['amount'].max())\n        .reset_index(name='amount')\n        .set_index(['customer_id', 'transaction_date']))\n</code>"
  },
  {
    "input": "Problem:\nYou have two DataFrames, `sales_data` and `returns_data`, that track daily sales and return transactions for a retail store.\n\n`sales_data`:\n```\n+------------+-------+\n|    Date    | Sales |\n+------------+-------+\n| 2023-10-01 |  200  |\n| 2023-10-02 |  150  |\n| 2023-10-03 |  300  |\n| 2023-10-04 |  250  |\n+------------+-------+\n```\n\n`returns_data`:\n```\n+------------+-------+\n|    Date    | Returns|\n+------------+-------+\n| 2023-10-01 |   20  |\n| 2023-10-02 |   10  |\n| 2023-10-03 |   30  |\n| 2023-10-05 |   15  |\n+------------+-------+\n```\n\nYour objective is to merge these two DataFrames on the \"Date\" column, ensuring to align the return values with the respective sales on the same date. For dates available in `sales_data` but not in `returns_data`, the return should be marked as 0. Additionally, you must add a column \"Net_Sales\" to reflect the actual sales after subtracting returns. The resulting DataFrame should not contain any duplicates in the date column, and should be sorted by date.\n\nThe expected output should look like this:\n```\n+------------+-------+---------+-----------+\n|    Date    | Sales | Returns | Net_Sales |\n+------------+-------+---------+-----------+\n| 2023-10-01 |  200  |   20    |    180    |\n| 2023-10-02 |  150  |   10    |    140    |\n| 2023-10-03 |  300  |   30    |    270    |\n| 2023-10-04 |  250  |   0     |    250    |\n| 2023-10-05 |   0   |   15    |   -15     |\n+------------+-------+---------+-----------+\n```\n\nCan you provide the code to achieve this result?\n\nA:\n<code>\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'Date': pd.to_datetime(['2023-10-01', '2023-10-02', '2023-10-03', '2023-10-04']),\n    'Sales': [200, 150, 300, 250]\n})\n\nreturns_data = pd.DataFrame({\n    'Date': pd.to_datetime(['2023-10-01', '2023-10-02', '2023-10-03', '2023-10-05']),\n    'Returns': [20, 10, 30, 15]\n})\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.merge(sales_data, returns_data, on='Date', how='outer').fillna(0)\nresult['Net_Sales'] = result['Sales'] - result['Returns']\nresult = result.sort_values(by='Date').reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame containing sales data for different products across various regions, structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Product': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'D'],\n    'Region': ['North', 'South', 'East', 'West', 'North', 'East', 'South', 'West'],\n    'Sales': [200, None, 150, 300, 100, None, 250, 400],\n    'Profit': [50, 20, 30, None, 10, 40, None, 70]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to accomplish the following tasks:\n1. Replace any `None` values in the 'Sales' column with the mean sales from the DataFrame.\n2. Calculate the total profit for each product, using only the non-null profit values, and store this in a new DataFrame.\n3. Finally, filter this results DataFrame to include only those products whose total profit exceeds 40.\n\nCan you fill in the blank to achieve this? \n\n```python\nmean_sales = df['Sales'].mean()\ndf['Sales'].fillna(mean_sales, inplace=True)\n\ntotal_profit = df.groupby('Product')['Profit'].sum().reset_index()\nresult = total_profit[total_profit['Profit'] > 40]\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = total_profit[total_profit['Profit'] > 40]\n</code>\n```"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame that contains sales data structured as follows:\n```\n       product   sales\n0      A         100\n1      B         200\n2      C         null\n3      A         150\n4      B         null\n5      C         250\n6      A         300\n7      B         400\n8      C         100\n```\nThe DataFrame is initialized with the following code:\n```python\nimport pandas as pd\ndf = pd.DataFrame({\n    'product': ['A', 'B', None, 'A', 'B', 'C', 'A', 'B', 'C'],\n    'sales': [100, 200, None, 150, None, 250, 300, 400, 100]\n})\n```\nYour goal is to create a new DataFrame that aggregates the total sales for each product while filling any null sales figures with the average sales for that product before aggregation. For product A, the average sales count would be 216.67 (considering the entries along with nulls).\n\nFill in the blank in the following line of code to achieve this:\n```python\nresult = ...\n```\nMake sure that products with no sales entries also appear in the final DataFrame with total sales displayed as 0.\n\nNote: Make sure to handle null values properly during the average calculation.",
    "output": "\n```python\nresult = df.groupby('product').agg({'sales': lambda x: x.fillna(x.mean()).sum()}).reindex(df['product'].unique(), fill_value=0).reset_index()\n```"
  },
  {
    "input": "Problem:\nI have the following multi-index DataFrame representing sales data per category and month:\n\n```python\nimport pandas as pd\nimport numpy as np\n\narrays = [['Electronics', 'Electronics', 'Clothing', 'Clothing'], \n          ['January', 'February', 'January', 'February']]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Category', 'Month'))\ndata = np.random.randint(100, 500, size=(4, 3))\ncolumns = ['Store_A', 'Store_B', 'Store_C']\ndf = pd.DataFrame(data, index=index, columns=columns)\n```\n\nThe DataFrame looks like this:\n\n```\n                     Store_A  Store_B  Store_C\nCategory    Month                            \nElectronics January     238     423     482\n            February    386     431     198\nClothing    January     286     333     433\n            February    457     166     120\n```\n\nI want to normalize the sales figures per store, so that each store's sales for each category (across both months) is represented as a percentage of the category's total sales for that month. The resulting DataFrame should preserve the multi-index structure while expressing sales as percentages.\n\nProvide the code that performs this normalization and stores the result in a variable named `normalized_df`.\n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\narrays = [['Electronics', 'Electronics', 'Clothing', 'Clothing'], \n          ['January', 'February', 'January', 'February']]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Category', 'Month'))\ndata = np.random.randint(100, 500, size=(4, 3))\ncolumns = ['Store_A', 'Store_B', 'Store_C']\ndf = pd.DataFrame(data, index=index, columns=columns)\n```\nnormalized_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n```",
    "output": "\n```python\nnormalized_df = df.div(df.groupby(level='Month').transform('sum')).multiply(100)\n```"
  },
  {
    "input": "Problem:\nI am working on a classification task and want to create a training and testing set from an imbalanced dataset. I need to apply stratified sampling to ensure that the class distribution in the split remains the same as in the original dataset. I am currently using the `train_test_split` function from scikit-learn, but I want to ensure that the sampling is stratified by the target variable. Given the following code setup:\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.DataFrame({\n    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'target': [0, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n})\n\nX = data[['feature1', 'feature2']]\ny = data['target']\n```\nI want to fill in the blank to perform a stratified split into training and testing sets. \nA:\n```python\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=...)\n```",
    "output": "\n```python\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products, including their categories and prices. The DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Category': ['Electronics', 'Furniture', 'Electronics', 'Clothing', 'Clothing', 'Furniture'],\n    'Product': ['TV', 'Sofa', 'Laptop', 'T-shirt', 'Jeans', 'Table'],\n    'Price': [300, 800, 1200, 40, 60, 150]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to normalize the 'Price' values within each product category to ensure that the data can be effectively analyzed, i.e., constrain the data values to a range between 0 and 1 within each category. Your goal is to add a new column 'Normalized_Price' to the existing DataFrame.\n\nHow can you achieve this?\n\n```python\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n```",
    "output": "\n<code>\ndf['Normalized_Price'] = df.groupby('Category')['Price'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a neural network model in PyTorch that has an output layer producing a tensor of shape (batch_size, num_classes), where each row corresponds to the predicted scores for each class in a multi-class classification problem. You also have a tensor of true labels of shape (batch_size,). \n\nGiven the prediction tensor `scores` and the tensor of true labels `labels`, your task is to generate a tensor `predicted_classes` that contains the index of the class with the highest score for each input, and then generate a boolean mask where `True` indicates that the predicted class matches the true label.\n\nFor example, if your scores tensor is:\n\n```\ntensor([[0.1, 0.3, 0.6],\n        [0.7, 0.2, 0.1],\n        [0.4, 0.5, 0.1]])\n```\n\nand your labels tensor is:\n\n```\ntensor([2, 0, 1])\n```\n\nYou need to create the boolean mask indicating correct predictions. \n\nHere\u2019s the setup:\n\n<code>\nimport torch\n\nscores = torch.tensor([[0.1, 0.3, 0.6],\n                       [0.7, 0.2, 0.1],\n                       [0.4, 0.5, 0.1]])\nlabels = torch.tensor([2, 0, 1])\n\n# Fill in the blank to compute the correct predictions mask\npredicted_classes = ...\n\ncorrect_mask = predicted_classes == labels  # This line is already provided\n</code>\n\nYour task is to fill the `predicted_classes` variable with the appropriate tensor result. \n\nBEGIN SOLUTION\n<code>",
    "output": "\npredicted_classes = torch.argmax(scores, dim=1)  # Get the index of the max score for each input\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset that contains sales data for a retail store, represented in a pandas DataFrame like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03', '2023-01-03'],\n    'item': ['apple', 'banana', 'apple', 'banana', 'apple'],\n    'quantity': [10, 20, 15, 5, 30],\n    'price_per_item': [0.5, 0.3, 0.5, 0.3, 0.5]\n}\nsales_df = pd.DataFrame(data)\n```\n\nYou want to calculate the total revenue for each item sold over the entire period represented in the DataFrame. The revenue for each item can be calculated as the sum of `quantity * price_per_item` for each record pertaining to that item. \n\nTo achieve this, you need to group the DataFrame by `item` and use an appropriate aggregation function.\n\nFill in the blank in the following code to produce a new DataFrame that summarizes the total revenue for each item:\n\n```python\ntotal_revenue_df = sales_df.groupby('item').__________\n```\nBEGIN SOLUTION\n<code>\n\n",
    "output": "\n<code>agg(total_revenue=('quantity', lambda x: (x * sales_df.loc[x.index, 'price_per_item']).sum()))</code>"
  },
  {
    "input": "Problem:\n\nI have a tensor `a` that holds some values and I want to apply a thresholding operation to it where values above a certain threshold are retained as-is, and values below that threshold should be set to zero. I am also required to keep track of how many values were zeroed out during this operation for further analysis. Given the tensor `a`, here is what I have so far:\n\n```python\nimport torch\na = torch.tensor([[0.5, 1.2, 3.0], [4.4, 0.1, 2.2]])\n\nthreshold = 1.0\nzeroed_out_count = 0\n# Your code should go here\n```\n\nA:\n\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nzeroed_out_count = (a < threshold).sum().item()\nresult = torch.where(a < threshold, torch.tensor(0.0), a)\n</code>"
  },
  {
    "input": "Problem:\n\nYou have collected a dataset containing various features of houses including their sizes (in square feet), number of bedrooms, and prices. Before training a machine learning model, you need to perform data preprocessing steps which include scaling the features and one-hot encoding any categorical variables.\n\nGiven the following dataset:\n\n```python\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'size': [1500, 1600, 1700, 1800, 1900],\n    'bedrooms': [3, 3, 3, 4, 4],\n    'price': [300000, 320000, 340000, 360000, 380000],\n    'location': ['city', 'suburb', 'city', 'suburb', 'city']\n})\n```\n\nYou need to scale the 'size' and 'price' columns to have a mean of 0 and a standard deviation of 1, and apply one-hot encoding to the 'location' column.\n\nPlease fill in the blank to create a preprocessed DataFrame called `X_processed`, which includes the necessary transformations:\n\n```python\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Define the preprocessing steps\nnumeric_features = ['size', 'price']\ncategorical_features = ['bedrooms', 'location']\n\n# Create preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n\n# Apply the transformations\nX_processed = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>X_processed = preprocessor.fit_transform(data)</code>"
  },
  {
    "input": "Problem:\n\nI am working with a dataset containing user ratings for multiple items, represented as a PyTorch tensor. I need to compute the cosine similarity between each pair of user rating vectors to identify which users have similar preferences. \n\nGiven the ratings tensor, I want to create a boolean mask that filters out user pairs with a cosine similarity below a certain threshold (e.g., 0.5) and returns a tensor that contains only the pairs meeting this criteria.\n\nThe ratings tensor is given below:\n```\nratings = torch.tensor([[5, 1, 0], \n                        [4, 0, 2], \n                        [0, 3, 4], \n                        [0, 0, 5]])\n```\n\nHow can I generate a mask for this filtering operation based on the cosine similarity calculation? \n\nA:\n<code>\nimport torch\nratings = torch.tensor([[5, 1, 0], \n                        [4, 0, 2], \n                        [0, 3, 4], \n                        [0, 0, 5]])\n</code>\nmask = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom torch.nn.functional import cosine_similarity\n\n# Compute cosine similarity between each pair of users\nsimilarity_matrix = cosine_similarity(ratings.unsqueeze(1), ratings.unsqueeze(0), dim=2)\n\n# Create a boolean mask for similarity above the threshold\nthreshold = 0.5\nmask = similarity_matrix > threshold\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing customer information, and you want to perform data normalization and dimensionality reduction in preparation for a machine learning model. Given your data in a pandas DataFrame `df` with numerical features, you are using the `StandardScaler` for normalization followed by `PCA` for dimensionality reduction.\n\nHere is an example code setup:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# Assume df is your input DataFrame with numerical columns only\n```\n\nYou want to extract the transformed features after applying normalization but before PCA is applied. How can you achieve this?\n\nStore the output of the transformation from `StandardScaler` into the variable `scaled_data`.\n\nscaled_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n```"
  },
  {
    "input": "Problem:\nI am working with two DataFrames, X and Y, that represent sales data over time. The structure of the DataFrames is as follows:\n\nX:\n```\n   Product  Sales        Date\n0       A    200  2023-01-01\n1       B    150  2023-01-02\n2       C    300  2023-01-03\n3       A    250  2023-01-01\n```\n\nY:\n```\n   Product  Sales        Date\n0       A    180  2023-01-02\n1       D    330  2023-01-04\n2       B    160  2023-01-03\n```\n\nI need to merge these DataFrames to create a consolidated view of sales data, ensuring that any duplicate entries for a Product on the same Date are merged such that their Sales values are summed. The resulting DataFrame should have the unique combinations of Product and Date, while maintaining the original data integrity and sorting by Date first and then by Product. The expected structure of the output DataFrame is:\n\n```\n   Product  Sales        Date\n0       A    200  2023-01-01\n1       A    180  2023-01-02\n2       B    150  2023-01-02\n3       B    320  2023-01-03 \n4       C    300  2023-01-03\n5       D    330  2023-01-04\n```\n\nCurrently, I am trying to use `pd.concat` but I am unsure how to handle the duplicates and sum their Sales properly.\n\nA:\n<code>\nimport pandas as pd\n\nX = pd.DataFrame({\n    'Product': ['A', 'B', 'C', 'A'],\n    'Sales': [200, 150, 300, 250],\n    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-01']\n})\n\nY = pd.DataFrame({\n    'Product': ['A', 'D', 'B'],\n    'Sales': [180, 330, 160],\n    'Date': ['2023-01-02', '2023-01-04', '2023-01-03']\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.concat([X, Y]).groupby(['Product', 'Date'], as_index=False).sum().sort_values(by=['Date', 'Product'])\n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset containing sales records with missing entries for some products, and I want to fill in those missing values using the mean of the existing sales for their respective categories. The dataset is structured like this:\n\n```\n+------------+-----------+--------+\n| category   | product   | sales  |\n+------------+-----------+--------+\n| electronics| phone     | 150    |\n| electronics| laptop    | NaN    |\n| groceries  | apple     | 30     |\n| groceries  | banana    | NaN    |\n| groceries  | orange    | 25     |\n| furniture   | chair     | 75     |\n| furniture   | table     | NaN    |\n+------------+-----------+--------+\n```\n\nI attempted to use a groupby operation, but I'm unsure how to fill the NaN values correctly while maintaining the mean for each category.\n\nI'd like to know how to calculate the mean sales per category and fill in the missing sales values for the products accordingly. \n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'category': ['electronics', 'electronics', 'groceries', 'groceries', 'groceries', 'furniture', 'furniture'],\n    'product': ['phone', 'laptop', 'apple', 'banana', 'orange', 'chair', 'table'],\n    'sales': [150, np.nan, 30, np.nan, 25, 75, np.nan]\n})\n\nmean_sales_per_category = df.groupby('category')['sales'].mean()\n```\ndf['sales'] = ... # put solution in this variable",
    "output": "\n```python\ndf['sales'] = df['sales'].fillna(df.groupby('category')['sales'].transform('mean'))\n```"
  },
  {
    "input": "Problem:\n\nI have a tensor in TensorFlow representing probabilities from a softmax layer, and I want to apply a boolean mask to zero out values below a certain threshold. The tensor shape is (32, 10), representing a batch of 32 samples with 10 classes. Simultaneously, I want to calculate the sum of the masked values along the class dimension. Here's how my current setup looks:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nprobs = tf.constant(np.random.rand(32, 10), dtype=tf.float32)\nthreshold = 0.2\n\n# I need to get the masked tensor and the sum of the masked values\nmasked_probs = ...\nsum_masked_probs = ...\n```\n\nCan you help me fill in the blanks for the variable assignments? Thanks!",
    "output": "\n```python\nmasked_probs = tf.where(probs >= threshold, probs, tf.zeros_like(probs))\nsum_masked_probs = tf.reduce_sum(masked_probs, axis=1)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames with the following structure:\n\ndf1:\n```\n   employee_id   department   date               score\n0           1         HR      2022-01-01         88\n1           2         IT      2022-01-01         91\n2           3         IT      2022-01-01         85\n3           1         HR      2022-02-01         82\n4           2         IT      2022-02-01         90\n```\n\ndf2:\n```\n   employee_id   date               score\n0           3      2022-01-05          85\n1           1      2022-01-05          90\n2           1      2022-03-01          92\n3           4      2022-01-05          75\n4           5      2022-01-05          88\n```\n\nThe task is to merge these two DataFrames into a single DataFrame such that:\n1. Rows are aligned by `employee_id` and `date`.\n2. If there are duplicate entries for an employee on a specific date in either DataFrame, keep the one with the highest score.\n3. Fill missing department information in df2 from df1 where applicable.\n\nThe expected DataFrame after merging and deduplication should look like this:\n```\n   employee_id   department   date               score\n0           1         HR      2022-01-01         88\n1           1         HR      2022-01-05         90\n2           1         HR      2022-02-01         82\n3           1         HR      2022-03-01         92\n4           2         IT      2022-01-01         91\n5           2         IT      2022-02-01         90\n6           3         IT      2022-01-01         85\n7           3         NaN     2022-01-05        85\n8           4         NaN     2022-01-05         75\n9           5         NaN     2022-01-05         88\n```\n\nSo far I have attempted the following code to start the merging process:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'employee_id': [1, 2, 3, 1, 2],\n                    'department': ['HR', 'IT', 'IT', 'HR', 'IT'],\n                    'date': ['2022-01-01', '2022-01-01', '2022-01-01', '2022-02-01', '2022-02-01'],\n                    'score': [88, 91, 85, 82, 90]})\n\ndf2 = pd.DataFrame({'employee_id': [3, 1, 1, 4, 5],\n                    'date': ['2022-01-05', '2022-01-05', '2022-03-01', '2022-01-05', '2022-01-05'],\n                    'score': [85, 90, 92, 75, 88]})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nmerged = pd.concat([df1, df2], ignore_index=True)\nmerged = merged.sort_values(['employee_id', 'date']).drop_duplicates(subset=['employee_id', 'date'], keep='last')\nresult = merged.merge(df1[['employee_id', 'department']], on='employee_id', how='left').fillna(method='ffill')\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame that consists of sales data for a retail store, containing columns for the 'store_id', 'product_id', 'quantity_sold', and 'revenue'. Some of the values in the 'quantity_sold' column may be missing (NaN). Your task is to calculate the total revenue per store while replacing NaN values in 'quantity_sold' with 0 for the sake of the calculation. The resulting DataFrame should include 'store_id', 'total_quantity_sold', and 'total_revenue', where 'total_quantity_sold' is the sum of 'quantity_sold' for each store and 'total_revenue' is the sum of 'revenue' for each store.\n\nHere's a sample DataFrame to start with:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'store_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'product_id': [101, 102, 101, 102, 101, 103],\n    'quantity_sold': [5, np.nan, 3, 4, np.nan, 2],\n    'revenue': [20, 15, 12, 16, 5, 10]\n})\n```\n\nYou want to perform the following operation:\n```python\nresult = ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df.fillna({'quantity_sold': 0}).groupby('store_id').agg(\n    total_quantity_sold=('quantity_sold', 'sum'),\n    total_revenue=('revenue', 'sum')\n).reset_index()\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset representing sales data for different products over several months. The dataframe includes duplicate entries for some products in certain months, and I need to process this data to keep only the most recent sales record for each product, while also replacing any missing month entries with the most recent available month. Afterward, I want to sort the final dataframe by month.\n\nHere's my initial dataframe:\n\n```python\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\n    'product_id': [1, 1, 2, 2, 3, 3, 1],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-03', '2023-01', np.nan, '2023-01'],\n    'sales': [100, 150, 200, 250, 300, 350, 200]\n})\n```\n\nI executed the following operations to find the most recent record per product:\n\n```python\nlatest = df.loc[df['month'].notnull()].sort_values(by='month').drop_duplicates(subset='product_id', keep='last')\n```\n\nNow, I want to fill the missing month values with the latest available month for each product, and finally sort the dataframe by 'month'.\n\nHow can I achieve the final result with a single line of code that fulfills this requirement?\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'product_id': [1, 1, 2, 2, 3, 3, 1],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-03', '2023-01', np.nan, '2023-01'],\n    'sales': [100, 150, 200, 250, 300, 350, 200]\n})\nfinal_result = ... # your solution here\n### BEGIN SOLUTION",
    "output": "\n<code>\nfinal_result = df.groupby('product_id').apply(lambda x: x.ffill().sort_values(by='month')).reset_index(drop=True).sort_values(by='month')\n</code>"
  },
  {
    "input": "Problem:\nYou are given a DataFrame containing sales data for different products over various months, structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product_A', 'Q1'): [200, 250, 300],\n    ('Product_A', 'Q2'): [220, 240, 320],\n    ('Product_B', 'Q1'): [150, 175, 200],\n    ('Product_B', 'Q2'): [180, 190, 210],\n}\n\nindex = ['January', 'February', 'March']\ndf = pd.DataFrame(data, index=index)\n```\n\nYou need to normalize the sales data for each product (column-wise normalization) and then create a new DataFrame that retains the original multi-index structure while also appending a new row at the bottom with the normalized sums for each product across all months. How can you accomplish this?\n\nA:\n<code>\ndf_normalized = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf_normalized = df.div(df.sum(axis=0), axis=1)\nnormalized_sums = df_normalized.sum(axis=0).to_frame().T\nnormalized_sums.index = ['Normalized_Sum']\ndf_normalized = pd.concat([df_normalized, normalized_sums])\n</code>"
  },
  {
    "input": "Problem:\nYou are given two DataFrames representing product sales over different months:\n\ndf_sales:\n   product_id    sale_date  quantity\n0          1  2023-01-01       100\n1          2  2023-01-01       200\n2          1  2023-01-15       150\n3          3  2023-02-01       300\n\ndf_returns:\n   product_id     return_date  quantity\n0          1  2023-01-05          50\n1          2  2023-02-01         100\n2          1  2023-01-10          20\n3          3  2023-02-02          30\n\nYou want to create a new DataFrame that includes all products with their total sales quantity minus the total returns. The outcome should maintain the product_id, sale_date aligned to the earliest sale date for each product and return_date in an appropriate format (e.g., '01-Jan-2023').\n\nThe resulting DataFrame should have the following columns: product_id, sale_date, total_sales, return_date, total_returns.\n\nYour current attempt with `pd.concat()` does not yield the desired results because of mismatched dates. You need to first merge the DataFrames on product_id while summing the quantities appropriately.\n\nA:\n<code>\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3],\n    'sale_date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-15', '2023-02-01']),\n    'quantity': [100, 200, 150, 300]\n})\n\ndf_returns = pd.DataFrame({\n    'product_id': [1, 2, 1, 3],\n    'return_date': pd.to_datetime(['2023-01-05', '2023-02-01', '2023-01-10', '2023-02-02']),\n    'quantity': [50, 100, 20, 30]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df_sales.groupby('product_id').agg(\n    sale_date=('sale_date', 'min'),\n    total_sales=('quantity', 'sum')\n).reset_index()\n\nreturns_summary = df_returns.groupby('product_id').agg(\n    return_date=('return_date', 'max'),\n    total_returns=('quantity', 'sum')\n).reset_index()\n\nresult = pd.merge(result, returns_summary, on='product_id', how='left')\nresult['return_date'] = result['return_date'].dt.strftime('%d-%b-%Y')\n</code>"
  },
  {
    "input": "Problem:\nI am working with a DataFrame that tracks store sales data with potentially missing values, structured like this:\n```\n   Store  Sales\n0      A  2000\n1      A  3000\n2      B  4000\n3      B  null\n4      C  5000\n5      C  6000\n6      C  null\n```\nThe DataFrame was created using:\n```python\ndf = pd.DataFrame({'Store': ['A', 'A', 'B', 'B', 'C', 'C', 'C'], \n                   'Sales': [2000, 3000, 4000, None, 5000, 6000, None]})\n```\nI want to compute the average sales for each store but ignore null values. Additionally, I want to label stores with an average greater than 3500 as \"High Performer\" and others as \"Regular\". The expected output should look like this:\n```\n   Store          Average      Label\n0     A           2500.0     Regular\n1     B           4000.0    High Performer\n2     C           5500.0    High Performer\n```\nI started by grouping and calculating the mean, but I want to know how to apply this custom labeling to the results. What is the correct way to accomplish this?\n\nA:\n<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Store': ['A', 'A', 'B', 'B', 'C', 'C', 'C'], \n                   'Sales': [2000, 3000, 4000, None, 5000, 6000, None]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby('Store', as_index=False).agg(Average=('Sales', 'mean'))\nresult['Label'] = result['Average'].apply(lambda x: 'High Performer' if x > 3500 else 'Regular')\n</code>"
  },
  {
    "input": "Problem:\n\nI have a multi-index DataFrame representing sales data for different products over different quarters and regions, structured like this:\n\n```python\nimport pandas as pd\n\narrays = [\n    ['Electronics', 'Electronics', 'Furniture', 'Furniture'],\n    ['North', 'South', 'North', 'South']\n]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Product', 'Region'))\ndata = {\n    'Q1': [200, 150, 300, 250],\n    'Q2': [230, 180, 310, 270],\n    'Q3': [250, 200, 320, 290],\n    'Q4': [270, 220, 330, 310]\n}\ndf = pd.DataFrame(data, index=index)\n```\n\nI want to normalize the sales figures across all quarters for each product, so that each quarter's sales figures for each product sum up to 1. What function would you use to perform this normalization across the DataFrame? \n\n```python\nnormalized_df = df. # Is there a specific Pandas operation to achieve this?\n```\n",
    "output": "\n```python\nnormalized_df = df.div(df.sum(axis=1), axis=0)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames A and B as follows:\nA\n    ID  Date       Value\n0   1   2021-01-01  10\n1   2   2021-01-02  20\n2   3   2021-01-03  30\nB\n    ID  Date       Value\n0   2   2021-01-01  15\n1   3   2021-01-03  25\n2   4   2021-01-04  35\n\nI want to concatenate these DataFrames based on the ID column, ensuring that I fill in the value for overlapping IDs with the value from DataFrame B, where available. If an ID exists in only one DataFrame, that row should still be present in the result. In addition, I want to retain the earliest date for each ID in the resulting DataFrame. Finally, the resulting DataFrame should be sorted by ID.\n\nThe expected output should look like this:\n    ID  Date       Value\n0   1   2021-01-01  10\n1   2   2021-01-01  15\n2   3   2021-01-03  25\n3   4   2021-01-04  35\n\nI attempted to achieve this using pd.concat but ran into issues with duplicated IDs and keeping the correct values and dates.\n\nA:\n<code>\nimport pandas as pd\n\nA = pd.DataFrame({\n    'ID': [1, 2, 3],\n    'Date': ['2021-01-01', '2021-01-02', '2021-01-03'],\n    'Value': [10, 20, 30]\n})\n\nB = pd.DataFrame({\n    'ID': [2, 3, 4],\n    'Date': ['2021-01-01', '2021-01-03', '2021-01-04'],\n    'Value': [15, 25, 35]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = (pd.concat([A, B])\n          .groupby('ID')\n          .agg({'Date': 'min', 'Value': 'last'})\n          .reset_index()\n          .sort_values(by='ID'))\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames representing product sales over different months.\n\nsales_prev:\n```\n   ProductID  Month   Sales\n0          1  2022-01    200\n1          2  2022-01    150\n2          3  2022-01    300\n3          1  2022-02    250\n4          2  2022-02    200\n```\n\nsales_curr:\n```\n   ProductID  Month   Sales\n0          1  2022-03    400\n1          2  2022-03    250\n2          4  2022-03    350\n```\n\nI would like to merge these two DataFrames, but I want to ensure that for any ProductID that appears in both DataFrames, I need to conditionally replace the values of the `Sales` column in the `sales_prev` DataFrame with the values from the `sales_curr` DataFrame. Also, any new products in `sales_curr` should be included in the final output, and the final DataFrame should be sorted by `ProductID`.\n\nThe desired output would look like this:\n```\n   ProductID  Month   Sales\n0          1  2022-03    400\n1          2  2022-03    250\n2          3  2022-01    300\n3          4  2022-03    350\n```\n\nI have already imported pandas and created the DataFrames but I'm unsure how to merge them while satisfying the requirements mentioned.\n\nA:\n<code>\nimport pandas as pd\n\n\nsales_prev = pd.DataFrame({'ProductID': [1, 2, 3, 1, 2],\n                            'Month': ['2022-01', '2022-01', '2022-01', '2022-02', '2022-02'],\n                            'Sales': [200, 150, 300, 250, 200]})\n\nsales_curr = pd.DataFrame({'ProductID': [1, 2, 4],\n                            'Month': ['2022-03', '2022-03', '2022-03'],\n                            'Sales': [400, 250, 350]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.concat([sales_prev[sales_prev['ProductID'].isin(sales_curr['ProductID']) == False],\n                    sales_curr.rename(columns={'Month': 'Month', 'Sales': 'Sales'})]).combine_first(\n                    sales_curr.set_index('ProductID').combine_first(\n                    sales_prev.set_index('ProductID'))).reset_index().sort_values(by='ProductID')\n</code>"
  },
  {
    "input": "Problem:\nI have a dataset encapsulated in a pandas DataFrame that contains user activity logs. The DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'user_id': [1, 2, 1, 3, 2, 3, 1, None],\n    'activity': ['login', 'logout', 'logout', 'login', 'login', 'logout', 'logout', 'login'],\n    'duration': [20, None, 15, 25, 30, 10, None, 5]\n}\ndf = pd.DataFrame(data)\n```\n\nI want to calculate the total time spent by each user based on their 'login' and 'logout' activities. However, the calculation should only include rows where 'user_id' is not null, and for 'logout' activities, if the 'duration' is also not null.\n\nMy desired output should have the following format:\n\n```\nuser_id  total_time\n1        (sum of durations for user_id 1 where activity is 'logout')\n2        (sum of durations for user_id 2 where activity is 'logout')\n3        (sum of durations for user_id 3 where activity is 'logout')\n```\n\nWhile I understand how to group and sum, I'm not sure how to include the conditions and manage null values effectively with a custom function. What code can I use to achieve this?\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df[df['user_id'].notnull() & (df['activity'] == 'logout') & df['duration'].notnull()] \\\n    .groupby('user_id', as_index=False)['duration'].sum() \\\n    .rename(columns={'duration': 'total_time'})\n```"
  },
  {
    "input": "## Problem\n\nI am working on a recommendation system and need to calculate the similarity between user preferences represented as a tensor in PyTorch. The preferences are stored in a 3D tensor `user_preferences` with dimensions corresponding to `[users, items, attributes]`. \n\nI want to compute the cosine similarity for each user\u2019s preferences against a target item represented by a tensor `target_item`. The goal is to create an output tensor of shape `[users, 1]` that contains the similarity scores.\n\nHere is how my input tensors look:\n\n```python\nimport torch\n\n# Shape: [users, items, attributes]\nuser_preferences = torch.tensor([\n    [[0.2, 0.3], [0.5, 0.7]],\n    [[0.1, 0.4], [0.8, 0.6]],\n    [[0.9, 0.3], [0.2, 0.1]]\n])\n\n# Shape: [1, attributes]\ntarget_item = torch.tensor([[0.5, 0.5]])\n```\n\nHow can I calculate the cosine similarity between each user\u2019s preferences (flattened to item dimension) and the target item?\n\nA:\n\n```python\nimport torch\n\nuser_preferences, target_item = load_data()\n# Calculate the cosine similarity scores\nsimilarity_scores = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nuser_preferences_flat = user_preferences.view(user_preferences.size(0), -1)\ntarget_item_flat = target_item.view(-1)\n\nsimilarity_scores = torch.nn.functional.cosine_similarity(user_preferences_flat, target_item_flat.unsqueeze(0), dim=1).unsqueeze(1)\nEND SOLUTION\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing information about various cars, including their make, model, year, horsepower, and type (SUV, Sedan, etc.). Your goal is to preprocess this data for machine learning use. You need to normalize the horsepower values and encode the categorical variable 'type'.\n\nHere's a sample of the data you are working with:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Make': ['Toyota', 'Honda', 'Ford', 'Chevrolet', 'Toyota'],\n    'Model': ['Camry', 'Civic', 'Mustang', 'Malibu', 'RAV4'],\n    'Year': [2020, 2021, 2019, 2018, 2020],\n    'Horsepower': [203, 158, 450, 250, 194],\n    'Type': ['Sedan', 'Sedan', 'Coupe', 'Sedan', 'SUV']\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to perform the following tasks:\n1. Normalize the 'Horsepower' column using Min-Max scaling.\n2. Convert the 'Type' column into numeric values, using one-hot encoding.\n\nAfter executing these transformations, you should obtain a DataFrame where the 'Horsepower' is normalized and 'Type' is represented in binary format.\n\nHere is the blank where you need to write your code to perform these transformations:\n\n```python\n# Normalizing horsepower and one-hot encoding type\nnormalized_df = _____\n```\n\nFill in the blank to achieve the required transformation.",
    "output": "\n```python\n# Normalizing horsepower and one-hot encoding type\nnormalized_df = df.copy()\nnormalized_df['Horsepower'] = (normalized_df['Horsepower'] - normalized_df['Horsepower'].min()) / (normalized_df['Horsepower'].max() - normalized_df['Horsepower'].min())\nnormalized_df = pd.get_dummies(normalized_df, columns=['Type'], drop_first=True)\n```"
  },
  {
    "input": "Problem:\n\nYou are tasked with managing a dataset of high-dimensional embeddings, represented as a tensor in PyTorch with shape (batch_size, num_features). You need to filter this tensor to keep only the embeddings that meet a certain condition defined by a mask tensor. \n\nSuppose you have the following setup:\n\n```python\nimport torch\n\nbatch_size = 5\nnum_features = 10\nembeddings = torch.randn(batch_size, num_features)\nmask = torch.tensor([True, False, True, False, True])  # This mask indicates which rows to keep\n\n# Fill in the code below to filter the embeddings tensor using the mask\n```\n\nHow can you achieve this filtering and return the resulting tensor of embeddings that pass the mask condition?\n\nA:\n<code>\nfiltered_embeddings = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfiltered_embeddings = embeddings[mask]\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames with records of sales transactions. The first DataFrame (`sales`) contains the following information:\n\n```python\nimport pandas as pd\n\nsales_data = {\n    'transaction_id': [1001, 1002, 1003, 1004, 1005],\n    'product_id': [501, 502, 503, 501, 504],\n    'quantity': [2, 3, 1, 1, 4],\n    'date': ['2022-01-01', '2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02']\n}\nsales = pd.DataFrame(sales_data)\n```\n\nThe second DataFrame (`products`) contains information about the products:\n\n```python\nproducts_data = {\n    'product_id': [501, 502, 503, 504],\n    'product_name': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n    'category': ['Gadgets', 'Gadgets', 'Gizmos', 'Gizmos']\n}\nproducts = pd.DataFrame(products_data)\n```\n\nI need to create a combined DataFrame that merges `sales` with `products` based on `product_id`. The final DataFrame should include `transaction_id`, `product_name`, `quantity`, and `date`. Also, entries with the same `transaction_id` should be sorted by `date` in ascending order.\n\nThe combined DataFrame should look like this:\n\n```\n   transaction_id product_name  quantity        date\n0             1001     Widget A         2  2022-01-01\n1             1002     Widget B         3  2022-01-01\n2             1003     Widget C         1  2022-01-01\n3             1004     Widget A         1  2022-01-02\n4             1005     Widget D         4  2022-01-02\n```\n\nI have initially attempted to merge the DataFrames as follows:\n\n```python\ncombined = pd.merge(sales, products, on='product_id')\n```\n\nHowever, I need the final DataFrame to reflect the specified columns and sorting criteria. \n\nWhat code should I use to achieve that?\n```python\nresult = ...  # put solution in this variable\n```",
    "output": "\n```python\nresult = combined[['transaction_id', 'product_name', 'quantity', 'date']].sort_values(by=['date', 'transaction_id']).reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\n\nYou are analyzing a dataset containing housing prices and various features such as square footage, number of bedrooms, and neighborhood quality. You want to preprocess this data using the `StandardScaler` from `sklearn.preprocessing` to normalize the features before training a regression model. The dataset is loaded into a Pandas DataFrame called `housing_data`. You need to scale the features in the DataFrame and return the transformed data as a NumPy array.\n\nConsider the following setup:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load your data\nhousing_data = pd.DataFrame({\n    'square_footage': [1500, 1800, 2400, 3000],\n    'bedrooms': [3, 4, 4, 5],\n    'neighborhood_quality': [8, 7, 9, 6]\n})\n\nscaler = StandardScaler()\n```\n\nWhat code would you use to transform the features in `housing_data` and store them in the variable `scaled_features`?\n\nA:\n\n<code>\nscaled_features = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nscaled_features = scaler.fit_transform(housing_data)\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame of users and their associated scores in multiple categories, and you want to compute the maximum score for each user while excluding any scores that are null (NaN). The DataFrame looks like this:\n\n```python\nimport pandas as pd\ndata = {\n    'user': ['Alice', 'Bob', 'Alice', 'Bob', 'Charlie'],\n    'category': ['A', 'A', 'B', 'B', 'A'],\n    'score': [95, None, 80, 100, None]\n}\ndf = pd.DataFrame(data)\n```\n\nThe desired output for each user should show the maximum score across all categories while discarding any null scores:\n\n```\n     user  max_score\n0   Alice        95.0\n1     Bob       100.0\n2 Charlie         0.0\n```\n\nHowever, your current attempt is not yielding the correct results. You want to replace any user's `max_score` with `0.0` if all their scores are null. Here\u2019s the code to modify:\n\n```python\nresult = df.groupby('user')['score'].apply(...).reset_index()\n```\n\nFill in the blank to derive the correct max score with the specified conditions.",
    "output": "\n```python\nresult = df.groupby('user')['score'].apply(lambda x: x.max() if x.notnull().any() else 0.0).reset_index(name='max_score')\n```"
  },
  {
    "input": "Problem:\nYou are analyzing a dataset of users' purchase transactions stored in a Pandas DataFrame with the following structure:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('user_id', 'user'): [1, 1, 2, 2, 3],\n    ('transaction', 'transaction_id'): [101, 102, 103, 104, 105],\n    ('purchase', 'item'): ['A', 'B', 'A', 'C', 'B'],\n    ('purchase', 'amount'): [25, 30, 15, 45, 20]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to calculate the total amount spent by each user per item category and normalize the resulting amounts by the maximum amount for each user. Your goal is to fill in the following code that will provide a normalized DataFrame with a multi-index, where the outer index is `user_id` and the inner index is the item purchased. The resulting DataFrame should look something like:\n\n```\n              amount\nuser_id item        \n1       A     1.00\n        B     1.20\n2       A     0.33\n        C     1.00\n3       B     1.00\n```\n\nTo achieve this, you must calculate the total purchase amounts and normalize them. Fill in the solution below to get the desired output:\n\n```python\nresult = ...\n```",
    "output": "\n```python\nresult = df.groupby(('user_id', 'user', 'purchase', 'item')).sum().reset_index()\nresult = result.set_index((('user_id', 'user'), ('purchase', 'item')))\nresult['amount'] = result['purchase', 'amount'] / result.groupby(level=0)['purchase', 'amount'].transform('max')\nresult = result[['amount']]\n```"
  },
  {
    "input": "Problem:\n\nI am working with a dataset containing customer transactions, and I'd like to perform feature engineering by creating a new feature that indicates the total transaction amount for each customer. My dataset is in a pandas DataFrame called `transactions` with the following structure:\n\n| customer_id | transaction_amount |\n|-------------|--------------------|\n| 1           | 100                |\n| 2           | 200                |\n| 1           | 150                |\n| 3           | 300                |\n| 2           | 50                 |\n\nI want to group the data by `customer_id` and calculate the total transaction amount for each customer, and then save this new feature into a variable called `total_amount`.\n\nA:\n\n<code>\nimport pandas as pd\ntransactions = pd.DataFrame({\n    'customer_id': [1, 2, 1, 3, 2],\n    'transaction_amount': [100, 200, 150, 300, 50]\n})\n</code>\ntotal_amount = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ntotal_amount = transactions.groupby('customer_id')['transaction_amount'].sum().reset_index()"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'client_id': [1, 1, 2, 2, 3, 3, 3],\n    'purchase_amount': [100, np.nan, 200, 250, np.nan, 300, 400],\n    'purchase_date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-02', '2023-01-01', '2023-01-04'])\n})\n```\n\nThe DataFrame includes some missing values in the `purchase_amount` column. You want to fill these missing values with the average purchase amount for each client. Additionally, you need to create a new column that contains the cumulative sum of `purchase_amount` for each client, but only considering the non-null values.\n\nHow can you achieve the following modified DataFrame?\n\n```python\n   client_id  purchase_amount purchase_date  avg_purchase client_cumsum\n0          1            100.0    2023-01-01           100.0         100.0\n1          1            100.0    2023-01-02           100.0         200.0\n2          2            200.0    2023-01-01           225.0         200.0\n3          2            250.0    2023-01-03           225.0         450.0\n4          3            333.3    2023-01-02           333.3         333.3\n5          3            300.0    2023-01-01           333.3         633.3\n6          3            400.0    2023-01-04           333.3        1033.3\n```\n\nYou can start your code with:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'client_id': [1, 1, 2, 2, 3, 3, 3],\n    'purchase_amount': [100, np.nan, 200, 250, np.nan, 300, 400],\n    'purchase_date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-02', '2023-01-01', '2023-01-04'])\n})\n```\n\nresult = ... # put solution in this variable",
    "output": "\n```python\ndf['avg_purchase'] = df.groupby('client_id')['purchase_amount'].transform(lambda x: x.fillna(x.mean()))\ndf['purchase_amount'] = df['avg_purchase']\ndf['client_cumsum'] = df.groupby('client_id')['purchase_amount'].cumsum()\nresult = df\n```"
  },
  {
    "input": "Problem:\nYou have a multi-index DataFrame representing sales data for various products over different months. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples(\n    [('Product_A', 'January'), ('Product_A', 'February'), \n     ('Product_B', 'January'), ('Product_B', 'February')],\n    names=['Product', 'Month']\n)\ndata = {\n    'Sales': [1500, 1600, 1300, 1800],\n    'Expenses': [500, 700, 400, 600]\n}\ndf = pd.DataFrame(data, index=index)\n```\n\nYour goal is to normalize the 'Sales' column within each product (i.e., subtract the mean and divide by the standard deviation for each product). You need to add this normalized data to the DataFrame as a new column called 'Normalized_Sales'. The final DataFrame should maintain the multi-index structure.\n\nWhat code should replace the ellipsis to achieve this?\n\n```python\ndf['Normalized_Sales'] = ...\n```",
    "output": "\n```python\ndf['Normalized_Sales'] = df.groupby(level='Product')['Sales'].transform(lambda x: (x - x.mean()) / x.std())\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a data science project where you want to analyze customer purchasing behavior from a dataset containing purchase amounts over time. You have a pandas DataFrame called `transactions` with two columns: `purchase_amount` and `purchase_date`. Your goal is to create a new column called `rolling_average` that calculates the rolling average of the `purchase_amount` over the last 7 days for each transaction.\n\nHere is the initial setup:\n\n```python\nimport pandas as pd\nfrom datetime import datetime\n\ndata = {\n    'purchase_amount': [100, 150, 200, 130, 170, 60, 90, 120, 180, 250],\n    'purchase_date': [\n        '2023-10-01', '2023-10-02', '2023-10-03', \n        '2023-10-05', '2023-10-06', '2023-10-07', \n        '2023-10-08', '2023-10-09', '2023-10-11', '2023-10-12'\n    ]\n}\n\ntransactions = pd.DataFrame(data)\ntransactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\ntransactions = transactions.set_index('purchase_date')\n```\n\nYou need to fill in the blank dot notation to create the `rolling_average` column using a rolling window of 7 days.\n\n```python\ntransactions['rolling_average'] = transactions['purchase_amount'].____(window='7D').mean()\n```\n\nFill in the blank with the appropriate function call.",
    "output": "\n```python\ntransactions['rolling_average'] = transactions['purchase_amount'].rolling(window='7D').mean()\n```"
  },
  {
    "input": "Problem:\nYou are given a pandas DataFrame that contains sales data for different products in different regions. The DataFrame `df` has the following structure:\n\n```\n   product    region     sales\n0     A       North      200\n1     A       South      150\n2     B       North      300\n3     B       South      NaN\n4     C       North      250\n5     C       South      NaN\n6     A       North      NaN\n7     B       South      100\n8     C       North      400\n```\n\nYou need to calculate the total sales for each product by region, while applying the following conditions:\n- If the sales value is `NaN`, treat it as zero for the purpose of summation.\n- After summing, create a new DataFrame that shows `total_sales` for each combination of `product` and `region`.\n\nYour desired output should look like this:\n\n```\n   product    region  total_sales\n0     A       North        200\n1     A       South        150\n2     B       North        300\n3     B       South        100\n4     C       North        650\n5     C       South          0\n```\n\nHere is the starting code:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'B', 'C'],\n    'region': ['North', 'South', 'North', 'South', 'North', 'South', 'North', 'South', 'North'],\n    'sales': [200, 150, 300, None, 250, None, None, 100, 400]\n})\n```\n\nFill in the blank to compute the desired result:\n\n```python\ntotal_sales_df = ...\n```",
    "output": "\n```python\ntotal_sales_df = df.groupby(['product', 'region'], as_index=False)['sales'].sum().fillna(0).rename(columns={'sales': 'total_sales'})\n```"
  },
  {
    "input": "Problem:\nSuppose you have a pandas DataFrame representing sales data for different products across multiple regions, structured as follows:\n\n```python\nimport pandas as pd\ndata = {\n    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'],\n    'product': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],\n    'sales': [200, 150, None, 80, 300, None, 100, 90]\n}\ndf = pd.DataFrame(data)\n```\n\nYour goal is to create a new DataFrame that shows the total sales for each product by region, where if any region has a `None` value for sales, the total for that product should also be `None`. If there are no `None` values, aggregate the sales.\n\nFor example, if you group by `product` and `region`, you should get:\n\n```\nproduct  region  total_sales\nA       North          200.0\nA       South          150.0\nA       East             NaN\nA       West            80.0\nB       North          300.0\nB       South            NaN\nB       East           100.0\nB       West            90.0\n```\n\nYou need to fill in the blank for computing `total_sales` based on the given conditions.\n\n```python\nresult = df.groupby(['product', 'region'])['sales'].___ # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby(['product', 'region'])['sales'].sum().where(df.groupby(['product', 'region'])['sales'].count() == df.groupby(['product', 'region'])['sales'].size())\n```"
  },
  {
    "input": "## Problem\n\nI have a Pandas DataFrame that represents survey results from different demographic groups, and the structure looks something like this:\n\n```\n      Age    Income  Gender    Response\nGroup1 25.0  50000  Male    [Yes, No]\nGroup2 30.0  60000  Female  [Yes, Yes, No]\nGroup3 35.0  70000  Male    [No]\nGroup4 40.0  80000  Female  [Yes]\n```\n\nI want to expand the 'Response' column into multiple binary indicator columns for each unique response (like Yes and No) while also normalizing the 'Income' column to have a mean of 0 and standard deviation of 1. The final DataFrame should have the structure:\n\n```\n      Age   Income       Gender  Yes  No\nGroup1 25.0  -1.341641  Male  1.0  1.0\nGroup2 30.0  -0.447214  Female  1.0  0.0\nGroup3 35.0   0.447214  Male  0.0  1.0\nGroup4 40.0   1.341641  Female  1.0  0.0\n```\n\nHow can I normalize the 'Income' column and expand the 'Response' column appropriately using Pandas?\n\n```python\nimport pandas as pd\n\ndata = {\n    'Age': [25.0, 30.0, 35.0, 40.0],\n    'Income': [50000, 60000, 70000, 80000],\n    'Gender': ['Male', 'Female', 'Male', 'Female'],\n    'Response': [['Yes', 'No'], ['Yes', 'Yes', 'No'], ['No'], ['Yes']]\n}\ndf = pd.DataFrame(data, index=['Group1', 'Group2', 'Group3', 'Group4'])\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\n# Normalize the 'Income' column\ndf['Income'] = (df['Income'] - df['Income'].mean()) / df['Income'].std()\n\n# Expand the 'Response' column into binary indicator columns\nresponse_dummies = df['Response'].explode().str.get_dummies().sum(level=0)\n\n# Combine the original DataFrame with the new response columns\nresult = pd.concat([df.drop(columns='Response'), response_dummies], axis=1)\n```"
  },
  {
    "input": "Problem:\nYou are working on a dataset containing information about customer transactions, and you have the following DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'CustomerID': [1, 1, 2, 2, 3, 3, 3],\n    'Transaction': ['T1', 'T2', 'T1', 'T3', 'T1', 'T2', 'T3'],\n    'Amount': [250, 150, 300, 500, 100, 200, 300],\n    'Date': ['2023-01-10', '2023-01-15', '2023-01-12', '2023-01-20', '2023-01-05', '2023-01-07', '2023-01-09']\n}\n\ndf = pd.DataFrame(data)\ndf['Date'] = pd.to_datetime(df['Date'])\n```\n\nYou want to create a new DataFrame that aggregates the total spent (`Amount`) by each customer across all their transactions, while also including the latest transaction date per customer. The resulting DataFrame should be structured with 'CustomerID' as the index, and columns should include 'TotalSpent' and 'LastTransactionDate'. \n\nHow can you achieve this?\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby('CustomerID').agg(TotalSpent=('Amount', 'sum'), LastTransactionDate=('Date', 'max')).reset_index().set_index('CustomerID')\n</code>"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing information about houses, including features such as the number of bedrooms, size in square feet, age of the house, and the sale price. You aim to preprocess this dataset by scaling the numeric features and encoding categorical features before fitting a linear regression model. The dataset is stored in a pandas DataFrame. \n\nYou need to apply MinMaxScaler on the numeric columns and OneHotEncoder for categorical columns. After preprocessing, you want to retrieve the transformed features into a new DataFrame called `X_transformed` that combines both encoded and scaled features.\n\nGiven the DataFrame `df` with features 'bedrooms', 'size_sqft', 'age', and 'location' (where 'location' is categorical), fill in the blank to create the transformation.\n\nA:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndf = load_data()\nnumeric_features = ['bedrooms', 'size_sqft', 'age']\ncategorical_features = ['location']\n\n# Define the transformations\ntransformer = ColumnTransformer(\n    transformers=[\n        ('num', MinMaxScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ]\n)\n\nX_transformed = ...  # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nX_transformed = transformer.fit_transform(df)\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing sales data for a retail store, structured in a multi-index DataFrame, where the first index level represents the product category and the second level represents the individual product names. The DataFrame looks like this:\n\n```\n                      sales   returns\ncategory   product                     \nElectronics Phone      1000      50\n           Laptop       500      20\nClothing   T-Shirt      300      15\n           Jeans        800      30\nGrocery    Apples       1200      40\n           Bananas      900      35\n```\n\nYou want to calculate the sales return ratio for each product, defined as the number of returns divided by the number of sales. After that, you want to normalize these return ratios using Min-Max normalization to scale the values between 0 and 1.\n\nCan you provide the code that will accomplish this task and store the normalized results in a new column called \"normalized_return_ratio\"?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    ('Electronics', 'Phone'): [1000, 50],\n    ('Electronics', 'Laptop'): [500, 20],\n    ('Clothing', 'T-Shirt'): [300, 15],\n    ('Clothing', 'Jeans'): [800, 30],\n    ('Grocery', 'Apples'): [1200, 40],\n    ('Grocery', 'Bananas'): [900, 35]\n}\n\ndf = pd.DataFrame.from_dict(data, orient='index', columns=['sales', 'returns'])\ndf.index = pd.MultiIndex.from_tuples(df.index)\n</code>\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['return_ratio'] = df['returns'] / df['sales']\nmin_ratio = df['return_ratio'].min()\nmax_ratio = df['return_ratio'].max()\ndf['normalized_return_ratio'] = (df['return_ratio'] - min_ratio) / (max_ratio - min_ratio)\nresult = df\n</code>"
  },
  {
    "input": "Problem:\nI have a PyTorch tensor `data` representing some samples and their features:\n\n```\ndata = tensor([[1.0, 2.0, 3.0],\n                [4.0, 5.0, 6.0],\n                [7.0, 8.0, 9.0]])\n```\n\nAnd I have a boolean mask `mask` indicating which elements I want to select:\n\n```\nmask = tensor([True, False, True])\n```\n\nI want to apply this mask to retain only the first and the third rows of the `data` tensor. The expected result is a new tensor as follows:\n\n```\ntensor([[1.0, 2.0, 3.0],\n        [7.0, 8.0, 9.0]])\n```\n\nHow can I accomplish this using PyTorch?\n\nA:\n<code>\nimport torch\n\ndata = torch.tensor([[1.0, 2.0, 3.0],\n                     [4.0, 5.0, 6.0],\n                     [7.0, 8.0, 9.0]])\nmask = torch.tensor([True, False, True])\n\nresult = ... # put solution in this variable\n### BEGIN SOLUTION",
    "output": "\n<code>\nresult = data[mask]\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data from a fictional online store as follows:\n\n| Category | Product | Sales  | Returns | Profit |\n|----------|---------|--------|---------|--------|\n| A        | P1     | 100    | 5       | 70     |\n| A        | P2     | 150    | 10      | 80     |\n| B        | P3     | 200    | 0       | 120    |\n| B        | P4     | 50     | 5       | 20     |\n| C        | P5     | 300    | 15      | 200    |\n| A        | P6     | NaN    | 5       | NaN    |\n| C        | P7     | 400    | NaN     | 300    |\n\nYou want to compute the following for each category:\n1. The total sales, ignoring NaN values.\n2. The total profit, replacing any NaN values with 0.\n3. The returns as a percentage of total sales, with NaN values handled appropriately.\n\nThe expected output should look like this:\n\n| Category | Total_Sales | Total_Profit | Returns_Percent |\n|----------|-------------|---------------|------------------|\n| A        | 250         | 150           | 4.0              |\n| B        | 250         | 140           | 2.0              |\n| C        | 700         | 500           | 2.14             |\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Category': ['A', 'A', 'B', 'B', 'C', 'A', 'C'],\n    'Product': ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7'],\n    'Sales': [100, 150, 200, 50, 300, np.nan, 400],\n    'Returns': [5, 10, 0, 5, 15, 5, np.nan],\n    'Profit': [70, 80, 120, 20, 200, np.nan, 300]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df.groupby('Category').agg(\n    Total_Sales=('Sales', 'sum'),\n    Total_Profit=('Profit', lambda x: x.fillna(0).sum()),\n    Returns_Percent=('Returns', lambda x: (x.sum() / df['Sales'].sum()) * 100 if df['Sales'].sum() > 0 else 0)\n).reset_index()"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame that tracks sales data for different products across various stores, as shown below:\n\n```\n      store     product  sales\n0      StoreA     Apples   200\n1      StoreB     Apples   150\n2      StoreA     Bananas  300\n3      StoreB     Bananas  NaN\n4      StoreA     Cherries 100\n5      StoreB     Cherries 250\n```\n\nYour task is to create a new DataFrame where you calculate the total sales for each product, but with a condition: if any of the sales entries for that product are null (NaN), replace that product's total sales with the average of the available sales values for that product. If there are no sales recorded for a product at all (all values are NaN), the resulting entry should reflect this as NaN.\n\nHere's how the initial DataFrame can be created:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'store': ['StoreA', 'StoreB', 'StoreA', 'StoreB', 'StoreA', 'StoreB'],\n    'product': ['Apples', 'Apples', 'Bananas', 'Bananas', 'Cherries', 'Cherries'],\n    'sales': [200, 150, 300, np.nan, 100, 250]\n}\ndf = pd.DataFrame(data)\n```\n\nBased on the above logic, fill in the `result` variable with the correct code to achieve the desired DataFrame.\n\nA:\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby('product')['sales'].apply(lambda x: x.mean() if x.isnull().any() else x.sum()).reset_index()\n```"
  },
  {
    "input": "Problem:\n\nI am working on a recommendation system using PyTorch, where I need to compute the cosine similarity between user embeddings and item embeddings. The user embeddings are stored in a tensor of shape (1000, 128) and the item embeddings in another tensor of shape (5000, 128). I want to extract the top K most similar items for each user based on cosine similarity. \n\nHere is my current setup:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\nuser_embeddings = torch.randn(1000, 128)\nitem_embeddings = torch.randn(5000, 128)\n```\n\nHow do I compute the cosine similarity and then get the top K similar items for each user?\n\n```python\nK = 5\nsimilarity = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nsimilarity = F.cosine_similarity(user_embeddings.unsqueeze(1), item_embeddings.unsqueeze(0), dim=2)\ntop_k_indices = torch.topk(similarity, K, dim=1).indices\nEND SOLUTION\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames, sales and returns, with the following data:\n\nsales:\n``` \n   order_id product_id quantity sold_date\n0        1        A         10  2023-01-01\n1        2        B         20  2023-01-02\n2        3        A         15  2023-01-03\n3        4        C         25  2023-01-04\n```\n\nreturns:\n```\n   order_id product_id quantity return_date\n0        1        A          5  2023-01-05\n1        3        A          10  2023-01-06\n2        5        D          7   2023-01-07\n```\n\nYou need to merge these two DataFrames such that you calculate the total quantity sold (subtracting any returns) for each product. The resulting DataFrame should contain the following columns: product_id, total_sold, and last_sold_date. The 'total_sold' should reflect the total quantity sold minus the quantity returned. The 'last_sold_date' should keep the maximum (latest) sold_date for each product.\n\nYour expected DataFrame should look like this:\n```\n  product_id total_sold last_sold_date\n0          A         15    2023-01-03\n1          B         20    2023-01-02\n2          C         25    2023-01-04\n3          D          0    NaT\n```\n\nYou tried using the following code:\n```\nresult = pd.merge(sales, returns, on='product_id', how='outer')\n```\n\nHowever, this did not give you the correct 'total_sold' and 'last_sold_date'.\n\nHow can you achieve the expected result in one line of code?\nA:\n```python\nimport pandas as pd\nsales = pd.DataFrame({'order_id': [1, 2, 3, 4], \n                      'product_id': ['A', 'B', 'A', 'C'], \n                      'quantity': [10, 20, 15, 25], \n                      'sold_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']})\nreturns = pd.DataFrame({'order_id': [1, 3, 5], \n                        'product_id': ['A', 'A', 'D'], \n                        'quantity': [5, 10, 7], \n                        'return_date': ['2023-01-05', '2023-01-06', '2023-01-07']})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```python\n\n\n```",
    "output": "\n```python\nresult = sales.groupby('product_id').agg(total_sold=('quantity', 'sum'), last_sold_date=('sold_date', 'max')).reset_index().merge(returns.groupby('product_id').agg(total_return=('quantity', 'sum')).reset_index(), on='product_id', how='outer').fillna(0).assign(total_sold=lambda x: x['total_sold'] - x['total_return']).drop(columns='total_return')\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing several numeric features, and you want to perform feature engineering by creating polynomial features to enhance the performance of your machine learning model. However, you want to avoid generating too many features. You would like to create polynomial features of degree 3, while limiting the number of output features to 10. How can you achieve this using the `PolynomialFeatures` class from `sklearn.preprocessing`? \n\nAssuming you have already imported the necessary libraries and loaded your data into a variable called `X`, your task is to generate the polynomial features and store them in the variable `poly_features`.\n\n```python\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Load your data into X (assume it's already done)\nX = load_data()\n\npoly = PolynomialFeatures(degree=3, max_degree=10) # Fill in the solution here\n```\n\npoly_features = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\npoly = PolynomialFeatures(degree=3, include_bias=False)\npoly_features = poly.fit_transform(X)[:, :10]\n```"
  },
  {
    "input": "Problem:\nI have a dataset containing various features and target variables. I would like to apply a log transformation to specific features in my dataset to stabilize variance and make the data more normally distributed. The features I want to transform are specified in a list called `features_to_transform`. After transforming these features, I wish to normalize the entire dataset using StandardScaler from scikit-learn to ensure that they are on a similar scale. Finally, I want to obtain the transformed and normalized features as a NumPy array.\n\nHere's the setup I have:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.DataFrame({\n    'feature1': [1, 2, 3, 4],\n    'feature2': [10, 20, 30, 40],\n    'feature3': [100, 200, 300, 400]\n})\nfeatures_to_transform = ['feature1', 'feature2']\n```\n\nThe final transformed and normalized features should be stored in the variable `transformed_data`. \n\nA:\n```python\nimport numpy as np\n\n# Log transform the specified features\ndata[features_to_transform] = np.log(data[features_to_transform])\n\n# Normalize the entire dataset\nscaler = StandardScaler()\ntransformed_data = ... # put solution in this variable\n```",
    "output": "\n```python\ntransformed_data = scaler.fit_transform(data)\n```"
  },
  {
    "input": "Problem:\nYou are given a DataFrame containing sales data for various products across different stores. The DataFrame has a multi-index structure, where the first index is the 'Store' and the second index is 'Product'. The columns include 'Sales' (numeric) and 'Region' (categorical, indicating the region of the store). Here's a sample DataFrame:\n\n```python\nimport pandas as pd\n\narrays = [['Store A', 'Store A', 'Store B', 'Store B', 'Store C', 'Store C'],\n          ['Product 1', 'Product 2', 'Product 1', 'Product 3', 'Product 2', 'Product 3']]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Store', 'Product'))\ndata = {'Sales': [200, 300, 150, 400, 100, 250], 'Region': ['East', 'East', 'West', 'West', 'North', 'North']}\ndf = pd.DataFrame(data, index=index)\n```\n\nYour task is to normalize the 'Sales' values within each 'Store' so that the sales values for each product sum to 1. Within the same DataFrame, after normalization, encode the 'Region' column into categorical integers, where East, North, and West correspond to 0, 1, and 2, respectively.\n\nAfter performing these operations, store the resulting DataFrame in a variable named `result`.\n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.copy()\nresult['Sales'] = result.groupby(level='Store')['Sales'].transform(lambda x: x / x.sum())\nresult['Region'] = result['Region'].astype('category').cat.codes\n```"
  },
  {
    "input": "Problem:\n\nI have two DataFrames, df_users and df_orders, representing user information and their corresponding orders as follows:\n\ndf_users\n```\n   user_id username\n0        1    Alice\n1        2      Bob\n2        3  Charlie\n```\n\ndf_orders\n```\n   order_id user_id  order_date amount\n0        101       1  2023-01-05   150\n1        102       2  2023-01-08   200\n2        103       2  2023-01-10   250\n3        104       4  2023-01-12   300\n```\n\nI need to merge these DataFrames such that you keep all users from df_users. For those users who have made orders, the corresponding order details should appear alongside their information. Where users haven\u2019t placed any orders, the order details should show NaNs. Furthermore, I want to replace any NaN values in the 'amount' column with 0, to represent no orders.\n\nThe expected output must look like this:\n```\n   user_id username  order_id  order_date amount\n0        1    Alice      101  2023-01-05   150.0\n1        2      Bob      102  2023-01-08   200.0\n2        2      Bob      103  2023-01-10   250.0\n3        3  Charlie      NaN         NaN     0.0\n```\n\nI attempted to merge using `pd.merge` but didn't know how to handle the NaN values correctly. \n\nA:\n<code>\nimport pandas as pd\n\ndf_users = pd.DataFrame({'user_id': [1, 2, 3],\n                         'username': ['Alice', 'Bob', 'Charlie']})\ndf_orders = pd.DataFrame({'order_id': [101, 102, 103, 104],\n                          'user_id': [1, 2, 2, 4],\n                          'order_date': ['2023-01-05', '2023-01-08', '2023-01-10', '2023-01-12'],\n                          'amount': [150, 200, 250, 300]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = pd.merge(df_users, df_orders, on='user_id', how='left').fillna({'amount': 0})"
  },
  {
    "input": "Problem:\nI have two pandas DataFrames that represent sales data and returns data. The sales DataFrame contains the following columns: 'order_id', 'customer_id', 'date', 'amount'. The returns DataFrame contains 'order_id', 'return_date', and 'amount_returned'. I need to merge these two DataFrames on 'order_id', ensuring that if an order has been returned, the 'amount' in the sales DataFrame is updated to reflect the return. Additionally, I want to keep the original columns intact while adding a new 'final_amount' column that shows the amount after returns have been deducted.\n\nHere are my DataFrames:\n\nsales_df:\n   order_id  customer_id       date  amount\n0        1           A 2023-01-01     100\n1        2           B 2023-01-02     150\n2        3           A 2023-01-03     200\n3        4           C 2023-01-04     300\n\nreturns_df:\n   order_id return_date  amount_returned\n0        2  2023-01-05               50\n1        3  2023-01-06              100\n\nI want the final DataFrame to look like this:\n   order_id  customer_id       date  amount  final_amount\n0        1           A 2023-01-01     100          100\n1        2           B 2023-01-02     150          100\n2        3           A 2023-01-03     200          100\n3        4           C 2023-01-04     300          300\n\nHow can I accomplish this with pandas? \n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'order_id': [1, 2, 3, 4],\n    'customer_id': ['A', 'B', 'A', 'C'],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']),\n    'amount': [100, 150, 200, 300]\n})\n\nreturns_df = pd.DataFrame({\n    'order_id': [2, 3],\n    'return_date': pd.to_datetime(['2023-01-05', '2023-01-06']),\n    'amount_returned': [50, 100]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmerged_df = sales_df.merge(returns_df, on='order_id', how='left')\nmerged_df['final_amount'] = merged_df['amount'] - merged_df['amount_returned'].fillna(0)\nresult = merged_df[['order_id', 'customer_id', 'date', 'amount', 'final_amount']]\n</code>"
  },
  {
    "input": "Problem:\nYou are given a dataset with user engagement metrics for an online platform over a period of time. The data looks like this:\n\n```python\nimport pandas as pd\nimport io\n\ndata = \"\"\"user,date,session_length,activity\nuser1,2023-01-01,30,click\nuser1,2023-01-02,45,scroll\nuser2,2023-01-01,20,click\nuser2,2023-01-03,35,click\nuser3,2023-01-01,50,scroll\nuser1,2023-01-03,60,scroll\nuser3,2023-01-02,40,click\"\"\"\ndf = pd.read_csv(io.StringIO(data))\n```\n\nYou want to pivot this DataFrame to show the total session length for each user by date, while also maintaining a multi-index where the first level is the user and the second level is the date. The activity column is not needed for the pivot operation.\n\nHow can you achieve this in Python using pandas? Fill in the following code snippet to create the desired pivot table:\n\n```python\nresult = df.pivot_table(index='user', columns='date', values='session_length', aggfunc='sum')\n```\n\nThis code should set the multi-index where the first level is the user and second level is the date, with total session lengths as the values.",
    "output": "\n```python\nresult = df.pivot_table(index='user', columns='date', values='session_length', aggfunc='sum').reset_index().set_index(['user'])\n```"
  },
  {
    "input": "### Problem:\nYou have two DataFrames representing sales data for different products. The first DataFrame (`df1`) contains historical sales data, while the second DataFrame (`df2`) contains updated sales information for the same products, but with some new sales entries and updated quantities. \n\n**df1:**\n```\n+---------+-----------+----------+-----------+\n| Product | SaleDate  | Quantity | Sales     |\n+---------+-----------+----------+-----------+\n| A       | 2022-01-01| 10       | 100       |\n| B       | 2022-01-05| 5        | 50        |\n| A       | 2022-01-10| 8        | 80        |\n| C       | 2022-01-15| 12       | 120       |\n+---------+-----------+----------+-----------+\n```\n\n**df2:**\n```\n+---------+-----------+----------+-----------+\n| Product | SaleDate  | Quantity | Sales     |\n+---------+-----------+----------+-----------+\n| A       | 2022-01-01| 5        | 50        |\n| A       | 2022-01-10| 10       | 100       |\n| D       | 2022-01-20| 7        | 70        |\n| B       | 2022-01-05| 6        | 60        |\n+---------+-----------+----------+-----------+\n```\n\nYour goal is to merge `df1` and `df2` into a single DataFrame such that:\n- For products that exist in both DataFrames, the `Quantity` and `Sales` should be summed up.\n- For products that exist only in one of the DataFrames, they should be included as is.\n- The resulting DataFrame should be sorted by `SaleDate` in ascending order.\n\nThe expected resultant format should look like this after the merge:\n```\n+---------+-----------+----------+-----------+\n| Product | SaleDate  | Quantity | Sales     |\n+---------+-----------+----------+-----------+\n| A       | 2022-01-01| 15       | 150       |\n| B       | 2022-01-05| 11       | 110       |\n| A       | 2022-01-10| 18       | 180       |\n| C       | 2022-01-15| 12       | 120       |\n| D       | 2022-01-20| 7        | 70        |\n+---------+-----------+----------+-----------+\n```\n\nHere's a partial implementation:\n```python\nimport pandas as pd\n\n# Creating the DataFrames df1 and df2\ndf1 = pd.DataFrame({\n    'Product': ['A', 'B', 'A', 'C'],\n    'SaleDate': ['2022-01-01', '2022-01-05', '2022-01-10', '2022-01-15'],\n    'Quantity': [10, 5, 8, 12],\n    'Sales': [100, 50, 80, 120]\n})\n\ndf2 = pd.DataFrame({\n    'Product': ['A', 'A', 'D', 'B'],\n    'SaleDate': ['2022-01-01', '2022-01-10', '2022-01-20', '2022-01-05'],\n    'Quantity': [5, 10, 7, 6],\n    'Sales': [50, 100, 70, 60]\n})\n\n# Implementation should go here\nresult = ...\n```",
    "output": "\n```python\nresult = pd.concat([df1, df2]).groupby(['Product', 'SaleDate'], as_index=False).sum().sort_values('SaleDate')\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing user information, which includes various features such as age, income, and purchasing behavior. However, you notice that the data contains outliers and missing values, which could affect model training. Your goal is to preprocess this dataset by removing outliers based on the Z-score method and imputing missing values with the median of the respective columns.\n\nYou have the following code snippet for the preprocessing:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\n\ndata = pd.DataFrame({\n    'age': [25, 30, 35, 40, 29, np.nan, 100, 28],\n    'income': [50000, 60000, 70000, 80000, np.nan, 500000, 120000, 58000],\n    'purchases': [10, 15, 12, 18, 20, 50, 1, np.nan]\n})\n\n# Fill in the missing values with the median\ndata_imputed = data.fillna(data.median())\n\n# Remove outliers based on Z-score\ndata_no_outliers = data_imputed[(np.abs(zscore(data_imputed.select_dtypes(include=[np.number]))) < 3).all(axis=1)]\n```\n\nHowever, the filtering step is not implemented correctly in the provided code snippet. To complete this, you need to replace the `...` in the following line to achieve the filtering correctly:\n\n```python\ndata_no_outliers = data_imputed[...]\n```\n\nFill in the blank with the appropriate condition to filter out the outliers. \n\nBEGIN SOLUTION\n```python\ndata_no_outliers = data_imputed[(np.abs(zscore(data_imputed.select_dtypes(include=[np.number]))) < 3).all(axis=1)]\n```",
    "output": "\n```python\ndata_no_outliers = data_imputed[(np.abs(zscore(data_imputed.select_dtypes(include=[np.number]))) < 3).all(axis=1)]\n```"
  },
  {
    "input": "Problem:\nI have a Pandas DataFrame which contains information about sales transactions with hierarchical column names representing different categories. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    ('Store', 'North'): [100, 150, 200],\n    ('Store', 'South'): [200, 250, 300],\n    ('Store', 'East'): [300, 350, 400],\n    ('Store', 'West'): [400, 450, 500],\n    ('Returns', 'North'): [10, 15, 20],\n    ('Returns', 'South'): [20, 25, 30],\n    ('Returns', 'East'): [30, 35, 40],\n    ('Returns', 'West'): [40, 45, 50]\n}\n\ndf = pd.DataFrame(data, index=['Q1', 'Q2', 'Q3'])\n```\n\nI want to calculate the total sales after accounting for returns and create a new multi-index DataFrame that shows these totals under a new level called `Total` with two sub-levels: `Store` and `Returns`. The output DataFrame should look like this:\n\n```\n           Store          Returns\n           North South East West\nTotal\nQ1        90    180  270  360\nQ2       135    225  315  405\nQ3       180    270  360  450\n```\n\nHow can I create this new DataFrame? Please provide the code to fill in the missing part.\n\nA:\n```python\ntotals = ... # put solution in this variable\n```",
    "output": "\n```python\ntotals = pd.DataFrame({\n    ('Total', 'Store'): df[('Store', '')].sum(axis=1) - df[('Returns', '')].sum(axis=1),\n    ('Total', 'Returns'): df[('Returns', '')].sum(axis=1)\n}).set_index(df.index)\n```"
  },
  {
    "input": "### Problem\nYou are working with two datasets containing sales records. The first DataFrame, `sales_data`, contains sales information with possible duplicate entries. The second DataFrame, `discounts_data`, holds discount percentages applicable based on item categories. Your goal is to combine these datasets in a way that calculates the final sales amount after applying the respective discounts.\n\n`sales_data`:\n```\n   item_id    category  amount\n0       A      toys      100\n1       B      books     200\n2       A      toys      150\n3       C      books      300\n4       A      toys      200\n```\n\n`discounts_data`:\n```\n   category  discount\n0      toys      0.10\n1      books     0.15\n```\n\n1. First, remove duplicate entries in the `sales_data` based on the `item_id`, keeping the last occurrence.\n2. Calculate the final sales amount for each category after applying the discount. \n3. After merging, your final DataFrame should include `item_id`, `category`, `amount`, and `final_amount` (i.e., amount after discount).\n\nPlease write the appropriate code to complete the following task:\n```python\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'item_id': ['A', 'B', 'A', 'C', 'A'],\n    'category': ['toys', 'books', 'toys', 'books', 'toys'],\n    'amount': [100, 200, 150, 300, 200]\n})\n\ndiscounts_data = pd.DataFrame({\n    'category': ['toys', 'books'],\n    'discount': [0.10, 0.15]\n})\n\nresult = ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nsales_data = sales_data.drop_duplicates(subset='item_id', keep='last')\nmerged_data = pd.merge(sales_data, discounts_data, on='category')\nmerged_data['final_amount'] = merged_data['amount'] * (1 - merged_data['discount'])\nresult = merged_data[['item_id', 'category', 'amount', 'final_amount']]\n```"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame that contains the sales data of different products across multiple months. The DataFrame has columns 'product_id', 'month', 'sales', and 'returns', where 'returns' can contain null values. Your goal is to calculate the net sales by subtracting the returns from the sales and also to categorize the net sales into performance tiers: \"poor\" for negative or zero net sales, \"average\" for net sales between 1 and 100, and \"excellent\" for net sales above 100. \n\nThe initial DataFrame is as follows:\n```python\ndf = pd.DataFrame({\n    'product_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02'],\n    'sales': [150, 200, 50, 70, 300, 0],\n    'returns': [10, pd.NA, 5, 20, pd.NA, 5]\n})\n```\n\nYou attempted to calculate the net sales and categorize them but encountered issues when handling the null values in the returns column. Here is an incomplete approach you tried:\n```python\ndf['net_sales'] = df['sales'] - df['returns']\ndf['performance'] = df['net_sales'].apply(lambda x: 'excellent' if x > 100 else ('average' if x > 0 else 'poor'))\n```\n\nHowever, this code resulted in errors due to null values interfering with the calculations. \n\nA:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02'],\n    'sales': [150, 200, 50, 70, 300, 0],\n    'returns': [10, pd.NA, 5, 20, pd.NA, 5]\n})\n```\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf['returns'] = df['returns'].fillna(0)\ndf['net_sales'] = df['sales'] - df['returns']\ndf['performance'] = df['net_sales'].apply(lambda x: 'excellent' if x > 100 else ('average' if x > 0 else 'poor'))\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products and regions, structured as follows:\n\n```plaintext\n    Product    Region  Sales\n0  Widget A      East   150\n1  Widget B      West   200\n2  Widget C      North  250\n3  Widget A      South  300\n4  Widget B      East   400\n5  Widget C      West   NaN\n6  Widget A      East   NaN\n7  Widget B      North  450\n```\n\nYou want to calculate the average sales per product while disregarding any NaN values, and you also want to append this average to the DataFrame as a new column called 'Avg_Sales'. The result should still allow for the regions and products to be displayed.\n\nYour desired output should look like this:\n\n```plaintext\n    Product    Region  Sales  Avg_Sales\n0  Widget A      East   150       225.0\n1  Widget B      West   200       283.33\n2  Widget C      North  250       250.0\n3  Widget A      South  300       225.0\n4  Widget B      East   400       283.33\n5  Widget C      West   NaN       250.0\n6  Widget A      East   NaN       225.0\n7  Widget B      North  450       283.33\n```\n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Product': ['Widget A', 'Widget B', 'Widget C', 'Widget A', 'Widget B', 'Widget C', 'Widget A', 'Widget B'],\n    'Region': ['East', 'West', 'North', 'South', 'East', 'West', 'East', 'North'],\n    'Sales': [150, 200, 250, 300, 400, np.nan, np.nan, 450]\n})\n```\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n```python\n\n",
    "output": "\n```python\navg_sales = df.groupby('Product')['Sales'].mean().reset_index()\navg_sales.rename(columns={'Sales': 'Avg_Sales'}, inplace=True)\ndf = df.merge(avg_sales, on='Product', how='left')\n```"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame that contains the scores of different students across multiple subjects, structured as follows:\n\n```python\nimport pandas as pd\ndf = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Charlie'],\n    'Math': [88, 92, 75],\n    'Science': [90, 95, 80],\n    'English': [85, 89, 70]\n})\n```\n\nI want to normalize the scores for each subject to be between 0 and 1, and I would like the resulting DataFrame to maintain the same structure, with scores replaced by their normalized values.\n\nCan you provide the correct code that fills in the blank below to accomplish this normalization using the Min-Max Scaling technique? \n\nA:\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnormalized_scores = ... # put solution in this variable\n```",
    "output": "\n```python\nnormalized_scores = scaler.fit_transform(df[['Math', 'Science', 'English']])\ndf[['Math', 'Science', 'English']] = normalized_scores\n```"
  },
  {
    "input": "Problem:\nSuppose you have the following DataFrame containing sales data for a small retail store:\n\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['T-shirt', 'Jeans', 'T-shirt', 'Shoes', 'Shoes', 'Jeans', 'Hat', None],\n    'sales': [20, 15, None, 30, 25, 10, 5, 12],\n    'returns': [1, None, 0, 5, 2, 1, None, 1]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to calculate the net sales for each product. Net sales can be defined as total sales minus total returns. If any value in sales or returns is missing (i.e., null), it should be treated as 0 for the computation, but the product should still be included in the final output.\n\nYou want the result to be a DataFrame with columns for the product name and the corresponding net sales, aggregated by product. The expected output for the DataFrame should look like this:\n\n```\n    product    net_sales\n0   Hat               5\n1   Jeans            24\n2   Shoes            53\n3   T-shirt          19\n```\n\nYour current attempt to aggregate the data is not including the null checks properly. You tried:\n\n```python\nresult = df.groupby('product').agg({'sales': 'sum', 'returns': 'sum'})\n```\n\nHowever, it does not handle the null values correctly. How can you adjust your code? \n\nA:\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['T-shirt', 'Jeans', 'T-shirt', 'Shoes', 'Shoes', 'Jeans', 'Hat', None],\n    'sales': [20, 15, None, 30, 25, 10, 5, 12],\n    'returns': [1, None, 0, 5, 2, 1, None, 1]\n}\n\ndf = pd.DataFrame(data)\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = df.groupby('product').agg(net_sales=('sales', lambda x: x.fillna(0).sum() - df.loc[x.index, 'returns'].fillna(0).sum())).reset_index()\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data from a retail store with missing values. Each row represents a transaction, including columns for the product category, transaction amount, and discount applied. Your task is to compute a new column called 'final_price' that represents the price after applying the discount, ensuring that you handle cases where either the discount or the transaction amount is null (treat null discounts as 0 and null amounts as 0). \n\nThe DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'C', 'B'],\n    'amount': [100, np.nan, 200, 150, 300],\n    'discount': [10, 20, np.nan, 5, np.nan]\n})\n```\n\nYou are asked to complete the following line of code to create the `final_price` column using a custom function:\n\n```python\ndf['final_price'] = df.apply(lambda row: row['amount'] * (1 - (row['discount'] / 100)) if pd.notnull(row['amount']) and pd.notnull(row['discount']) else np.nan, axis=1)\n```\n\nHowever, using the above code, you find it repetitive and want a more elegant solution for handling null values gracefully using DataFrame methods. \n\nFill in the blank to replace the lambda function:\n\n```python\ndf['final_price'] = df.apply(__________, axis=1)\n```\n\nDesired output when applied:\n```python\n   category  amount  discount  final_price\n0        A   100.0      10.0          90.0\n1        B     NaN      20.0           NaN\n2        A   200.0       NaN         200.0\n3        C   150.0       5.0         142.5\n4        B   300.0       NaN         300.0\n```",
    "output": "\n```python\ndf['final_price'] = df.apply(lambda row: row['amount'] * (1 - (row['discount'] / 100)) if pd.notnull(row['amount']) else 0, axis=1).fillna(0)\n```"
  },
  {
    "input": "Problem:\nYou are working with a sales DataFrame containing daily transactions of products in different regions. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Date': pd.date_range(start='2023-01-01', periods=6),\n    'Region': ['East', 'West', 'East', 'West', 'East', 'West'],\n    'Sales': [200, np.nan, 150, 250, np.nan, 300],\n    'Returns': [np.nan, 50, 20, np.nan, 10, 70]\n})\n```\n\nYour goal is to calculate the 'Net Sales' for each region, considering that 'Net Sales' is defined as 'Sales' minus 'Returns'. If there are any NaN values in either 'Sales' or 'Returns', those rows should be excluded from the calculation of 'Net Sales'. The expected result for 'Net Sales' should be stored in a new column in the DataFrame.\n\nHow can I achieve this?\n\n```python\ndf = ... # put solution in this variable\n```",
    "output": "\n```python\ndf = df.dropna(subset=['Sales', 'Returns'])\ndf['Net Sales'] = df['Sales'] - df['Returns']\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames, df_sales and df_products, that I need to work with. The df_sales DataFrame contains sales data, including product IDs and quantities sold, while df_products contains information about the products. The goal is to combine these two DataFrames on the product ID, remove any duplicate sales entries (keeping the first occurrence), and replace any missing product details in the merged DataFrame with placeholder values.\n\nHere are the DataFrames:\n\ndf_sales:\n```\n   product_id  quantity\n0           1        10\n1           1        15\n2           2        5\n3           3        20\n4           2        10\n```\n\ndf_products:\n```\n   product_id    product_name   category\n0           1  Product A       Type X\n1           2  Product B       Type Y\n2           3  Product C       Type Z\n3           4  Product D       Type W\n```\n\nAfter merging, the expected DataFrame should look like this:\n```\n   product_id  quantity   product_name   category\n0           1        10      Product A   Type X\n1           1        15      Product A   Type X\n2           2         5      Product B   Type Y\n3           3        20      Product C   Type Z\n4           2        10      Product B   Type Y\n```\n\nI want to make sure that any missing product details (like product_id 4 in df_products) are replaced with \"Missing Product\" in the `product_name` and \"Unknown Category\" in the `category`.\n\nSo far, I attempted the following:\n\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({'product_id': [1, 1, 2, 3, 2],\n                         'quantity': [10, 15, 5, 20, 10]})\n\ndf_products = pd.DataFrame({'product_id': [1, 2, 3, 4],\n                            'product_name': ['Product A', 'Product B', 'Product C', 'Product D'],\n                            'category': ['Type X', 'Type Y', 'Type Z', 'Type W']})\n\nmerged_df = pd.merge(df_sales, df_products, on='product_id')\nmerged_df = merged_df.drop_duplicates()\nmerged_df['product_name'].fillna(\"Missing Product\", inplace=True)\nmerged_df['category'].fillna(\"Unknown Category\", inplace=True)\n```\n\nI need to complete the merging and handle the duplicates properly. What should I put in the following placeholder for completing the code?\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = merged_df.fillna({'product_name': 'Missing Product', 'category': 'Unknown Category'})\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset of image embeddings produced from a neural network. Each image is represented by a tensor of shape `[batch_size, embedding_dimension]`, where `batch_size` is 200 and `embedding_dimension` is 512. You want to compute the cosine similarity between each pair of embeddings to create a similarity matrix. To do this, you need to normalize the embeddings first. You have the following tensor containing the embeddings:\n\n```python\nimport torch\n\nembeddings = torch.randn(200, 512)  # Example tensor of image embeddings\n```\n\nTo proceed with calculating the cosine similarity, which requires the embeddings to be normalized, how would you normalize the `embeddings` tensor so that each row has a Euclidean norm of 1?\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = embeddings / embeddings.norm(dim=1, keepdim=True)\n</code>\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame consisting of user transaction data that looks like this:\n\n| user_id | transaction_date | amount  | status  |\n|---------|------------------|---------|---------|\n| 1       | 2023-01-01       | 100.00  | Success |\n| 1       | 2023-01-02       | 50.00   | Failed  |\n| 2       | 2023-01-01       | 200.00  | Success |\n| 2       | 2023-01-03       | NaN     | Success |\n| 1       | 2023-01-04       | NaN     | Failed  |\n\nI want to compute the total successful transaction amount for each user while ensuring that any failed transactions or null values in the amount column do not affect the calculations. I also want to create a new column that provides a summary of the status of each user, where the user is classified as 'Active' if they have a successful transaction and 'Inactive' otherwise.\n\nMy code starts like this:\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'user_id': [1, 1, 2, 2, 1],\n    'transaction_date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-03', '2023-01-04'],\n    'amount': [100.0, 50.0, 200.0, np.nan, np.nan],\n    'status': ['Success', 'Failed', 'Success', 'Success', 'Failed']\n}\n\ndf = pd.DataFrame(data)\n\nI have already grouped the DataFrame by user_id and am using apply but couldn't finalize the summary and aggregation.\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df.groupby('user_id').agg(\n    total_successful_amount=('amount', lambda x: x[df['status'] == 'Success'].sum()),\n    status_summary=('status', lambda x: 'Active' if 'Success' in x.values else 'Inactive')\n).reset_index()\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products across several months. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Product': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'Month': ['2022-01', '2022-02', '2022-01', '2022-02', '2022-01', '2022-02'],\n    'Sales': [100, 150, np.nan, 200, 300, np.nan]\n})\n```\n\nYour task is to calculate the total sales for each product, replacing any missing sales values in a month with the average sales of that product across all months. The expected output should show each product with the corrected sales total, and the DataFrame should have an additional column 'Adjusted_Sales' that reflects the total sales after this adjustment.\n\nThe resulting DataFrame should appear like this:\n\n```\n  Product     Month  Sales  Adjusted_Sales\n0       A  2022-01  100.0            250.0\n1       A  2022-02  150.0            250.0\n2       B  2022-01    NaN            200.0\n3       B  2022-02  200.0            200.0\n4       C  2022-01  300.0            300.0\n5       C  2022-02    NaN            300.0\n```\n\nFill in the blank:\n```python\ndf['Adjusted_Sales'] = ...\n```",
    "output": "\n```python\ndf['Adjusted_Sales'] = df.groupby('Product')['Sales'].transform(lambda x: x.fillna(x.mean()))\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing the sales information of various products along with their sales regions. The dataset is structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product', 'Name'): ['A', 'B', 'C', 'A', 'B'],\n    ('Product', 'Category'): ['Electronics', 'Furniture', 'Electronics', 'Electronics', 'Furniture'],\n    ('Region', 'West'): [100, 200, 150, 300, None],\n    ('Region', 'East'): [None, 250, 100, 400, 150],\n    ('Region', 'North'): [50, 75, None, None, 50]\n}\ndf = pd.DataFrame(data)\n```\n\nThe DataFrame `df` has a multi-level column structure with two levels: `'Product'` and `'Region'`. \n\nYour goal is to normalize the sales data such that you extract a DataFrame containing only the total sales for each product across all regions (summing up the sales values). The resulting DataFrame should have the following format:\n\n```\n    Product   Total_Sales\n0      A          ?\n1      B          ?\n2      C          ?\n```\n\nTo accomplish this, you need to fill in the blank below with the appropriate code:\n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby(('Product', 'Name')).sum().reset_index()\nresult.columns = ['Product', 'Total_Sales']\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing information about different products in a retail store. Each product has several attributes like category, price, and sales. You want to create a MultiIndex DataFrame using the Pandas library, where the first level of the index is the product category and the second level is the product name. You also want to normalize the sales data for each product within its category to ensure comparability.\n\nAssume you have a dictionary of data as follows:\n```python\ndata = {\n    'category': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Grocery'],\n    'product': ['Smartphone', 'Laptop', 'Sofa', 'Table', 'Rice'],\n    'price': [699.99, 1299.99, 499.99, 199.99, 29.99],\n    'sales': [150, 80, 20, 50, 200]\n}\n```\n\nYour task is to create a MultiIndex DataFrame from this dictionary, normalize the 'sales' column per category, and then return the resulting DataFrame. \n\nThe expected structure should look like this, where sales are normalized within their respective categories:\n```\n                     price  sales\ncategory    product             \nElectronics Smartphone  699.99  1.000\n            Laptop     1299.99  0.533\nFurniture   Sofa        499.99  0.400\n            Table      199.99  1.000\nGrocery     Rice        29.99  1.000\n```\n\nHow can you achieve this in Python?\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'category': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Grocery'],\n    'product': ['Smartphone', 'Laptop', 'Sofa', 'Table', 'Rice'],\n    'price': [699.99, 1299.99, 499.99, 199.99, 29.99],\n    'sales': [150, 80, 20, 50, 200]\n}\n\ndf = pd.DataFrame(data)\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df.set_index(['category', 'product']).groupby(level=0).apply(lambda x: x.assign(sales=x['sales'] / x['sales'].max()))</code>"
  },
  {
    "input": "Problem:\nYou have two DataFrames representing sales data from different sources as follows:\n\nsales_df1:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:00:00 | Widget A |   10     |\n |  2     | 2023-01-01 11:00:00 | Widget B |   15     |\n |  3     | 2023-01-01 12:00:00 | Widget A |   5      |\n +--------+----------+----------+----------+\n```\n\nsales_df2:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:05:00 | Widget A |   7      |\n |  3     | 2023-01-01 12:05:00 | Widget A |   3      |\n |  4     | 2023-01-01 12:10:00 | Widget C |   8      |\n +--------+----------+----------+----------+\n```\n\nYour task is to merge these two DataFrames based on the `Product` column and ensure that in the final DataFrame, you include all the original records. If there are overlapping timestamps for the same product, sum the quantities from both DataFrames. Finally, sort the resulting DataFrame by `Timestamp`.\n\nAfter merging them efficiently, the resulting DataFrame should look like this:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:00:00 | Widget A |   17     |\n |  2     | 2023-01-01 11:00:00 | Widget B |   15     |\n |  3     | 2023-01-01 12:00:00 | Widget A |   8      |\n |  4     | 2023-01-01 12:10:00 | Widget C |   8      |\n +--------+----------+----------+----------+\n```\n\nUse the following placeholders in your solution:\n```python\nimport pandas as pd\n\nsales_df1 = pd.DataFrame({\n    'OrderID': [1, 2, 3],\n    'Timestamp': pd.to_datetime(['2023-01-01 10:00:00', '2023-01-01 11:00:00', '2023-01-01 12:00:00']),\n    'Product': ['Widget A', 'Widget B', 'Widget A'],\n    'Quantity': [10, 15, 5]\n})\n\nsales_df2 = pd.DataFrame({\n    'OrderID': [1, 3, 4],\n    'Timestamp': pd.to_datetime(['2023-01-01 10:05:00', '2023-01-01 12:05:00', '2023-01-01 12:10:00']),\n    'Product': ['Widget A', 'Widget A', 'Widget C'],\n    'Quantity': [7, 3, 8]\n})\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = pd.concat([sales_df1, sales_df2]).groupby(['Product', 'Timestamp'], as_index=False).agg({'Quantity': 'sum'}).reset_index()\nresult['OrderID'] = range(1, len(result) + 1)\nresult = result[['OrderID', 'Timestamp', 'Product', 'Quantity']].sort_values(by='Timestamp').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\n\nGiven the following DataFrame that contains sales data across different regions and product categories:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Region': ['North', 'North', 'South', 'South', 'East', 'East'],\n    'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n    'Sales': [100, 150, 200, 250, 300, 350]\n}\n\ndf = pd.DataFrame(data)\ndf.set_index(['Region', 'Category'], inplace=True)\n```\n\nYou want to normalize the sales data within each region for better comparison. How can you achieve this using Pandas?\n\nFill in the blank:\n\nnormalized_sales = df._____()  # Use the appropriate method to normalize the sales values in the DataFrame. \n\nA:\n<code>\nnormalized_sales = (df - df.groupby(level='Region').transform('mean')) / df.groupby(level='Region').transform('std')\n</code>",
    "output": "\n<code>\nnormalized_sales = (df - df.groupby(level='Region').transform('mean')) / df.groupby(level='Region').transform('std')\n</code>"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame containing sales data for different products across several regions. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'C', 'B', 'B', 'C'],\n    'Region': ['North', 'South', 'North', 'East', 'South', 'East', 'North'],\n    'Sales': [100, np.nan, 150, 200, 300, np.nan, 250],\n    'Discount': [10, 20, 15, 25, 30, np.nan, 5],\n    'Discounted_Sales': [90, np.nan, 127.5, 150, 210, np.nan, 237.5]\n}\n\ndf = pd.DataFrame(data)\n```\n\nI need to compute the total sales for each product in the 'North' region after applying a discount if available. If a discount is 'NaN', I want to treat it as zero. Here\u2019s the logic:\n1. Calculate the effective sales after discount for all entries in the 'North' region. If 'Discount' is NaN, use 0 instead.\n2. Sum the 'Sales' for each product in the 'North' region using a conditional aggregation.\n\nHow can I achieve this in pandas? \n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'C', 'B', 'B', 'C'],\n    'Region': ['North', 'South', 'North', 'East', 'South', 'East', 'North'],\n    'Sales': [100, np.nan, 150, 200, 300, np.nan, 250],\n    'Discount': [10, 20, 15, 25, 30, np.nan, 5],\n    'Discounted_Sales': [90, np.nan, 127.5, 150, 210, np.nan, 237.5]\n}\n\ndf = pd.DataFrame(data)\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf_north = df[df['Region'] == 'North']\ndf_north['Effective_Discount'] = df_north['Discount'].fillna(0)\ndf_north['Effective_Sales'] = df_north['Sales'] - df_north['Effective_Discount']\nresult = df_north.groupby('Product')['Effective_Sales'].sum()\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames containing sales data for different product categories over time. One DataFrame has the daily sales for each category, and the other has updated monthly sales figures. Your task is to update the daily sales DataFrame with the monthly sales figures wherever applicable, while also handling duplicate entries for the same date and ensuring that the final DataFrame has all dates sorted in ascending order.\n\nHere's the structure of your DataFrames:\n\ndaily_sales:\n```\n       date  category  sales\n0  2023-01-01      A      100\n1  2023-01-01      B      150\n2  2023-01-02      A      200\n3  2023-01-03      B      250\n4  2023-01-03      A      300\n```\n\nmonthly_sales:\n```\n       date  category  sales\n0  2023-01-01      A      120\n1  2023-01-01      B      160\n2  2023-01-02      A      220\n3  2023-01-03      C      300\n```\n\nYou want to create a new DataFrame that carries forward the daily sales but replaces them with the monthly sales data where provided, ensuring all dates are present and sorted:\n```\n       date  category  sales\n0  2023-01-01      A      120\n1  2023-01-01      B      160\n2  2023-01-02      A      220\n3  2023-01-02      B      150\n4  2023-01-03      A      300\n5  2023-01-03      B      250\n6  2023-01-03      C      300\n```\n\nComplete the code to achieve this, starting from the DataFrames provided:\n\n```python\nimport pandas as pd\n\ndaily_sales = pd.DataFrame({\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03', '2023-01-03'],\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'sales': [100, 150, 200, 250, 300]\n})\n\nmonthly_sales = pd.DataFrame({\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03'],\n    'category': ['A', 'B', 'A', 'C'],\n    'sales': [120, 160, 220, 300]\n})\n\n# Fill in the blank with the code that creates the final DataFrame\nresult = ...  # put your solution in this variable\n```\n",
    "output": "\n```python\nresult = pd.concat([daily_sales, monthly_sales]).drop_duplicates(subset=['date', 'category'], keep='last').sort_values(by=['date', 'category']).reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame that contains information about sales transactions as shown below:\n\n| Transaction_ID | Product   | Quantity | Price | Discount |\n|----------------|-----------|----------|-------|----------|\n| 1              | A         | 2        | 30    | NaN      |\n| 2              | B         | 1        | 20    | 5        |\n| 3              | A         | NaN      | 30    | 3        |\n| 4              | C         | 1        | 50    | 10       |\n| 5              | B         | 3        | NaN   | NaN      |\n| 6              | A         | 4        | 30    | 0        |\n\nI need to calculate the total sales for each product, where total sales are defined as `(Quantity * Price) - Discount`. If `Quantity` or `Price` is NaN for any transaction, it should be ignored in the total calculation. Any `Discount` value that is NaN should be treated as 0 for the calculations. The expected output should show the total sales per product as follows:\n\n| Product | Total_Sales |\n|---------|-------------|\n| A       | ?           |\n| B       | ?           |\n| C       | ?           |\n\nA:\n<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Transaction_ID': [1, 2, 3, 4, 5, 6],\n    'Product': ['A', 'B', 'A', 'C', 'B', 'A'],\n    'Quantity': [2, 1, None, 1, 3, 4],\n    'Price': [30, 20, 30, 50, None, 30],\n    'Discount': [None, 5, 3, 10, None, 0]\n})\n</code>\ntotal_sales = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\ndf['Discount'] = df['Discount'].fillna(0)\ntotal_sales = df.dropna(subset=['Quantity', 'Price']).groupby('Product').apply(lambda x: (x['Quantity'] * x['Price'] - x['Discount']).sum()).reset_index(name='Total_Sales')\n</code>"
  },
  {
    "input": "Problem:\n\nYou have collected survey data from various customers about their purchasing behavior across different product categories. The data is stored in a CSV file represented by a Pandas DataFrame, which includes information on the customer ID, age group, product category, and the amount spent on each product as shown below:\n\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': ['C1', 'C1', 'C2', 'C2', 'C3', 'C3'],\n    'age_group': ['18-24', '18-24', '25-34', '25-34', '35-44', '35-44'],\n    'product_category': ['Electronics', 'Groceries', 'Groceries', 'Electronics', 'Electronics', 'Groceries'],\n    'amount_spent': [200, 50, 100, 300, 150, 200]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to perform a normalization on the `amount_spent` column within each `age_group`. How can you achieve this while maintaining the structure of your DataFrame, including `customer_id` and other associated columns?\n\nPlease fill in the blank solution below:\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf['normalized_amount'] = ...\n```",
    "output": "\n```python\ndf['normalized_amount'] = df.groupby('age_group')['amount_spent'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\n```"
  },
  {
    "input": "Problem:\nI have a Pandas DataFrame that contains sales data with a multi-index structure. The data looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Store A', '2023'): {'Apples': 30, 'Oranges': 15},\n    ('Store A', '2024'): {'Apples': 45, 'Oranges': 10},\n    ('Store B', '2023'): {'Apples': 25, 'Oranges': 20},\n    ('Store B', '2024'): {'Apples': 50, 'Oranges': 30}\n}\ndf = pd.DataFrame(data).T\ndf.index.names = ['Store', 'Year']\ndf.columns = ['Apples', 'Oranges']\n```\n\nI want to normalize the sales data along each product across the stores for each year, where normalization is defined as the z-score of the sales volume. I would like to calculate the z-scores and format the DataFrame such that each row contains the original data and the respective z-score for each product. The final result should maintain the multi-index structure. \n\nWhat code can I use to compute the normalized z-scores and place them below each respective product in a structured format? You can use scipy's stats.zscore function for this purpose.\n\nA:\n```python\nresult = ...  # put solution in this variable\n```",
    "output": "\n```python\nfrom scipy.stats import zscore\n\nz_scores = df.groupby(level='Year').apply(zscore)\nresult = pd.concat([df, z_scores], keys=['Original', 'Z-Score'], names=['Type'])\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset that includes customer transactions. You have a pandas dataframe containing customer IDs, transaction amounts, and the month of transaction. The dataframe looks like this:\n\n```\nimport pandas as pd\n\ndata = {\n    'CustomerID': ['C1', 'C2', 'C1', 'C3', 'C2', 'C1', 'C3', 'C2'],\n    'TransactionAmount': [100, 200, 150, 300, 250, 80, 400, 230],\n    'Month': ['2023-01', '2023-01', '2023-02', '2023-01', '2023-02', '2023-03', '2023-01', '2023-03']\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou want to calculate the total transaction amount for each customer by month and create a new dataframe that has a multi-index composed of the `CustomerID` and the `Month`, along with the total transaction amounts in a new column named `TotalAmount`. In other words, the resulting dataframe should show each customer's monthly total transaction amounts:\n\n```\n                   TotalAmount\nCustomerID Month              \nC1        2023-01         ...\nC1        2023-02         ...\nC1        2023-03         ...\nC2        2023-01         ...\nC2        2023-02         ...\nC3        2023-01         ...\n```\n\nWhat code can you fill in below to achieve this transformation?\n\nA:\n<code>\ndf_grouped = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf_grouped = df.groupby(['CustomerID', 'Month'])['TransactionAmount'].sum().reset_index()\ndf_grouped = df_grouped.set_index(['CustomerID', 'Month']).rename(columns={'TransactionAmount': 'TotalAmount'})\n</code>"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame that represents an e-commerce company's sales data, structured like this:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'customer_id': [1, 2, 1, 2, 3, 3, 4, 5],\n    'purchase_amount': [100, 200, None, 300, 50, None, 400, 150],\n    'purchase_date': pd.to_datetime(['2023-01-01', '2023-01-03', '2023-01-02', '2023-01-04', '2023-01-01', None, '2023-01-03', '2023-01-02'])\n})\n```\n\nYour task is to calculate the total purchase amount for each customer, but while doing so, you must ignore null (`None`) values in the `purchase_amount` column. In addition, if a customer has no recorded purchases (even after ignoring nulls), they should receive a total of 0. \n\nThe output DataFrame should have two columns: `customer_id` and `total_purchase_amount`. \nFor example, the expected output for the above DataFrame should look like this:\n\n```\n   customer_id  total_purchase_amount\n0            1                  100.0\n1            2                  500.0\n2            3                   50.0\n3            4                  400.0\n4            5                  150.0\n```\n\nFill in the blank to achieve this in one line of code:\n\n```python\nresult = df.groupby('customer_id')['purchase_amount'].__________().reset_index()\n```\n",
    "output": "\n```python\nresult = df.groupby('customer_id')['purchase_amount'].sum(min_count=1).reset_index()\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset with various features and a target variable. I want to apply a standardization preprocessing step to my features and then validate the preprocessing by checking the mean and standard deviation of the transformed features. I have defined my features in a variable called `X` and now I want to standardize them using `StandardScaler` from `sklearn.preprocessing`. After standardizing, I need a specific line of code that computes the mean and standard deviation of the resulting scaled features. \n\nA:\n<code>\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nX = np.array([[1, 2], [3, 4], [5, 6]])\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</code>\nmean_std = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nmean_std = (X_scaled.mean(axis=0), X_scaled.std(axis=0))</code>"
  },
  {
    "input": "Problem:\n\nI have a tensor `data` representing features of different samples in a machine learning task, with the shape of (100, 20). I also have a boolean mask `mask` of shape (100, 20) that indicates which features should be considered (True for selected features, False for ignored ones). My goal is to apply this mask to the tensor such that only selected features remain in the resulting tensor, and I need to count how many features were selected for each sample.\n\nHere's a snippet of how my tensors look:\n\n```python\ndata = torch.randn(100, 20)  # Shape (100, 20)\nmask = (torch.rand(100, 20) > 0.5)  # Random True/False values\n```\n\nAfter applying the mask to `data`, I want to obtain a tensor that represents the count of selected features for each of the 100 samples.\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\ndata, mask = load_data()\n</code>\nselected_counts = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nselected_counts = mask.sum(dim=1)\n</code>"
  },
  {
    "input": "Problem:\nYou have been given a dataset representing customer transactions in a retail store, where each transaction record includes information about the customer, product, and purchase amount. The dataset is stored in a Pandas DataFrame as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'CustomerID': [1, 2, 1, 3, 2, 1],\n    'Product': ['A', 'B', 'A', 'C', 'B', 'C'],\n    'Amount': [20, 30, 25, 40, 35, 50],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03'])\n}\ndf = pd.DataFrame(data)\n```\n\nThe DataFrame looks like this:\n\n```\n   CustomerID Product  Amount       Date\n0           1       A      20 2023-01-01\n1           2       B      30 2023-01-01\n2           1       A      25 2023-01-02\n3           3       C      40 2023-01-02\n4           2       B      35 2023-01-03\n5           1       C      50 2023-01-03\n```\n\nYou want to pivot this data such that each unique customer is represented as a row, each unique product is represented as a column, and the values in the DataFrame reflect the sum of the amounts spent by each customer on each product. Additionally, you want to assign a multi-index to the columns composed of 'Product' and 'Transaction', where 'Transaction' indicates the order of the transactions (1st, 2nd, etc.).\n\nAfter these operations, the resulting DataFrame should look like the following structure:\n\n```\n                 Amount       \nProduct          A   B   C\nCustomerID                \n1                45  NaN  50\n2               NaN  65  NaN\n3               NaN  NaN  40\n```\n\nPlease fill in the blank with the appropriate code to accomplish the above task in Pandas:\n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.pivot_table(index='CustomerID', columns='Product', values='Amount', aggfunc='sum').fillna(np.nan)\n```"
  },
  {
    "input": "Problem:\nI am working with two DataFrames, `sales_data` and `returns_data`, which contain sales and return information, respectively. Below are the structures of both DataFrames:\n\n`sales_data`\n```\n   OrderID     Date   Amount\n0        1 2023-01-01  150.00\n1        2 2023-01-01  200.00\n2        3 2023-01-02  250.00\n3        4 2023-01-03  300.00\n```\n\n`returns_data`\n```\n   OrderID     Date   Amount\n0        2 2023-01-01   50.00\n1        3 2023-01-02  100.00\n2        5 2023-01-04  200.00\n```\n\nI want to create a new DataFrame, `result`, where I merge `sales_data` with `returns_data` on `OrderID`. The `Amount` in the `returns_data` should subtract from the corresponding `Amount` in the `sales_data` where `OrderID` matches. If there is no matching `OrderID` in `returns_data`, the `Amount` from `sales_data` should remain unchanged. \n\nAdditionally, the `result` DataFrame should have the following structure, including an indicator column called `Returned` to specify if the item was returned:\n\n```\n   OrderID     Date   Amount  Returned\n0        1 2023-01-01  150.00     False\n1        2 2023-01-01  150.00      True\n2        3 2023-01-02  150.00      True\n3        4 2023-01-03  300.00     False\n```\n\nCan you help me fill in the code to perform this operation?\n\nA:\n<code>\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'OrderID': [1, 2, 3, 4],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03']),\n    'Amount': [150.00, 200.00, 250.00, 300.00]\n})\n\nreturns_data = pd.DataFrame({\n    'OrderID': [2, 3, 5],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-04']),\n    'Amount': [50.00, 100.00, 200.00]\n})\n\nresult = ... # fill in your solution here\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = sales_data.merge(returns_data, on='OrderID', how='left', suffixes=('', '_return'))\nresult['Amount'] = result['Amount'] - result['Amount_return'].fillna(0)\nresult['Returned'] = result['Amount_return'].notna()\nresult = result[['OrderID', 'Date', 'Amount', 'Returned']]\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing images represented as tensors. Each image is a tensor of shape (3, 224, 224) representing RGB channels of size 224x224. You want to apply a thresholding operation to remove low pixel values. Specifically, you would like to set all pixel values below a given threshold to zero, while keeping the higher values unchanged. \n\nHow can you achieve this using boolean masking?\n\nA:\n\n<code>\nimport torch\n\nimages = load_data()  # Assume this returns a tensor of shape (N, 3, 224, 224)\nthreshold = 0.1\n</code>\nmasked_images = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nmasked_images = images * (images >= threshold)"
  },
  {
    "input": "Problem:\nI have two DataFrames representing product sales data. The first DataFrame `df_sales` contains information on various products sold, along with their sales dates and quantities, while the second DataFrame `df_inventory` contains the current stock levels of the same products.\n\nHere are the DataFrames:\n\ndf_sales:\n```\n   product_id         sale_date  quantity\n0         A1     2023/01/10        10\n1         A2     2023/01/10        15\n2         A1     2023/01/12         5\n3         A3     2023/01/13         3\n4         A2     2023/01/15        20\n```\n\ndf_inventory:\n```\n   product_id  current_stock\n0         A1             20\n1         A2             10\n2         A3             5\n3         A4             0\n```\n\nI want to create a new DataFrame that lists the total quantity sold for each product (from `df_sales`) alongside the remaining stock from `df_inventory`. If there are duplicate sales for a product, sum those quantities correctly. The DataFrame should include all products from `df_inventory`, and if there are no sales for a product, its sold quantity should be marked as 0. The 'sale_date' should not be part of the output.\n\nThe result should look like this:\n```\n   product_id  total_sold  current_stock\n0         A1          15             20\n1         A2          35             10\n2         A3           3              5\n3         A4           0              0\n```\n\nCurrently, the result generated from my attempts using pd.merge() is not giving me the output I need because the total sold is not being computed. I want to know how to achieve the expected result.\n\nA:\n<code>\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': ['A1', 'A2', 'A1', 'A3', 'A2'],\n    'sale_date': ['2023/01/10', '2023/01/10', '2023/01/12', '2023/01/13', '2023/01/15'],\n    'quantity': [10, 15, 5, 3, 20]\n})\n\ndf_inventory = pd.DataFrame({\n    'product_id': ['A1', 'A2', 'A3', 'A4'],\n    'current_stock': [20, 10, 5, 0]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ntotal_sold = df_sales.groupby('product_id')['quantity'].sum().reset_index()\ntotal_sold.columns = ['product_id', 'total_sold']\nresult = pd.merge(df_inventory, total_sold, on='product_id', how='left').fillna(0)\nresult['total_sold'] = result['total_sold'].astype(int)\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames containing sales data. The first DataFrame `df_sales` looks like this:\n\n```\n    StoreID      Date  Sales\n0      S001 2023-01-01    500\n1      S002 2023-01-01    300\n2      S001 2023-01-02    550\n3      S002 2023-01-02    600\n```\n\nThe second DataFrame `df_returns` is structured as follows:\n\n```\n    StoreID      Date  Returns\n0      S001 2023-01-02    50\n1      S002 2023-01-01    30\n2      S002 2023-01-02    70\n3      S001 2023-01-03    20\n```\n\nI want to create a new DataFrame that contains all the sales data along with the returns for each store on the appropriate dates. If there are no returns for a given day, that value should be filled with 0. The result should look like this:\n\n```\n    StoreID      Date  Sales  Returns\n0      S001 2023-01-01    500        0\n1      S002 2023-01-01    300       30\n2      S001 2023-01-02    550       50\n3      S002 2023-01-02    600       70\n4      S001 2023-01-03    0       20\n```\n\nHow do I accomplish this merging while ensuring that any missing return values are replaced with zero? \n\nA:\n<code>\nimport pandas as pd\n\n\ndf_sales = pd.DataFrame({\n    'StoreID': ['S001', 'S002', 'S001', 'S002'],\n    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'Sales': [500, 300, 550, 600]\n})\n\ndf_returns = pd.DataFrame({\n    'StoreID': ['S001', 'S002', 'S002', 'S001'],\n    'Date': ['2023-01-02', '2023-01-01', '2023-01-02', '2023-01-03'],\n    'Returns': [50, 30, 70, 20]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.merge(df_sales, df_returns, on=['StoreID', 'Date'], how='left').fillna(0)\nresult['Returns'] = result['Returns'].astype(int)\nresult = result.sort_values(by=['StoreID', 'Date']).reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\nContext\nI have two DataFrames that need to be combined while also ensuring data integrity and managing duplicates effectively. \n\nThe first DataFrame is as follows:\n```python\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n```\n\nAnd the second DataFrame is:\n```python\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\n\nI want to merge these two DataFrames on the `product_id` column, but I need to handle the case where `product_id` values in `df2` may result in duplicate rows in the final merged DataFrame, and keep only the first occurrence. The output should include all products and their corresponding stores, with an added column for availability.\n\nFinally, I would like to make sure the final DataFrame is sorted by `product_id`, in ascending order. The expected output should resemble:\n```python\n   product_id        products  price    store  availability\n0         101          Widget  20.99      NaN           NaN\n1         102          Gadget  25.50      NaN           NaN\n2         103          Doodad  15.75  Store A          True\n3         104  Thingamajig  22.10  Store B         False\n4         105           NaN     NaN  Store D          True\n```\n\nYou can find below the beginning of my implementation:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2.drop_duplicates(subset='product_id'), on='product_id', how='outer').sort_values(by='product_id').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\n\nIn PyTorch, I have a tensor `scores` representing the similarity scores between pairs of objects, and it has the shape (200, 200). I want to create a boolean mask that identifies which scores exceed a certain threshold (e.g., 0.5). \n\nAfter applying this mask, I want to count how many of the original scores are above the threshold. \n\nGiven `scores` as follows:\n\n```python\nimport torch\nscores = torch.rand((200, 200))  # Scores between 0 and 1\n```\n\nHow can I achieve this in a concise manner?\n\nA:\n\n```python\nimport torch\nscores = torch.rand((200, 200))  # Scores between 0 and 1\n```\ncount_above_threshold = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ncount_above_threshold = (scores > 0.5).sum().item()\n```"
  },
  {
    "input": "Problem:\nYou are provided with the following DataFrame that contains sales information of different products across various stores. \n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'store': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n    'product': ['apple', 'banana', 'apple', 'banana', 'apple', 'banana', 'banana', 'apple', 'banana'],\n    'sales': [200, 300, np.nan, 400, 500, np.nan, 300, 250, 350]\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to compute the total sales per store, but if any sales entry for a store is NaN, the entire total sales for that store should also be NaN.\n\nTo achieve this, you must modify the following code to produce the desired output:\n\n```python\nresult = df.groupby('store')['sales'].___\n```\n\nDesired output:\n```\nstore\nA      NaN\nB      NaN\nC      300.0\nName: sales, dtype: float64\n```",
    "output": "\n```python\nresult = df.groupby('store')['sales'].sum().where(df.groupby('store')['sales'].count() == df.groupby('store')['sales'].size())\n```"
  },
  {
    "input": "Problem:\n\nContext:\nI have two DataFrames representing product sales data from two different regions. I need to combine these datasets while ensuring that products from the second region replace the sales figures of the first region wherever they coincide. Additionally, I want to add a column that denotes whether a product's sales data from the second region is filling in a duplicate entry.\n\nDataFrames:\ndf1:\n```\n    Product    Sales\n0   A          150\n1   B          200\n2   C          300\n```\n\ndf2:\n```\n    Product    Sales\n0   B          250\n1   D          400\n```\n\nThe goal is to achieve a final DataFrame like this, maintaining the order of products from df1:\n```\n    Product    Sales     is_duplicated\n0   A          150      False\n1   B          250      True\n2   C          300      False\n3   D          400      True\n```\n\nYou can start with this code snippet:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'Product': ['A', 'B', 'C'], 'Sales': [150, 200, 300]})\ndf2 = pd.DataFrame({'Product': ['B', 'D'], 'Sales': [250, 400]})\n```\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nmerged = pd.merge(df1, df2, on='Product', how='outer', suffixes=('_df1', '_df2'))\nmerged['Sales'] = merged['Sales_df2'].combine_first(merged['Sales_df1'])\nmerged['is_duplicated'] = merged['Sales_df2'].notnull()\nresult = merged[['Product', 'Sales', 'is_duplicated']]\n```"
  }
]