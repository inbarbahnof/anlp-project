[
  {
    "input": "Problem:\nYou have a dataset containing information about product sales across different regions for various months. The dataframe looks as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'region': ['North', 'South', 'East', 'West', 'North', 'South'],\n    'month': ['Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Jan'],\n    'sales': [np.nan, 150, 200, 300, 250, np.nan],\n    'discount': [10, 20, np.nan, 5, 15, 10]\n})\n```\n\nYou want to calculate the average sales per region for the month of January. However, when calculating the average, you want to apply a custom condition: if the sales value is NaN, it should not be included in the average calculation, and instead, if the discount is positive, you want to include a dummy value of 100 in place of the NaN sales for that calculation. \n\nUltimately, you desire to return a DataFrame with the average sales for January by region.\n\nCan you complete the following line of code to achieve this?\n\n```python\nresult = df[df['month'] == 'Jan'].groupby('region').agg({'sales': lambda x: x.fillna(100) if df.loc[x.index, 'discount'].gt(0).any() else x.mean()})\n```\n",
    "output": "\n```python\nresult = df[df['month'] == 'Jan'].groupby('region').agg({'sales': lambda x: x.fillna(100) if (df.loc[x.index, 'discount'] > 0).any() else x.mean()})\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for various products over a week, including some records with missing values. The structure of the DataFrame is as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'B', 'C', 'A', 'C', 'B', 'C', 'A'],\n    'Sales': [150, np.nan, 200, 250, np.nan, 300, 350, np.nan, 400, 450],\n    'Date': pd.date_range(start='2023-01-01', periods=10)\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to compute the total sales for each product, but you need to ensure that if any sales figures for a product are missing (NaN), the total sales for that product also becomes NaN. You start with the following code for summing the sales:\n\n```python\ntotal_sales = df.groupby('Product')['Sales'].sum()\n```\n\nHowever, this does not yield the desired result with NaN handling. How can you modify your code to get the correct total sales that meet this requirement?\n\nHere is what I want to achieve:\n\n```\nProduct\nA      NaN\nB      NaN\nC      NaN\nName: Sales, dtype: float64\n```\n\nFill in the blank with the correct code:\n```python\ntotal_sales = ...\n```",
    "output": "\n```python\ntotal_sales = df.groupby('Product')['Sales'].apply(lambda x: x.sum() if x.isnull().sum() == 0 else np.nan)\n```"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame containing sales data for different products across multiple regions, structured like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    \"region\": [\"North\", \"South\", \"East\", \"West\", \"North\", \"South\", \"East\", \"West\"],\n    \"product\": [\"A\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n    \"sales\": [200, 150, 300, None, 250, None, 350, 100]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to create a new DataFrame that aggregates the total sales for each product, but with a condition: if any of the sales entries for that product are null (None), then the total sales for that product should also be null. Otherwise, the total should be calculated by summing the sales amounts. \n\nThe final DataFrame should have columns for \"product\" and a new column \"total_sales\". It should look like this:\n\n```\n  product  total_sales\n0       A         NaN\n1       B         450.0\n```\n\nFill in the blank to compute the desired result:\n\n```python\nresult = df.groupby('product')['sales'].... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby('product')['sales'].sum().where(df.groupby('product')['sales'].notnull().all()).reset_index(name='total_sales')\n```"
  },
  {
    "input": "Problem:\nYou have two pandas DataFrames representing sales transactions from two different months. The first DataFrame, `df_january`, includes sales data for January and looks like this:\n\n```python\n   transaction_id  product_id  quantity  sale_date   revenue\n0              1          101        2  2023-01-01       20\n1              2          102        1  2023-01-05       30\n2              3          103        5  2023-01-10      150\n3              4          101        3  2023-01-15       60\n4              5          104        1  2023-01-20       25\n```\n\nThe second DataFrame, `df_february`, gives data for February and looks like this:\n\n```python\n   transaction_id  product_id  quantity  sale_date   revenue\n0              6          103        4  2023-02-05      120\n1              7          104        2  2023-02-10       50\n2              8          105        1  2023-02-20       15\n3              9          101        1  2023-02-25       10\n4             10          102        2  2023-02-28       60\n```\n\nYou want to merge these two DataFrames such that:\n- All transactions from both months display together.\n- For the merged DataFrame, replace any missing `quantity`, `sale_date`, or `revenue` in February with the values from January, using the `product_id` as the key to look up these values.\n- Finally, ensure the DataFrame is sorted by `sale_date` and retains all transactions.\n\nHowever, you're noticing that there may be duplicate entries for `product_id` within one month when compared to the other month, and you want to drop the duplicates from the merged DataFrame based on the `transaction_id`, keeping the first occurrence.\n\nGiven the two DataFrames, how would you implement the merge and relevant processing?\n\nA:\n<code>\nimport pandas as pd\n\ndf_january = pd.DataFrame({\n    'transaction_id': [1, 2, 3, 4, 5],\n    'product_id': [101, 102, 103, 101, 104],\n    'quantity': [2, 1, 5, 3, 1],\n    'sale_date': pd.to_datetime(['2023-01-01', '2023-01-05', '2023-01-10', '2023-01-15', '2023-01-20']),\n    'revenue': [20, 30, 150, 60, 25]\n})\n\ndf_february = pd.DataFrame({\n    'transaction_id': [6, 7, 8, 9, 10],\n    'product_id': [103, 104, 105, 101, 102],\n    'quantity': [4, 2, 1, 1, 2],\n    'sale_date': pd.to_datetime(['2023-02-05', '2023-02-10', '2023-02-20', '2023-02-25', '2023-02-28']),\n    'revenue': [120, 50, 15, 10, 60]\n})\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.merge(df_february, df_january, on='product_id', how='outer', suffixes=('_feb', '_jan'))\nresult['quantity'] = result['quantity_feb'].combine_first(result['quantity_jan'])\nresult['sale_date'] = result['sale_date_feb'].combine_first(result['sale_date_jan'])\nresult['revenue'] = result['revenue_feb'].combine_first(result['revenue_jan'])\nresult = result[['transaction_id_feb', 'product_id', 'quantity', 'sale_date', 'revenue']].rename(columns={'transaction_id_feb': 'transaction_id'})\nresult = result.drop_duplicates(subset='transaction_id').sort_values(by='sale_date').reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\n\nYou have collected survey data from various customers about their purchasing behavior across different product categories. The data is stored in a CSV file represented by a Pandas DataFrame, which includes information on the customer ID, age group, product category, and the amount spent on each product as shown below:\n\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': ['C1', 'C1', 'C2', 'C2', 'C3', 'C3'],\n    'age_group': ['18-24', '18-24', '25-34', '25-34', '35-44', '35-44'],\n    'product_category': ['Electronics', 'Groceries', 'Groceries', 'Electronics', 'Electronics', 'Groceries'],\n    'amount_spent': [200, 50, 100, 300, 150, 200]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to perform a normalization on the `amount_spent` column within each `age_group`. How can you achieve this while maintaining the structure of your DataFrame, including `customer_id` and other associated columns?\n\nPlease fill in the blank solution below:\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf['normalized_amount'] = ...\n```",
    "output": "\n```python\ndf['normalized_amount'] = df.groupby('age_group')['amount_spent'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames, sales and returns, with the following data:\n\nsales:\n``` \n   order_id product_id quantity sold_date\n0        1        A         10  2023-01-01\n1        2        B         20  2023-01-02\n2        3        A         15  2023-01-03\n3        4        C         25  2023-01-04\n```\n\nreturns:\n```\n   order_id product_id quantity return_date\n0        1        A          5  2023-01-05\n1        3        A          10  2023-01-06\n2        5        D          7   2023-01-07\n```\n\nYou need to merge these two DataFrames such that you calculate the total quantity sold (subtracting any returns) for each product. The resulting DataFrame should contain the following columns: product_id, total_sold, and last_sold_date. The 'total_sold' should reflect the total quantity sold minus the quantity returned. The 'last_sold_date' should keep the maximum (latest) sold_date for each product.\n\nYour expected DataFrame should look like this:\n```\n  product_id total_sold last_sold_date\n0          A         15    2023-01-03\n1          B         20    2023-01-02\n2          C         25    2023-01-04\n3          D          0    NaT\n```\n\nYou tried using the following code:\n```\nresult = pd.merge(sales, returns, on='product_id', how='outer')\n```\n\nHowever, this did not give you the correct 'total_sold' and 'last_sold_date'.\n\nHow can you achieve the expected result in one line of code?\nA:\n```python\nimport pandas as pd\nsales = pd.DataFrame({'order_id': [1, 2, 3, 4], \n                      'product_id': ['A', 'B', 'A', 'C'], \n                      'quantity': [10, 20, 15, 25], \n                      'sold_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']})\nreturns = pd.DataFrame({'order_id': [1, 3, 5], \n                        'product_id': ['A', 'A', 'D'], \n                        'quantity': [5, 10, 7], \n                        'return_date': ['2023-01-05', '2023-01-06', '2023-01-07']})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```python\n\n\n```",
    "output": "\n```python\nresult = sales.groupby('product_id').agg(total_sold=('quantity', 'sum'), last_sold_date=('sold_date', 'max')).reset_index().merge(returns.groupby('product_id').agg(total_return=('quantity', 'sum')).reset_index(), on='product_id', how='outer').fillna(0).assign(total_sold=lambda x: x['total_sold'] - x['total_return']).drop(columns='total_return')\n```"
  },
  {
    "input": "Problem:\nYou are provided with two DataFrames, `sales` and `returns`, structured as follows:\n\n`sales`:\n```\n    OrderID    Date       Product  Amount\n0      101 2021-01-01   Widget A     20\n1      102 2021-01-02   Widget B     30\n2      103 2021-01-03   Widget A     25\n3      104 2021-01-04   Widget C     50\n4      105 2021-01-05   Widget B     40\n```\n\n`returns`:\n```\n    OrderID    Date       Product  Amount\n0      102 2021-01-03   Widget B     10\n1      104 2021-01-06   Widget C     50\n2      101 2021-01-01   Widget A     5\n```\n\nYour goal is to create a consolidated DataFrame that combines `sales` and `returns`, adjusting the `Amount` for returning products. For each `OrderID`, if there is a matching entry in `returns`, the `Amount` in `sales` should be reduced by the `Amount` from `returns`. If the `Amount` becomes negative, it should be set to zero. Additionally, you need to format the `Date` to \"dd-MMM-yyyy\" and sort the results by `Date`.\n\nThe resulting DataFrame should look like:\n```\n    OrderID    Date       Product  Amount\n0      101 01-Jan-2021   Widget A     15\n1      102 02-Jan-2021   Widget B     20\n2      103 03-Jan-2021   Widget A     25\n3      104 04-Jan-2021   Widget C      0\n4      105 05-Jan-2021   Widget B     40\n```\n\nAssuming you've already imported pandas and created the dataframes, complete the code below:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'OrderID': [101, 102, 103, 104, 105],\n    'Date': pd.to_datetime(['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05']),\n    'Product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],\n    'Amount': [20, 30, 25, 50, 40]\n})\n\nreturns = pd.DataFrame({\n    'OrderID': [102, 104, 101],\n    'Date': pd.to_datetime(['2021-01-03', '2021-01-06', '2021-01-01']),\n    'Product': ['Widget B', 'Widget C', 'Widget A'],\n    'Amount': [10, 50, 5]\n})\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = sales.copy()\nfor index, row in returns.iterrows():\n    result.loc[result['OrderID'] == row['OrderID'], 'Amount'] -= row['Amount']\nresult['Amount'] = result['Amount'].clip(lower=0)\nresult['Date'] = result['Date'].dt.strftime('%d-%b-%Y')\nresult = result.sort_values(by='Date').reset_index(drop=True)\n```"
  },
  {
    "input": "## Problem:\n\nYou have a batch of embeddings represented as a 2D tensor with shape `[batch_size, embedding_dim]`. Let's say the embeddings tensor is:\n\n```python\nembeddings = torch.rand((64, 128))  # 64 samples with 128-dimensional embeddings\n```\n\nYou also have a tensor of dot products with a shape of `[64, 64]` representing similarity scores between these embeddings.\n\nYour task is to apply a threshold of 0.5 to create a binary mask where values greater than or equal to the threshold are marked with 1, and those below are marked with 0. Additionally, you should ensure that there are no self-comparisons in the similarity scores (i.e., set the diagonal to 0).\n\nHow can you generate the binary mask tensor from the similarity scores tensor? \n\nPlease fill in the following variable with your solution:\n\n```python\nsimilarity_scores = torch.rand((64, 64))  # Sample similarity scores\n```\n\nA:\n\n```python\nembeddings = torch.rand((64, 128))\nsimilarity_scores = ...  # put solution in this variable\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nsimilarity_scores = torch.rand((64, 64))  # Sample similarity scores\nsimilarity_scores.fill_diagonal_(0)  # Set diagonal to 0 for no self-comparisons\nmask = (similarity_scores >= 0.5).int()  # Create binary mask with threshold\n```"
  },
  {
    "input": "Problem:\n\nI have a multi-index DataFrame representing sales data for different products over different quarters and regions, structured like this:\n\n```python\nimport pandas as pd\n\narrays = [\n    ['Electronics', 'Electronics', 'Furniture', 'Furniture'],\n    ['North', 'South', 'North', 'South']\n]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Product', 'Region'))\ndata = {\n    'Q1': [200, 150, 300, 250],\n    'Q2': [230, 180, 310, 270],\n    'Q3': [250, 200, 320, 290],\n    'Q4': [270, 220, 330, 310]\n}\ndf = pd.DataFrame(data, index=index)\n```\n\nI want to normalize the sales figures across all quarters for each product, so that each quarter's sales figures for each product sum up to 1. What function would you use to perform this normalization across the DataFrame? \n\n```python\nnormalized_df = df. # Is there a specific Pandas operation to achieve this?\n```\n",
    "output": "\n```python\nnormalized_df = df.div(df.sum(axis=1), axis=0)\n```"
  },
  {
    "input": "Problem:\n\nI have a tensor in TensorFlow representing probabilities from a softmax layer, and I want to apply a boolean mask to zero out values below a certain threshold. The tensor shape is (32, 10), representing a batch of 32 samples with 10 classes. Simultaneously, I want to calculate the sum of the masked values along the class dimension. Here's how my current setup looks:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nprobs = tf.constant(np.random.rand(32, 10), dtype=tf.float32)\nthreshold = 0.2\n\n# I need to get the masked tensor and the sum of the masked values\nmasked_probs = ...\nsum_masked_probs = ...\n```\n\nCan you help me fill in the blanks for the variable assignments? Thanks!",
    "output": "\n```python\nmasked_probs = tf.where(probs >= threshold, probs, tf.zeros_like(probs))\nsum_masked_probs = tf.reduce_sum(masked_probs, axis=1)\n```"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame that contains sales data structured as follows:\n```\n       product   sales\n0      A         100\n1      B         200\n2      C         null\n3      A         150\n4      B         null\n5      C         250\n6      A         300\n7      B         400\n8      C         100\n```\nThe DataFrame is initialized with the following code:\n```python\nimport pandas as pd\ndf = pd.DataFrame({\n    'product': ['A', 'B', None, 'A', 'B', 'C', 'A', 'B', 'C'],\n    'sales': [100, 200, None, 150, None, 250, 300, 400, 100]\n})\n```\nYour goal is to create a new DataFrame that aggregates the total sales for each product while filling any null sales figures with the average sales for that product before aggregation. For product A, the average sales count would be 216.67 (considering the entries along with nulls).\n\nFill in the blank in the following line of code to achieve this:\n```python\nresult = ...\n```\nMake sure that products with no sales entries also appear in the final DataFrame with total sales displayed as 0.\n\nNote: Make sure to handle null values properly during the average calculation.",
    "output": "\n```python\nresult = df.groupby('product').agg({'sales': lambda x: x.fillna(x.mean()).sum()}).reindex(df['product'].unique(), fill_value=0).reset_index()\n```"
  },
  {
    "input": "### Problem:\n\nI have a batch of images defined as a PyTorch tensor with shape `(batch_size, channels, height, width)`. Each image has been processed to obtain feature maps, and I want to apply a boolean mask to extract specific features based on a condition. The tensor is as follows:\n\n```python\nimport torch\n\nfeature_maps = torch.tensor([[[[0.1, 0.2], [0.3, 0.4]], \n                               [[0.5, 0.6], [0.7, 0.8]]], \n                              [[[1.0, 1.2], [1.3, 1.4]], \n                               [[1.5, 1.6], [1.7, 1.8]]]])\n```\n\nTo filter the feature maps, I need a mask that selects values greater than `0.5`. I already have the tensor defined, but I need help applying the boolean mask to get a tensor that only includes the values satisfying the condition. The desired output should still maintain the structure of the original tensor but have the values below `0.5` replaced by zeros.\n\nFill in the blank with the correct code needed for the masking operation:\n\n```python\nmask = feature_maps > 0.5\nmasked_feature_maps = ... # put solution in this variable\n```\n",
    "output": "\n```python\nmasked_feature_maps = feature_maps * mask\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data with multi-level indexing for products sold across different regions and quarters of the year. Here\u2019s a sample of the data:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples(\n    [('North', 'Q1'), ('North', 'Q2'), ('North', 'Q3'), ('North', 'Q4'),\n     ('South', 'Q1'), ('South', 'Q2'), ('South', 'Q3'), ('South', 'Q4')],\n    names=['Region', 'Quarter']\n)\ndata = {\n    'Sales': [25000, 27000, 24000, 30000, 15000, 18000, 16000, 20000],\n    'Profit': [5000, 6000, 4000, 7000, 3000, 3500, 2500, 4000]\n}\ndf = pd.DataFrame(data, index=index)\n```\n\nYou want to normalize the \"Sales\" column for each region while keeping the DataFrame structure intact. The final DataFrame should keep the multi-index but include the normalized sales values alongside the existing data.\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.copy()\nresult['Normalized_Sales'] = df.groupby(level='Region')['Sales'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset containing several numerical features and a categorical outcome. Your task is to preprocess the data: standardize the numerical features and one-hot encode the categorical feature. After preprocessing, you want to check the shape of the resultant dataset to ensure the transformation has been applied correctly.\n\nGiven the following code snippet to prepare your data:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = pd.DataFrame({\n    'Feature1': [1, 2, 3, 4, 5],\n    'Feature2': [10, 20, 30, 40, 50],\n    'Category': ['A', 'B', 'A', 'B', 'A']\n})\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), ['Feature1', 'Feature2']),\n        ('cat', OneHotEncoder(), ['Category'])\n    ])\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n```\n\nYou need to apply the transformations and obtain the shape of the new transformed dataset. How can you do this?\n\n```python\ntransformed_data = pipeline.fit_transform(data)\n```\n\nThe shape of the transformed dataset should be stored in a variable named `result_shape`. \n\nresult_shape = ... # put solution in this variable",
    "output": "\n```python\nresult_shape = transformed_data.shape\n```"
  },
  {
    "input": "Problem:\nI have a dataset containing various features related to house prices, and I would like to preprocess the data before applying a predictive model. I need to transform the 'area' and 'bedrooms' columns to achieve a better distribution by using the natural logarithm to handle skewed distributions. I've already imported the necessary libraries and loaded the data into a pandas DataFrame called `df`. What I need is to apply the logarithmic transformation to these columns and assign the result back into the DataFrame.\n\nHere is what I have so far:\n```python\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame for demonstration\ndata = {\n    'area': [1500, 1800, 2400, 3000, 3500],\n    'bedrooms': [3, 4, 4, 5, 6],\n    'price': [300000, 340000, 450000, 580000, 620000]\n}\ndf = pd.DataFrame(data)\n```\nApply the transformation and store the results back in the DataFrame:\n```python\ndf['area'] = ...\ndf['bedrooms'] = ...\n```",
    "output": "\n```python\ndf['area'] = np.log(df['area'])\ndf['bedrooms'] = np.log(df['bedrooms'])\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a project that involves processing a set of multi-dimensional feature vectors using PyTorch. Each feature vector is structured as a tensor of shape (batch_size, channels, height, width). Your task is to implement a method that applies a specific boolean mask tensor to the feature tensor to zero out certain elements conditionally, based on whether they meet a threshold.\n\nYou have the following tensors:\n\n- `features`: a tensor of shape (100, 3, 32, 32) representing a batch of 100 images with 3 channels and 32x32 pixels.\n- `mask`: a boolean tensor of the same shape, indicating which elements should be kept (True) or zeroed out (False).\n\nThe output should be a tensor of the same shape where elements of `features` are multiplied by the corresponding elements of `mask`. \n\nA:\n\n<code>\nimport numpy as np\nimport torch\nfeatures, mask = load_data()\n</code>\noutput = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\noutput = features * mask.float()\n</code>"
  },
  {
    "input": "Problem:\nYou have two DataFrames, `sales_data` and `returns_data`, as follows:\n\n`sales_data`\n```\n    order_id   product_id  quantity  sale_date\n0        001         P01        10  2023-01-01\n1        002         P02        5   2023-01-03\n2        003         P01        8   2023-01-05\n3        004         P03        20  2023-01-07\n4        005         P01        15  2023-01-09\n```\n\n`returns_data`\n```\n    return_id   product_id  return_quantity  return_date\n0          R01         P01               3  2023-01-06\n1          R02         P02               1  2023-01-07\n2          R03         P01               2  2023-01-10\n```\n\nYour goal is to create a new DataFrame called `net_sales` that summarizes the total sales for each product, while adjusting for returns. If a product has been returned, you should subtract the returned quantity from the total sold quantity. The resulting DataFrame should have columns for `product_id`, `total_sold`, and `net_sales`, formatted in such a way that `net_sales` can't be negative.\n\nHere\u2019s the starting code you have:\n\n```python\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'order_id': ['001', '002', '003', '004', '005'],\n    'product_id': ['P01', 'P02', 'P01', 'P03', 'P01'],\n    'quantity': [10, 5, 8, 20, 15],\n    'sale_date': pd.to_datetime(['2023-01-01', '2023-01-03', '2023-01-05', '2023-01-07', '2023-01-09'])\n})\n\nreturns_data = pd.DataFrame({\n    'return_id': ['R01', 'R02', 'R03'],\n    'product_id': ['P01', 'P02', 'P01'],\n    'return_quantity': [3, 1, 2],\n    'return_date': pd.to_datetime(['2023-01-06', '2023-01-07', '2023-01-10'])\n})\n\n# Your code for net_sales calculation starts here:\nnet_sales = sales_data.groupby('product_id')['quantity'].sum().reset_index()\nnet_sales = ... # put solution in this variable\n```\n\nFill in the blank with code that accomplishes the task of computing `net_sales`. Ensure that negative values are handled correctly.",
    "output": "\n```python\nreturns_summary = returns_data.groupby('product_id')['return_quantity'].sum().reset_index()\nnet_sales = net_sales.merge(returns_summary, on='product_id', how='left').fillna(0)\nnet_sales['net_sales'] = net_sales['quantity'] - net_sales['return_quantity']\nnet_sales['net_sales'] = net_sales['net_sales'].clip(lower=0)\nnet_sales = net_sales[['product_id', 'quantity', 'net_sales']].rename(columns={'quantity': 'total_sold'})\n```"
  },
  {
    "input": "Problem:\n\nI have a tensor representing a set of embeddings for documents and I need to calculate the cosine similarity matrix between these document embeddings. The embeddings tensor looks like this:\n\n```python\ntorch.FloatTensor([[0.1, 0.2, 0.3],\n                   [0.4, 0.5, 0.6],\n                   [0.7, 0.8, 0.9]])\n```\n\nI'll then use a boolean mask to filter out the pairs whose similarity is below a certain threshold (e.g., 0.9). The output should include the indices of the document pairs that meet this criterion.\n\nHow can I accomplish this? \n\nA:\n<code>\nimport torch\n\nembeddings = torch.FloatTensor([[0.1, 0.2, 0.3],\n                                 [0.4, 0.5, 0.6],\n                                 [0.7, 0.8, 0.9]])\n</code>\nsimilarity_matrix = ... # put solution in this variable \nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_matrix = cosine_similarity(embeddings)\nthreshold = 0.9\nmask = similarity_matrix > threshold\nindices = torch.nonzero(mask, as_tuple=True)\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset containing various attributes of houses, including square footage, number of bedrooms, and sale price. You want to preprocess the data by normalizing the continuous variables (square footage and sale price) using `StandardScaler` from `sklearn.preprocessing`. The standardized data should then be combined with the categorical variable (number of bedrooms) encoded as one-hot vectors. The final output should be a NumPy array containing the combined data.\n\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Simulated dataset\ndata = pd.DataFrame({\n    'square_footage': [1500, 2400, 3000, 1800],\n    'bedrooms': [3, 4, 5, 3],\n    'sale_price': [300000, 450000, 600000, 350000]\n})\n\n# Begin preprocessing\nscaler = StandardScaler()\nencoder = OneHotEncoder(sparse=False)\n\n# Normalize continuous variables\nnormalized_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>normalized_data = np.hstack((scaler.fit_transform(data[['square_footage', 'sale_price']]),\nencoder.fit_transform(data[['bedrooms']])))\n</code>",
    "output": "\n<code>normalized_data = np.hstack((scaler.fit_transform(data[['square_footage', 'sale_price']]),\nencoder.fit_transform(data[['bedrooms']])))\n</code>"
  },
  {
    "input": "Problem:\n\nI have two tensors representing embeddings of words in a vocabulary. I need to compute the cosine similarity between each pair of word embeddings from two different tensors and then create a boolean mask indicating which pairs have a similarity score greater than a specified threshold.\n\nGiven two tensors, `embeddings_a` and `embeddings_b`, how can I calculate the cosine similarity and then obtain a mask indicating which similarities exceed the threshold of 0.5?\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nembeddings_a, embeddings_b = load_data()\n</code>\nsimilarity_mask = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ncosine_similarity = torch.nn.functional.cosine_similarity(embeddings_a.unsqueeze(1), embeddings_b.unsqueeze(0), dim=2)\nsimilarity_mask = cosine_similarity > 0.5\n</code>"
  },
  {
    "input": "Problem:\nYou have two Pandas DataFrames as follows:\n\ndf1:\n```\n   employee_id      name   department      start_date  salary\n0           101      John     Sales          2020/01/01   50000\n1           102      Mary     HR             2019/05/12   60000\n2           103      Sam      IT             2020/08/22   55000\n3           104      Anna     IT             2021/04/01   65000\n```\n\ndf2:\n```\n   employee_id      start_date  bonus\n0           103    2020/02/15   3000\n1           104    2020/11/01   4000\n2           105    2021/01/01   5000\n```\n\nYour task is to concatenate the two DataFrames based on `employee_id`, fill the `name` and `department` for the entries in `df2` where `employee_id` matches from `df1`, and sort the resulting DataFrame first by `employee_id` and then by `start_date`. The resulting DataFrame `result` should have the following format:\n\n```\n   employee_id      name   department      start_date  salary  bonus\n0           101      John     Sales          2020/01/01   50000   NaN\n1           102      Mary     HR             2019/05/12   60000   NaN\n2           103      Sam      IT             2020/02/15   55000   3000\n3           104      Anna     IT             2020/11/01   65000   4000\n4           105      NaN      NaN            2021/01/01    NaN   5000\n```\n\nThe code you've written for concatenating and filling is as follows:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'employee_id': [101, 102, 103, 104],\n                    'name': ['John', 'Mary', 'Sam', 'Anna'],\n                    'department': ['Sales', 'HR', 'IT', 'IT'],\n                    'start_date': ['2020/01/01', '2019/05/12', '2020/08/22', '2021/04/01'],\n                    'salary': [50000, 60000, 55000, 65000]})\n\ndf2 = pd.DataFrame({'employee_id': [103, 104, 105],\n                    'start_date': ['2020/02/15', '2020/11/01', '2021/01/01'],\n                    'bonus': [3000, 4000, 5000]})\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2, on='employee_id', how='outer').sort_values(by=['employee_id', 'start_date'])\n```"
  },
  {
    "input": "## Problem:\n\nYou have two tensors, `A` and `B`, representing feature vectors for a binary classification task. The shapes of these tensors are `(50, 128)` each, where `50` is the number of samples and `128` is the feature dimension. You want to compute a similarity score for each pair of vectors from `A` and `B` using the cosine similarity formula, and return a mask indicating which pairs have a similarity score greater than a given threshold, say `0.8`.\n\nFor the cosine similarity, you need to perform element-wise operations to compute the dot products and magnitudes. Your initial attempt using PyTorch's operations has been partially completed.\n\nHere is the beginning of your code:\n\n```python\nimport torch\n\nA = load_data_A()  # Shape: (50, 128)\nB = load_data_B()  # Shape: (50, 128)\nthreshold = 0.8\n\n# Compute the dot products\ndot_product = torch.mm(A, B.t())\n\n# Compute magnitudes\nnorm_A = torch.norm(A, dim=1).unsqueeze(1)\nnorm_B = torch.norm(B, dim=1).unsqueeze(0)\n\n# Compute cosine similarity\ncosine_similarity = dot_product / (norm_A * norm_B)\n\n# Now, you want to create a boolean mask based on the threshold\nmask = ...\n```\n\nYou need to fill in the blank to produce the `mask` tensor that has the same shape as `cosine_similarity` but contains `True` for elements greater than `0.8` and `False` otherwise.\n\nA:\n\n```python\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nmask = cosine_similarity > threshold\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame that consists of sales data for a retail store, containing columns for the 'store_id', 'product_id', 'quantity_sold', and 'revenue'. Some of the values in the 'quantity_sold' column may be missing (NaN). Your task is to calculate the total revenue per store while replacing NaN values in 'quantity_sold' with 0 for the sake of the calculation. The resulting DataFrame should include 'store_id', 'total_quantity_sold', and 'total_revenue', where 'total_quantity_sold' is the sum of 'quantity_sold' for each store and 'total_revenue' is the sum of 'revenue' for each store.\n\nHere's a sample DataFrame to start with:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'store_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'product_id': [101, 102, 101, 102, 101, 103],\n    'quantity_sold': [5, np.nan, 3, 4, np.nan, 2],\n    'revenue': [20, 15, 12, 16, 5, 10]\n})\n```\n\nYou want to perform the following operation:\n```python\nresult = ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df.fillna({'quantity_sold': 0}).groupby('store_id').agg(\n    total_quantity_sold=('quantity_sold', 'sum'),\n    total_revenue=('revenue', 'sum')\n).reset_index()\n```"
  },
  {
    "input": "### Problem:\nI have two DataFrames X and Y that represent sales data over different months with potential duplicate entries. The structure of these DataFrames is as follows:\n\nX\n```\n    Product  Sales  Month\n0       A      10    Jan\n1       B      20    Feb\n2       A      15    Mar\n3       C      5     Jan\n```\n\nY\n```\n    Product  Sales  Month\n0       A      10    Jan\n1       B      30    Feb\n2       A      20    Mar\n3       D      25    Jan\n```\n\nI want to merge these two DataFrames such that:\n- The merged DataFrame should contain all unique products from both DataFrames.\n- For each product, if the product appears in both DataFrames, the sales should be summed up.\n- I want to add a new column `Year` with the value `2023` to all rows.\n- The resulting DataFrame should be sorted first by `Month` and then by `Product`.\n\nAfter implementing the merge, I suspect there might be some duplicates in month-related entries that need handling. The expected format for the final DataFrame is:\n\n```\n   Product  Sales  Month  Year\n0       A      40    Jan  2023\n1       B      50    Feb  2023\n2       C      5     Jan  2023\n3       D      25    Jan  2023\n```\n\nPlease write the necessary code to achieve this and set the merged DataFrame to the variable `result` below.\n\n```python\nimport pandas as pd\n\nX = pd.DataFrame({'Product': ['A', 'B', 'A', 'C'],\n                  'Sales': [10, 20, 15, 5],\n                  'Month': ['Jan', 'Feb', 'Mar', 'Jan']})\n\nY = pd.DataFrame({'Product': ['A', 'B', 'A', 'D'],\n                  'Sales': [10, 30, 20, 25],\n                  'Month': ['Jan', 'Feb', 'Mar', 'Jan']})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.concat([X, Y]).groupby(['Product', 'Month'], as_index=False).sum()\nresult['Year'] = 2023\nresult = result.sort_values(by=['Month', 'Product']).reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nI am working on a dataset containing historical sales data, and I want to create a new feature that captures the seasonality of sales. Given a DataFrame `df` with a column 'date' containing timestamps and a column 'sales', I want to calculate the monthly average sales and add it as a new column 'monthly_avg_sales'. My DataFrame has the following structure:\n\n```\n   date                 sales\n0  2021-01-01           200\n1  2021-01-15           150\n2  2021-02-01           300\n3  2021-02-15           250\n...\n```\n\nI already have the DataFrame imported as well as the required libraries, and I'm looking to fill in the code to compute this new column. Here\u2019s the code I have so far:\n\n<code>\nimport pandas as pd\nfrom datetime import datetime\n\n# Assuming df is already defined and has 'date' and 'sales' columns\ndf['date'] = pd.to_datetime(df['date'])\n</code>\ndf['monthly_avg_sales'] = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['monthly_avg_sales'] = df.groupby(df['date'].dt.to_period('M'))['sales'].transform('mean')\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames with records of sales transactions. The first DataFrame (`sales`) contains the following information:\n\n```python\nimport pandas as pd\n\nsales_data = {\n    'transaction_id': [1001, 1002, 1003, 1004, 1005],\n    'product_id': [501, 502, 503, 501, 504],\n    'quantity': [2, 3, 1, 1, 4],\n    'date': ['2022-01-01', '2022-01-01', '2022-01-01', '2022-01-02', '2022-01-02']\n}\nsales = pd.DataFrame(sales_data)\n```\n\nThe second DataFrame (`products`) contains information about the products:\n\n```python\nproducts_data = {\n    'product_id': [501, 502, 503, 504],\n    'product_name': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n    'category': ['Gadgets', 'Gadgets', 'Gizmos', 'Gizmos']\n}\nproducts = pd.DataFrame(products_data)\n```\n\nI need to create a combined DataFrame that merges `sales` with `products` based on `product_id`. The final DataFrame should include `transaction_id`, `product_name`, `quantity`, and `date`. Also, entries with the same `transaction_id` should be sorted by `date` in ascending order.\n\nThe combined DataFrame should look like this:\n\n```\n   transaction_id product_name  quantity        date\n0             1001     Widget A         2  2022-01-01\n1             1002     Widget B         3  2022-01-01\n2             1003     Widget C         1  2022-01-01\n3             1004     Widget A         1  2022-01-02\n4             1005     Widget D         4  2022-01-02\n```\n\nI have initially attempted to merge the DataFrames as follows:\n\n```python\ncombined = pd.merge(sales, products, on='product_id')\n```\n\nHowever, I need the final DataFrame to reflect the specified columns and sorting criteria. \n\nWhat code should I use to achieve that?\n```python\nresult = ...  # put solution in this variable\n```",
    "output": "\n```python\nresult = combined[['transaction_id', 'product_name', 'quantity', 'date']].sort_values(by=['date', 'transaction_id']).reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing several numeric features, and you want to perform feature engineering by creating polynomial features to enhance the performance of your machine learning model. However, you want to avoid generating too many features. You would like to create polynomial features of degree 3, while limiting the number of output features to 10. How can you achieve this using the `PolynomialFeatures` class from `sklearn.preprocessing`? \n\nAssuming you have already imported the necessary libraries and loaded your data into a variable called `X`, your task is to generate the polynomial features and store them in the variable `poly_features`.\n\n```python\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Load your data into X (assume it's already done)\nX = load_data()\n\npoly = PolynomialFeatures(degree=3, max_degree=10) # Fill in the solution here\n```\n\npoly_features = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\npoly = PolynomialFeatures(degree=3, include_bias=False)\npoly_features = poly.fit_transform(X)[:, :10]\n```"
  },
  {
    "input": "## Problem\nYou have a dataset containing customer information that includes their last purchase amounts recorded in a DataFrame. You want to preprocess this data by normalizing the purchase amounts using Min-Max scaling, which will transform the amounts into a range between 0 and 1. \n\nThe dataset looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': [1, 2, 3, 4, 5],\n    'last_purchase_amount': [150.00, 200.00, 50.00, 300.00, 250.00]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to create a new column called `scaled_purchase_amount` that holds the normalized values. Fill in the missing code below to achieve this:\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\npurchase_amounts = df[['last_purchase_amount']]\ndf['scaled_purchase_amount'] = scaler.fit_transform(purchase_amounts)\n\n# Fill in the missing code to transform these purchase amounts.\nresult = ...\n```\n### BEGIN SOLUTION\n",
    "output": "\n```python\nresult = df\n```"
  },
  {
    "input": "Problem:\n\nI have a batch of data represented as a PyTorch tensor that contains embedding vectors. The tensor `data` has dimensions (batch_size, embedding_dim) and I'm trying to compute the cosine similarity between each pair of embedding vectors in the batch. However, I want to focus on only those pairs that are from a specific category, indicated by a binary mask tensor. The mask tensor `category_mask` has the same batch_size and is of type `torch.BoolTensor`, where a value of `1` indicates the vectors to consider. \n\nHere is the code I've set up:\n\n```python\nimport torch\n\n# Sample data\ndata = torch.rand((5, 10))  # 5 embeddings of dimension 10\ncategory_mask = torch.BoolTensor([1, 0, 1, 0, 1])  # Select specific embeddings\n\n# Now I want to calculate the cosine similarity between the selected embeddings \n# and each of them in a pair-wise fashion. \nsimilarity_matrix = torch.nn.functional.cosine_similarity(data[category_mask], \n                                                           dim=1, \n                                                           eps=1e-8)\n\n# I want to compute a squared similarity score between the pairs of selected embeddings.\nsquared_similarity = ... # put solution in this variable\n```\n\nWhat should I put in `squared_similarity` to achieve this?",
    "output": "\n```python\nsquared_similarity = torch.matmul(similarity_matrix.unsqueeze(1), similarity_matrix.unsqueeze(0)) ** 2\n```"
  },
  {
    "input": "Problem:\nYou are given a dataset with user engagement metrics for an online platform over a period of time. The data looks like this:\n\n```python\nimport pandas as pd\nimport io\n\ndata = \"\"\"user,date,session_length,activity\nuser1,2023-01-01,30,click\nuser1,2023-01-02,45,scroll\nuser2,2023-01-01,20,click\nuser2,2023-01-03,35,click\nuser3,2023-01-01,50,scroll\nuser1,2023-01-03,60,scroll\nuser3,2023-01-02,40,click\"\"\"\ndf = pd.read_csv(io.StringIO(data))\n```\n\nYou want to pivot this DataFrame to show the total session length for each user by date, while also maintaining a multi-index where the first level is the user and the second level is the date. The activity column is not needed for the pivot operation.\n\nHow can you achieve this in Python using pandas? Fill in the following code snippet to create the desired pivot table:\n\n```python\nresult = df.pivot_table(index='user', columns='date', values='session_length', aggfunc='sum')\n```\n\nThis code should set the multi-index where the first level is the user and second level is the date, with total session lengths as the values.",
    "output": "\n```python\nresult = df.pivot_table(index='user', columns='date', values='session_length', aggfunc='sum').reset_index().set_index(['user'])\n```"
  },
  {
    "input": "Problem:\nI am working with PyTorch and I have two tensors A and B of the same shape. I want to create a new tensor C that contains elements from A where the corresponding elements in B are greater than or equal to a threshold value of 0.5. If an element in B is less than 0.5, I want the corresponding element in C to be set to 0. I\u2019m not sure how to achieve this boolean masking operation effectively.\n\nA:\n<code>\nimport torch\n\nA = torch.tensor([[0.1, 0.6, 0.3], [0.7, 0.2, 0.9]])\nB = torch.tensor([[0.4, 0.8, 0.1], [0.6, 0.3, 0.5]])\n</code>\nC = ... # put solution in this variable",
    "output": "\n<code>\nC = torch.where(B >= 0.5, A, torch.tensor(0.0))\n</code>"
  },
  {
    "input": "Problem:\nI have a dataset containing information about an e-commerce company's sales. The dataset consists of the following columns: `Category`, `Subcategory`, `Sales`, and `Quantity`. Here is a snippet of the data:\n\n```\nCategory      Subcategory    Sales    Quantity\nElectronics   Mobile Phones  300      10\nElectronics   Laptops       600      5\nClothing      Men           200      15\nClothing      Women         250      20\nClothing      Kids          150      12\nElectronics   Accessories   70       30\n```\n\nI would like to normalize the `Sales` column within each `Category`. That is, for each category, subtract the mean of the `Sales` from each sales entry and then divide by the standard deviation of the `Sales` for that category. Additionally, I want to encode the `Category` as categorical variables so that I can use it effectively in machine learning models later on.\n\nCan you provide a code snippet that accomplishes these transformations, while keeping the original structure of the DataFrame intact? Place your code in the variable `result`. \n\nHere's the skeleton code I have:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Category': ['Electronics', 'Electronics', 'Clothing', 'Clothing', 'Clothing', 'Electronics'],\n    'Subcategory': ['Mobile Phones', 'Laptops', 'Men', 'Women', 'Kids', 'Accessories'],\n    'Sales': [300, 600, 200, 250, 150, 70],\n    'Quantity': [10, 5, 15, 20, 12, 30]\n}\n\ndf = pd.DataFrame(data)\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\ndf['Sales_Normalized'] = df.groupby('Category')['Sales'].transform(lambda x: (x - x.mean()) / x.std())\ndf['Category'] = df['Category'].astype('category')\nresult = df\n```"
  },
  {
    "input": "### Problem:\nYou have a dataset containing information about various products and their sales in different regions. The data is presented in a DataFrame with a multi-index consisting of ('Region', 'Product'). Here's a sample of the DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('North', 'A'): [100, 150, 200],\n    ('North', 'B'): [80, 60, 90],\n    ('South', 'A'): [110, 130, 120],\n    ('South', 'B'): [70, 100, 80],\n}\nindex = pd.date_range(start='2023-01-01', periods=3, freq='D')\nsales_df = pd.DataFrame(data, index=index)\n```\n\nI want to normalize the sales values for each product across the regions, where the normalized value is calculated as `(value - min) / (max - min)`. \n\nIn order to do this, you'll need to reshape the DataFrame to handle the multi-index appropriately, perform the normalization, and then reshape it back to the original multi-index format.\n\nPlease fill in the following line to perform this operation and obtain the resulting normalized DataFrame:\n\n```python\nnormalized_sales = ...  # put solution in this variable\n```",
    "output": "\n```python\nnormalized_sales = (sales_df - sales_df.min()) / (sales_df.max() - sales_df.min())\n```"
  },
  {
    "input": "Problem:\n\nYou have a pandas DataFrame containing sales data for different products across various regions. The DataFrame is structured with a multi-index composed of 'Region' and 'Product', and contains sales figures for the year 2023. Here\u2019s a sample DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('North', 'A'): [200, 250, 300],\n    ('North', 'B'): [150, 175, 200],\n    ('South', 'A'): [300, 350, 400],\n    ('South', 'B'): [100, 125, 150],\n}\n\nindex = pd.date_range('2023-01-01', periods=3, freq='M')\ndf = pd.DataFrame(data, index=index)\nprint(df)\n```\n\nYour task is to normalize the sales figures within each region so that they sum up to 1 for each month. This requires applying a transformation to the DataFrame to achieve the desired normalization. After applying your transformation, the DataFrame should reflect normalized sales values.\n\nPlease fill in the following code to perform the normalization:\n\n```python\nnormalized_df = df.div(df.sum(axis=1), axis=0)\n```\n\nWhat does the `normalized_df` look like after your code executes? \n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    ('North', 'A'): [200, 250, 300],\n    ('North', 'B'): [150, 175, 200],\n    ('South', 'A'): [300, 350, 400],\n    ('South', 'B'): [100, 125, 150],\n}\n\nindex = pd.date_range('2023-01-01', periods=3, freq='M')\ndf = pd.DataFrame(data, index=index)\n```\nnormalized_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nnormalized_df = df.div(df.sum(axis=1), axis=0)\n```"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame representing sales data from multiple stores, structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'store': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'sales': [200, np.nan, 300, 400, 500, np.nan],\n    'returns': [5, 3, np.nan, 2, 1, 0]\n}\ndf = pd.DataFrame(data)\n```\n\nYour task is to compute the total sales for each store, but if a store has any NaN values in the sales column, you want the total for that store to be NaN. Additionally, you want to compute the average of the returns for each store, ignoring NaN values.\n\nThe final result should be a DataFrame with a store as the index, the total sales and the average returns as the columns. It should look like this:\n\n```\n         total_sales  average_returns\nstore                                \nA              200.0              4.0\nB                NaN              2.0\nC              500.0              0.5\n```\n\nFill in the blank with the correct code to achieve this:\n\n```python\nresult = df.groupby('store').agg(\n    total_sales=lambda x: x.sum(skipna=False),\n    average_returns=lambda x: x.mean()\n)\nresult.reset_index(inplace=True)\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df.groupby('store').agg(\n    total_sales=lambda x: x.sum() if x.isnull().sum() == 0 else np.nan,\n    average_returns=lambda x: x.mean()\n)\nresult.reset_index(inplace=True)\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a recommendation system using PyTorch. You have a user-item interaction tensor `interactions` of shape (n_users, n_items) where each element represents a score indicating the user's preference for a particular item. You want to filter this tensor to get only the scores of items that have been positively rated (greater than zero). After filtering, you want to compute the mean score per user from the remaining items. \n\nHere\u2019s the code stub you have:\n\n```python\nimport torch\n\n# Sample tensor representing user-item interactions\ninteractions = torch.tensor([[5.0, 0.0, 3.0],\n                             [0.0, 2.0, 0.0],\n                             [4.0, 0.0, 1.0]])\n\n# Create a mask for positive ratings\nmask = interactions > 0\n\n# Filter the interactions tensor\nfiltered_interactions = interactions[mask] # This line needs correction\n\n# Compute the mean score per user\nmean_scores = ...\n\n```\n\nFill in the blank in the `filtered_interactions` assignment to correctly apply the mask. The result should maintain the original structure of filtering while considering the user-item relations.",
    "output": "\n```python\nfiltered_interactions = interactions * mask\nmean_scores = filtered_interactions.sum(dim=1) / mask.sum(dim=1)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames, sales and returns, as follows:\n\nsales:\n```\n    OrderID    OrderDate   ProductID   Quantity\n0      1001  2023-01-01       A123         10\n1      1002  2023-01-02       B234          5\n2      1003  2023-01-03       A123          3\n3      1004  2023-01-04       C345         20\n4      1005  2023-01-05       B234          2\n```\n\nreturns:\n```\n   ReturnID  ReturnDate   ProductID   Quantity\n0       R001  2023-01-02       A123          2\n1       R002  2023-01-05       B234          1\n2       R003  2023-01-06       C345          5\n```\n\nI need to create a new DataFrame combining both sales and returns, maintaining the integrity of the sales data while adjusting the quantity based on the returns. Specifically, if a product was returned, the quantity of that product in the sales DataFrame should be decreased accordingly. The resulting DataFrame should contain the original sales data with an additional column indicating the adjusted quantity.\n\nThe desired output should look like this:\n\n```\n    OrderID    OrderDate   ProductID   Quantity  Adjusted_Quantity\n0      1001  2023-01-01       A123         10                  8\n1      1002  2023-01-02       B234          5                  4\n2      1003  2023-01-03       A123          3                  3\n3      1004  2023-01-04       C345         20                 15\n4      1005  2023-01-05       B234          2                  1\n```\n\nI'm currently unsure how to adjust the quantities in the sales DataFrame according to the returns without losing data integrity. \n\nA:\n<code>\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'OrderID': [1001, 1002, 1003, 1004, 1005],\n    'OrderDate': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']),\n    'ProductID': ['A123', 'B234', 'A123', 'C345', 'B234'],\n    'Quantity': [10, 5, 3, 20, 2]\n})\n\nreturns = pd.DataFrame({\n    'ReturnID': ['R001', 'R002', 'R003'],\n    'ReturnDate': pd.to_datetime(['2023-01-02', '2023-01-05', '2023-01-06']),\n    'ProductID': ['A123', 'B234', 'C345'],\n    'Quantity': [2, 1, 5]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\n# Group returns by ProductID and sum the quantities\nreturns_grouped = returns.groupby('ProductID')['Quantity'].sum().reset_index()\n\n# Merge sales with the grouped returns\nmerged = pd.merge(sales, returns_grouped, on='ProductID', how='left', suffixes=('', '_Returned'))\n\n# Fill NaN values with 0 for returned quantities\nmerged['Quantity_Returned'] = merged['Quantity_Returned'].fillna(0)\n\n# Calculate adjusted quantity\nmerged['Adjusted_Quantity'] = merged['Quantity'] - merged['Quantity_Returned']\n\n# Select relevant columns for the final result\nresult = merged[['OrderID', 'OrderDate', 'ProductID', 'Quantity', 'Adjusted_Quantity']]\n</code>"
  },
  {
    "input": "Problem:\nYou are analyzing a dataset of users' purchase transactions stored in a Pandas DataFrame with the following structure:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('user_id', 'user'): [1, 1, 2, 2, 3],\n    ('transaction', 'transaction_id'): [101, 102, 103, 104, 105],\n    ('purchase', 'item'): ['A', 'B', 'A', 'C', 'B'],\n    ('purchase', 'amount'): [25, 30, 15, 45, 20]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to calculate the total amount spent by each user per item category and normalize the resulting amounts by the maximum amount for each user. Your goal is to fill in the following code that will provide a normalized DataFrame with a multi-index, where the outer index is `user_id` and the inner index is the item purchased. The resulting DataFrame should look something like:\n\n```\n              amount\nuser_id item        \n1       A     1.00\n        B     1.20\n2       A     0.33\n        C     1.00\n3       B     1.00\n```\n\nTo achieve this, you must calculate the total purchase amounts and normalize them. Fill in the solution below to get the desired output:\n\n```python\nresult = ...\n```",
    "output": "\n```python\nresult = df.groupby(('user_id', 'user', 'purchase', 'item')).sum().reset_index()\nresult = result.set_index((('user_id', 'user'), ('purchase', 'item')))\nresult['amount'] = result['purchase', 'amount'] / result.groupby(level=0)['purchase', 'amount'].transform('max')\nresult = result[['amount']]\n```"
  },
  {
    "input": "Problem:\nI have the following multi-index DataFrame representing sales data per category and month:\n\n```python\nimport pandas as pd\nimport numpy as np\n\narrays = [['Electronics', 'Electronics', 'Clothing', 'Clothing'], \n          ['January', 'February', 'January', 'February']]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Category', 'Month'))\ndata = np.random.randint(100, 500, size=(4, 3))\ncolumns = ['Store_A', 'Store_B', 'Store_C']\ndf = pd.DataFrame(data, index=index, columns=columns)\n```\n\nThe DataFrame looks like this:\n\n```\n                     Store_A  Store_B  Store_C\nCategory    Month                            \nElectronics January     238     423     482\n            February    386     431     198\nClothing    January     286     333     433\n            February    457     166     120\n```\n\nI want to normalize the sales figures per store, so that each store's sales for each category (across both months) is represented as a percentage of the category's total sales for that month. The resulting DataFrame should preserve the multi-index structure while expressing sales as percentages.\n\nProvide the code that performs this normalization and stores the result in a variable named `normalized_df`.\n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\narrays = [['Electronics', 'Electronics', 'Clothing', 'Clothing'], \n          ['January', 'February', 'January', 'February']]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Category', 'Month'))\ndata = np.random.randint(100, 500, size=(4, 3))\ncolumns = ['Store_A', 'Store_B', 'Store_C']\ndf = pd.DataFrame(data, index=index, columns=columns)\n```\nnormalized_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n```",
    "output": "\n```python\nnormalized_df = df.div(df.groupby(level='Month').transform('sum')).multiply(100)\n```"
  },
  {
    "input": "Problem:\nI'm currently working with PyTorch to normalize a batch of feature vectors represented as a 2D tensor. My tensor `data` has shape (5, 3), and looks like this:\n```python\ndata = torch.tensor([[1.0, 2.0, 3.0],\n                     [4.0, 5.0, 6.0],\n                     [7.0, 8.0, 9.0],\n                     [10.0, 11.0, 12.0],\n                     [13.0, 14.0, 15.0]])\n```\nI want to normalize each feature (each column) by subtracting the mean and dividing by the standard deviation across each column. How should I implement this normalization while ensuring that the output tensor maintains the same shape?\n\nA:\n<code>\nimport torch\n\ndata = torch.tensor([[1.0, 2.0, 3.0],\n                     [4.0, 5.0, 6.0],\n                     [7.0, 8.0, 9.0],\n                     [10.0, 11.0, 12.0],\n                     [13.0, 14.0, 15.0]])\n\nnormalized_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nnormalized_data = (data - data.mean(dim=0)) / data.std(dim=0)\n</code>"
  },
  {
    "input": "Problem:\n\nIn a machine learning project, I have a tensor representing the scores of various predictions from a model, and I need to rank them to identify the top N predictions per batch sample. The tensor `scores` has a shape of (batch_size, num_predictions) and contains floating-point numbers representing the predicted scores. I want to create a new tensor that contains the indices of the top N predictions for each batch sample.\n\nHere's an example structure of the `scores` tensor:\n\n```\n[[0.1, 0.4, 0.2],\n [0.5, 0.3, 0.9],\n [0.6, 0.2, 0.8]]\n```\nIf `N=2`, the desired output should look like:\n\n```\n[[1, 2],\n [2, 0],\n [0, 2]]\n```\n\nPlease fill in the blank in the code snippet below to achieve this using PyTorch:\n\n```python\nimport torch\n\nscores = torch.tensor([[0.1, 0.4, 0.2],\n                       [0.5, 0.3, 0.9],\n                       [0.6, 0.2, 0.8]])\n\nN = 2\ntop_indices = ... # put solution in this variable\n```",
    "output": "\n```python\ntop_indices = torch.topk(scores, N, dim=1).indices\n```"
  },
  {
    "input": "Problem:\n\nYou have two tensors in PyTorch, `A` and `B`, both of size (100, 100). You need to create a new tensor, `C`, that contains the sum of tensor `A` and the elements of tensor `B` only where `B` is greater than zero. If the elements in `B` are less than or equal to zero, the corresponding elements in `C` should just be the elements from tensor `A`.\n\nGiven the following code where `A` and `B` are already defined:\n\n```python\nimport torch\n\nA = load_data_A()  # A tensor of shape (100, 100)\nB = load_data_B()  # A tensor of shape (100, 100)\n```\n\nFill in the blank to compute tensor `C`:\n\n```python\nC = A + ...  # put solution in this variable\n```",
    "output": "\n```python\nC = A + torch.where(B > 0, B, torch.zeros_like(B))  # put solution in this variable\n```"
  },
  {
    "input": "Problem:\n\nIn PyTorch, I have a tensor `scores` representing the similarity scores between pairs of objects, and it has the shape (200, 200). I want to create a boolean mask that identifies which scores exceed a certain threshold (e.g., 0.5). \n\nAfter applying this mask, I want to count how many of the original scores are above the threshold. \n\nGiven `scores` as follows:\n\n```python\nimport torch\nscores = torch.rand((200, 200))  # Scores between 0 and 1\n```\n\nHow can I achieve this in a concise manner?\n\nA:\n\n```python\nimport torch\nscores = torch.rand((200, 200))  # Scores between 0 and 1\n```\ncount_above_threshold = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ncount_above_threshold = (scores > 0.5).sum().item()\n```"
  },
  {
    "input": "### Problem\nI have a dataset containing customer transactions, and I want to analyze it for insights. The DataFrame has a multi-index consisting of 'CustomerID' and 'TransactionDate', with columns for 'Amount', 'Category', and 'PaymentMethod'. Here's a sample of the data:\n\n```\n                          Amount   Category       PaymentMethod\nCustomerID TransactionDate                                       \n1          2023-01-01      100.00  Groceries            Credit\n1          2023-01-05       50.00  Utilities           Debit\n2          2023-01-02      200.00  Groceries            Credit\n2          2023-01-08      150.00  Entertainment       Credit\n3          2023-01-01       30.00  Groceries            Cash\n```\n\nI need to normalize the 'Amount' column for each customer, turning it into a z-score to identify how much each transaction deviates from the customer's average transaction amount over time. This means creating a new DataFrame that keeps the original structure but adds a new column 'Amount_ZScore' next to the 'Amount' column, showing the normalized values.\n\nHow can I achieve this using Pandas? Complete the following line of code to create the normalized 'Amount_ZScore' column:\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {\n    ('CustomerID', ''): [1, 1, 2, 2, 3],\n    ('TransactionDate', ''): ['2023-01-01', '2023-01-05', '2023-01-02', '2023-01-08', '2023-01-01'],\n    ('Amount', ''): [100.00, 50.00, 200.00, 150.00, 30.00],\n    ('Category', ''): ['Groceries', 'Utilities', 'Groceries', 'Entertainment', 'Groceries'],\n    ('PaymentMethod', ''): ['Credit', 'Debit', 'Credit', 'Credit', 'Cash']\n}\ndf = pd.DataFrame(data).set_index([('CustomerID', ''), ('TransactionDate', '')])\n\n# Normalization code to write\ndf['Amount_ZScore'] = (df[('Amount', '')] - df[('Amount', '')].groupby(level=0).transform('mean')) / df[('Amount', '')].groupby(level=0).transform('std')\n```\nFill in the blank to complete the code that achieves the z-score normalization for the 'Amount' column.",
    "output": "\n```python\ndf[('Amount_ZScore', '')] = (df[('Amount', '')] - df[('Amount', '')].groupby(level=0).transform('mean')) / df[('Amount', '')].groupby(level=0).transform('std')\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset that contains information about customers and their purchase history in a retail store, and you're tasked with preparing the data for a machine learning model. The dataset includes features like \"CustomerID\", \"Age\", \"Gender\", \"Annual Income\", and \"Spending Score\". However, you need to preprocess the \"Gender\" column which contains categorical data (\"Male\" and \"Female\") and need to scale the \"Annual Income\" and \"Spending Score\". You also need to handle any potential missing values in the dataset.\n\nHere is the code you have so far for loading the data:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = {\n    'CustomerID': [1, 2, 3, 4, 5],\n    'Age': [19, 21, 20, 23, 25],\n    'Gender': ['Male', 'Female', 'Female', None, 'Male'],\n    'Annual Income': [15000, 20000, None, 25000, 30000],\n    'Spending Score': [39, 81, 6, 77, None]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to create a preprocessing pipeline that includes the following steps: \n1. One-hot encode the \"Gender\" column,\n2. Fill any missing values in \"Annual Income\" and \"Spending Score\" with their respective column means,\n3. Scale the \"Annual Income\" and \"Spending Score\" using standard scaling.\n\nFill in the blank in the following code to create the appropriate transformation pipeline:\n\n```python\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), ['Age', 'Annual Income', 'Spending Score']),\n        ('cat', OneHotEncoder(), ['Gender'])\n    ],\n    remainder='drop'\n)\n\npipeline = Pipeline(steps=[\n    ('preprocessor', ________)  # put solution in this variable\n])\n\ntransformed_data = pipeline.fit_transform(df)\n```\nBEGIN SOLUTION\n<code>\nSimpleImputer(strategy='mean')\n</code>",
    "output": "\n<code>\nPipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('preprocessor', preprocessor)\n])\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset containing information about customer transactions, including customer ID, transaction amount, and transaction date. Some of the transaction amounts are missing. You want to fill in these missing amounts using interpolation based on the mean of the amounts in the same month. Then, you plan to apply a logarithmic transformation to normalize the distribution of transaction amounts for further analysis using machine learning models.\n\nHere's a sample dataset:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'customer_id': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n    'transaction_amount': [100, np.nan, 200, 150, np.nan, np.nan, 300, 400, np.nan],\n    'transaction_date': pd.to_datetime(['2023-01-01', '2023-01-15', '2023-01-20', \n                                        '2023-02-01', '2023-02-10', '2023-02-15', \n                                        '2023-03-01', '2023-03-05', '2023-03-10'])\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to fill in the missing `transaction_amount` values with the mean of the available transaction amounts for each respective month, and then apply the logarithmic transformation to the `transaction_amount` column. \n\nComplete the statement below to achieve this, storing the result in the `df` DataFrame:\n\n```python\n# Fill missing values by group and apply logarithmic transformation\ndf['transaction_amount'] = df.groupby(df['transaction_date'].dt.to_period('M'))['transaction_amount'].transform(lambda x: x.fillna(x.mean()).apply(np.log))\n```\n\nBEGIN SOLUTION\n```python\n\n```",
    "output": "\n```python\n# Fill missing values by group and apply logarithmic transformation\ndf['transaction_amount'] = df.groupby(df['transaction_date'].dt.to_period('M'))['transaction_amount'].transform(lambda x: x.fillna(x.mean()).apply(np.log))\n```"
  },
  {
    "input": "Problem:\n\nI am working with a dataset of user ratings for various movies, represented as a tensor where rows correspond to users and columns correspond to movies. I need to compute a similarity score based on cosine similarity between two users, defined as the dot product of their rating vectors divided by the product of their magnitudes. I have extracted the ratings for two users as follows:\n\nimport torch\n\nratings = load_data()  # This returns a tensor of shape (num_users, num_movies)\nuser_a = ratings[0]    # Ratings for user A\nuser_b = ratings[1]    # Ratings for user B\n\nHowever, I\u2019m struggling with the following calculation to obtain the cosine similarity score between user A and user B. \n\nsimilarity_score = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nsimilarity_score = torch.dot(user_a, user_b) / (torch.norm(user_a) * torch.norm(user_b))\n</code>"
  },
  {
    "input": "### Problem:\n\nYou are working with a PyTorch tensor that represents the embedding of words in a vocabulary, with a shape of (1000, 128), where 1000 is the number of words and 128 is the dimensionality of their embeddings. You want to find the cosine similarity between a given query embedding and all the embeddings in the vocabulary, then you wish to retrieve the indices of the top 5 most similar words.\n\nThe cosine similarity can be calculated using the formula:\n\n\\[ \\text{similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]\n\nIn your case, you already have the query embedding in a tensor `query_embedding` with shape (128,).\n\nHere\u2019s what you have tried:\n\n```python\nimport torch\n\n# Example tensors\nembeddings = torch.randn(1000, 128)  # Word embeddings\nquery_embedding = torch.randn(128)    # Query embedding\n\n# Normalize the embeddings\nnormalized_embeddings = ...\n```\n\nFill in the blank to compute the cosine similarity, then get the indices of the top 5 most similar words:\n\nA:\n```python\ntop_indices = ...  # put solution in this variable\n```",
    "output": "\n```python\nnormalized_embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\nnormalized_query = query_embedding / query_embedding.norm()\ncosine_similarities = torch.matmul(normalized_embeddings, normalized_query)\ntop_indices = torch.topk(cosine_similarities, 5).indices\n```"
  },
  {
    "input": "Problem:\n\nYou have two DataFrames, sales and inventory, structured as follows:\n\nsales:\n```\n   product_id   date    sales\n0           1  2023-01-01     100\n1           2  2023-01-01      50\n2           1  2023-01-02     150\n3           3  2023-01-02      80\n```\n\ninventory:\n```\n   product_id restock_date  stock\n0           1   2023-01-05     200\n1           2   2023-01-03     100\n2           3   2023-01-04     60\n3           4   2023-01-06     50\n```\n\nYou need to merge these two DataFrames such that:\n1. All rows from sales are retained.\n2. Missing inventory information (where `sales.product_id` does not match `inventory.product_id`) should be filled with `NaN`.\n3. Ensure the date formats in the final DataFrame are in the format \"DD-Mon-YYYY\".\n\nThe output DataFrame should look like this:\n\n```\n   product_id       date  sales restock_date  stock\n0           1  01-Jan-2023     100   2023-01-05   200\n1           2  01-Jan-2023      50   2023-01-03   100\n2           1  02-Jan-2023     150   2023-01-05   200\n3           3  02-Jan-2023      80   2023-01-04    60\n```\n\nYou attempt the merge but receive a DataFrame that does not format the dates correctly. Your task is to write the code that combines these DataFrames while applying the necessary transformations for the dates.\n\nHere is your starting point:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3],\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'sales': [100, 50, 150, 80]\n})\n\ninventory = pd.DataFrame({\n    'product_id': [1, 2, 3, 4],\n    'restock_date': ['2023-01-05', '2023-01-03', '2023-01-04', '2023-01-06'],\n    'stock': [200, 100, 60, 50]\n})\n```\n\nYour solution should be assigned to `result`:\n```python\nresult = ...  # put your solution in this variable\n```",
    "output": "\n```python\nresult = pd.merge(sales, inventory, on='product_id', how='left')\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult['restock_date'] = pd.to_datetime(result['restock_date']).dt.strftime('%Y-%m-%d')\n```"
  },
  {
    "input": "Problem:\nYou are working with two Pandas DataFrames that provide sales data for different products from two different channels (online and in-store). The DataFrames are structured as follows:\n\ndf_online:\n```\n    product_id  date       sales\n0           A  2023-01-01   100\n1           B  2023-01-02    50\n2           A  2023-01-03   150\n3           C  2023-01-02   75\n```\n\ndf_in_store:\n```\n    product_id  date       sales\n0           A  2023-01-01   90\n1           B  2023-01-02    60\n2           A  2023-01-03   120\n3           D  2023-01-02   40\n```\n\nYou need to concatenate these two DataFrames, filling in missing `sales` for the `product_id` from `df_online` using the data from `df_in_store`. However, you need to ensure that in case of duplicate `product_id` and `date`, the sales from `df_in_store` should overwrite the sales from `df_online`. After concatenation, ensure that the resulting DataFrame is sorted by `date` and `product_id`.\n\nThe expected final DataFrame should look like this:\n```\n    product_id      date       sales\n0           A  2023-01-01   100\n1           A  2023-01-01    90\n2           A  2023-01-03   150\n3           A  2023-01-03   120\n4           B  2023-01-02    50\n5           B  2023-01-02    60\n6           C  2023-01-02   75\n7           D  2023-01-02   40\n```\n\nCan you fill in the blank below with the correct code to obtain this result?\n```python\nimport pandas as pd\n\ndf_online = pd.DataFrame({\n    'product_id': ['A', 'B', 'A', 'C'],\n    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-02'],\n    'sales': [100, 50, 150, 75]\n})\n\ndf_in_store = pd.DataFrame({\n    'product_id': ['A', 'B', 'A', 'D'],\n    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-02'],\n    'sales': [90, 60, 120, 40]\n})\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.concat([df_online, df_in_store]).sort_values(by=['date', 'product_id']).groupby(['product_id', 'date'], as_index=False).first()\n```"
  },
  {
    "input": "Problem:\n\nI have a PyTorch tensor representing the embeddings of a batch of images, with a shape of [64, 128], where 64 is the batch size and 128 is the embedding dimension. I also have a tensor of boolean masks of shape [64] indicating whether each embedding is valid or not. I want to calculate the mean of only the valid embeddings (i.e., where the mask is True). How can I do this efficiently while ignoring the invalid ones?\n\nA:\n<code>\nimport torch\n\nembeddings = torch.randn(64, 128)  # Randomly generated embeddings\nmask = torch.tensor([True, False, True, True, False, True, False, True, True, True, \n                     False, True, True, False, True, False, True, True, False, \n                     True, True, False, False, True, True, False, True, \n                     True, False, False, True, True, False, True, \n                     True, False, False, True, True, False, \n                     True, True, True, True, False, True, \n                     False, True, True, False, True, False, \n                     False, True, True, True, False, True, \n                     True, True, True, False, True, True], dtype=torch.bool)\n\n# Compute the mean of valid embeddings \nmean_valid_embeddings = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmean_valid_embeddings = embeddings[mask].mean(dim=0)\n</code>"
  },
  {
    "input": "### Problem:\n\nI have a DataFrame containing e-commerce sales data as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'OrderID': [1, 2, 3, 4, 5],\n    'CustomerID': ['C001', 'C002', 'C001', 'C003', 'C002'],\n    'ProductID': ['P001', 'P002', 'P001', 'P003', 'P002'],\n    'Quantity': [1, 2, 3, 1, 2],\n    'Price': [10.0, 20.0, 10.0, 5.0, 20.0],\n    'OrderDate': ['2021-01-01', '2021-01-02', '2021-01-01', '2021-01-03', '2021-01-02']\n}\ndf = pd.DataFrame(data)\n```\n\nI want to achieve the following tasks:\n\n1. Normalize the `Price` column using min-max normalization so that its values range from 0 to 1.\n2. Create a multi-index DataFrame that organizes the sales by `CustomerID` and `OrderID`.\n3. Extract the normalized price along with the quantity and create a new DataFrame that shows:\n\n```\nCustomerID  OrderID  Quantity  NormalizedPrice\nC001        1       1         0.5\n           3       3         0.5\nC002        2       2         1.0\n           5       2         1.0\nC003        4       1         0.0\n```\n\nWhat is the code to achieve this?\n\n```python\nimport pandas as pd\n\ndata = {\n    'OrderID': [1, 2, 3, 4, 5],\n    'CustomerID': ['C001', 'C002', 'C001', 'C003', 'C002'],\n    'ProductID': ['P001', 'P002', 'P001', 'P003', 'P002'],\n    'Quantity': [1, 2, 3, 1, 2],\n    'Price': [10.0, 20.0, 10.0, 5.0, 20.0],\n    'OrderDate': ['2021-01-01', '2021-01-02', '2021-01-01', '2021-01-03', '2021-01-02']\n}\ndf = pd.DataFrame(data)\n\n# Normalize Price column\ndf['NormalizedPrice'] = (df['Price'] - df['Price'].min()) / (df['Price'].max() - df['Price'].min())\n\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>\ndf.set_index(['CustomerID', 'OrderID'], inplace=True)\nresult = df[['Quantity', 'NormalizedPrice']]\n</code>\n",
    "output": "\n<code>\ndf.set_index(['CustomerID', 'OrderID'], inplace=True)\nresult = df[['Quantity', 'NormalizedPrice']]\n</code>"
  },
  {
    "input": "Problem:\n\nI am working with a dataset containing customer transactions, and I'd like to perform feature engineering by creating a new feature that indicates the total transaction amount for each customer. My dataset is in a pandas DataFrame called `transactions` with the following structure:\n\n| customer_id | transaction_amount |\n|-------------|--------------------|\n| 1           | 100                |\n| 2           | 200                |\n| 1           | 150                |\n| 3           | 300                |\n| 2           | 50                 |\n\nI want to group the data by `customer_id` and calculate the total transaction amount for each customer, and then save this new feature into a variable called `total_amount`.\n\nA:\n\n<code>\nimport pandas as pd\ntransactions = pd.DataFrame({\n    'customer_id': [1, 2, 1, 3, 2],\n    'transaction_amount': [100, 200, 150, 300, 50]\n})\n</code>\ntotal_amount = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ntotal_amount = transactions.groupby('customer_id')['transaction_amount'].sum().reset_index()"
  },
  {
    "input": "Problem:\nI have a dataset containing various features and target variables. I would like to apply a log transformation to specific features in my dataset to stabilize variance and make the data more normally distributed. The features I want to transform are specified in a list called `features_to_transform`. After transforming these features, I wish to normalize the entire dataset using StandardScaler from scikit-learn to ensure that they are on a similar scale. Finally, I want to obtain the transformed and normalized features as a NumPy array.\n\nHere's the setup I have:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.DataFrame({\n    'feature1': [1, 2, 3, 4],\n    'feature2': [10, 20, 30, 40],\n    'feature3': [100, 200, 300, 400]\n})\nfeatures_to_transform = ['feature1', 'feature2']\n```\n\nThe final transformed and normalized features should be stored in the variable `transformed_data`. \n\nA:\n```python\nimport numpy as np\n\n# Log transform the specified features\ndata[features_to_transform] = np.log(data[features_to_transform])\n\n# Normalize the entire dataset\nscaler = StandardScaler()\ntransformed_data = ... # put solution in this variable\n```",
    "output": "\n```python\ntransformed_data = scaler.fit_transform(data)\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a DataFrame containing user activity data for a mobile application. The DataFrame has the following columns: `UserID`, `Activity`, and `Timestamp`. Each user may have multiple activities recorded at different timestamps. Your goal is to transform this data to analyze the frequency of each activity per user, while normalizing the counts. The sample DataFrame looks like this:\n\n```\n  UserID    Activity              Timestamp\n0      1      Login      2023-09-01 10:00:00\n1      1      Logout     2023-09-01 12:00:00\n2      1      Purchase   2023-09-01 11:30:00\n3      2      Login      2023-09-01 09:00:00\n4      2      Purchase   2023-09-01 09:30:00\n5      2      Logout     2023-09-01 11:00:00\n6      3      Login      2023-09-01 08:30:00\n7      3      Purchase   2023-09-01 09:00:00\n```\n\nYou need to pivot this DataFrame such that each activity type has its own column, and the values indicate the frequency a user performed that activity. Additionally, normalize the counts per user so that they sum to 1 for each user. The resulting DataFrame should look like this:\n\n```\n  UserID   Login   Logout   Purchase\n0      1     0.33     0.33     0.33\n1      2     0.33     0.33     0.33\n2      3     0.50     0.00     0.50\n```\n\nHow can you achieve this?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'UserID': [1, 1, 1, 2, 2, 2, 3, 3],\n    'Activity': ['Login', 'Logout', 'Purchase', 'Login', 'Purchase', 'Logout', 'Login', 'Purchase'],\n    'Timestamp': pd.to_datetime(['2023-09-01 10:00:00', '2023-09-01 12:00:00', '2023-09-01 11:30:00',\n                                  '2023-09-01 09:00:00', '2023-09-01 09:30:00', '2023-09-01 11:00:00',\n                                  '2023-09-01 08:30:00', '2023-09-01 09:00:00'])\n}\n\ndf = pd.DataFrame(data)\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby(['UserID', 'Activity']).size().unstack(fill_value=0)\nresult = result.div(result.sum(axis=1), axis=0)\n</code>"
  },
  {
    "input": "Problem:\nYou are tasked with normalizing a dataset that contains multiple features with varying ranges. You want to apply Min-Max scaling to transform all the features into the range [0, 1]. The dataset is provided as a Pandas DataFrame. After scaling, you want to add the scaled features back into the DataFrame, keeping the original features intact. Given the DataFrame `df`, how do you achieve this with the MinMaxScaler from sklearn?\n\nA:\n<code>\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = {\n    'feature1': [10, 20, 30, 40],\n    'feature2': [200, 300, 400, 500],\n    'feature3': [5, 10, 15, 20]\n}\ndf = pd.DataFrame(data)\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(df)\n\n</code>\ndf_scaled = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf_scaled = pd.DataFrame(scaled_features, columns=df.columns)"
  },
  {
    "input": "Problem:\nYou are given two DataFrames, `sales` and `returns`:\n\n`sales`\n```\n+----------+------------+-------+\n|  OrderID |   Date     | Amount|\n+----------+------------+-------+\n|    1001  | 2020-01-10 |  200  |\n|    1002  | 2020-01-15 |  150  |\n|    1003  | 2020-02-05 |  300  |\n|    1004  | 2020-02-07 |  250  |\n+----------+------------+-------+\n```\n\n`returns`\n```\n+----------+-----------+-------+\n|  OrderID |   Date    | Amount|\n+----------+-----------+-------+\n|    1002  | 2020-01-20|   50  |\n|    1003  | 2020-02-15|   100 |\n|    1005  | 2020-02-20|   150 |\n+----------+-----------+-------+\n```\n\nYou need to create a new DataFrame that combines `sales` and `returns` based on `OrderID`. Ensure that:\n\n1. For each `OrderID` present in both DataFrames, you'll want to subtract the amount in `returns` from the corresponding `sales` amount.\n2. Any `OrderID` that is only in `sales` should still be included with the original sales amount and a value of `0` for returns.\n3. Any `OrderID` that is only in `returns` should be displayed with its amount, but should not contribute to the combined sales total.\n\nThe resulting DataFrame should have the following format: \n\n```\n+----------+------------+-------+-------+\n|  OrderID |   Sales    | Returns| Net   |\n+----------+------------+-------+-------+\n|    1001  |    200     |   0   |  200  |\n|    1002  |    150     |  50   |  100  |\n|    1003  |    300     |  100  |  200  |\n|    1004  |    250     |   0   |  250  |\n|    1005  |    0      |  150  | -150  |\n+----------+------------+-------+-------+\n```\n\nUse the following code to set up the DataFrames:\n\n```\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'OrderID': [1001, 1002, 1003, 1004],\n    'Date': pd.to_datetime(['2020-01-10', '2020-01-15', '2020-02-05', '2020-02-07']),\n    'Amount': [200, 150, 300, 250]\n})\n\nreturns = pd.DataFrame({\n    'OrderID': [1002, 1003, 1005],\n    'Date': pd.to_datetime(['2020-01-20', '2020-02-15', '2020-02-20']),\n    'Amount': [50, 100, 150]\n})\n\nresult = ... # put your solution here\n```",
    "output": "\n```python\nresult = pd.merge(sales, returns, on='OrderID', how='outer', suffixes=('_Sales', '_Returns')).fillna(0)\nresult['Sales'] = result['Amount_Sales'].astype(int)\nresult['Returns'] = result['Amount_Returns'].astype(int)\nresult['Net'] = result['Sales'] - result['Returns']\nresult = result[['OrderID', 'Sales', 'Returns', 'Net']]\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset of customer transactions and want to derive meaningful features from the timestamps of these transactions. Your goal is to extract features such as the hour of the day, day of the week, and whether the transaction occurred on a weekend. You have a dataframe called `transactions` with a column named `timestamp` in datetime format. \n\nYou need to create a new dataframe that contains the original timestamps along with these three new features. Fill in the blank with a function that achieves this transformation using pandas.\n\nA:\n<code>\nimport pandas as pd\n\n# Sample transaction data\ndata = {'timestamp': pd.to_datetime(['2022-01-01 08:23', '2022-01-02 14:45', '2022-01-03 18:12'])}\ntransactions = pd.DataFrame(data)\n\ndef extract_features(df):\n    # Add the hour of the day, day of the week, and weekend flag\n    df['hour'] = df['timestamp'].dt.hour\n    df['day_of_week'] = df['timestamp'].dt.day_name()\n    df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5\n    return df\n\n# Fill in the blank\nnew_transactions = extract_features(transactions)\n</code>\nnew_transactions = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nnew_transactions = extract_features(transactions)  \n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset of image embeddings represented as a tensor using PyTorch. The tensor `embeddings` has a shape of (100, 128), where 100 is the number of images and 128 represents the feature dimensions of those embeddings. I want to compute a similarity score between these embeddings and a query embedding vector `query_embedding` of shape (128). Specifically, I want to calculate the cosine similarity for each embedding with respect to the query_embedding and return the top 5 most similar embeddings.\n\nHere is the code snippet I have started with:\n\n```python\nimport torch\n\nembeddings = torch.randn(100, 128)  # Random tensor to represent image embeddings\nquery_embedding = torch.randn(128)   # Random tensor for the query embedding\n\n# Your task is to compute the cosine similarity and get the indices of the top 5 most similar embeddings.\nsimilarity_scores = ...  # Fill in this line\n\n# Get the indices of the top 5 most similar embeddings\ntop_5_indices = torch.topk(similarity_scores, 5).indices\n```\n\nWhat should I put in place of the ellipsis (...)?",
    "output": "\n```python\nsimilarity_scores = torch.nn.functional.cosine_similarity(embeddings, query_embedding.unsqueeze(0), dim=1)\n```"
  },
  {
    "input": "Problem:\nI'm building a recommendation system using PyTorch to compute the similarity between user embeddings and item embeddings. I have two tensors: `user_embeddings` of shape [50, 128] representing 50 users with 128-dimensional feature vectors, and `item_embeddings` of shape [100, 128] for 100 items. I want to compute the cosine similarity between each user and each item to create a similarity matrix.\n\nThe cosine similarity is defined as:\n\n\\[\n\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\n\\]\n\nwhere \\( A \\cdot B \\) is the dot product and \\( ||A|| \\) and \\( ||B|| \\) are the norms of the respective tensors. \n\nHow can I compute the cosine similarity efficiently, resulting in a tensor of shape [50, 100]?\n\n```python\nimport torch\n\nuser_embeddings = torch.randn(50, 128)\nitem_embeddings = torch.randn(100, 128)\n```\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nuser_norms = user_embeddings.norm(dim=1, keepdim=True)\nitem_norms = item_embeddings.norm(dim=1, keepdim=True)\nsimilarity_matrix = torch.mm(user_embeddings, item_embeddings.t()) / (user_norms * item_norms.t())\nresult = similarity_matrix\n</code>"
  },
  {
    "input": "Problem:\nI have a dataset containing information about various customers and their purchasing behavior, which has several numerical features. However, some features contain outliers that could skew my analysis. I want to detect and remove outliers based on the IQR method, specifically identifying data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, for each feature individually. The dataset is represented as a Pandas DataFrame, `df`, and I want to return a cleaned DataFrame without these outliers. How can I modify my DataFrame accordingly?\n\nHere is some example code to start with:\n```python\nimport pandas as pd\n\ndata = {\n    'Age': [25, 27, 29, 34, 200],  # 200 is an outlier\n    'Salary': [50000, 54000, 58000, 60000, 1000000],  # 1000000 is an outlier\n}\ndf = pd.DataFrame(data)\n```\nWhat would the line of code look like to perform this outlier removal effectively?\nA:\n<code>\ndf_cleaned = ... # put solution in this variable\n</code>",
    "output": "\n<code>\ndf_cleaned = df[(df.apply(lambda x: (x >= x.quantile(0.25) - 1.5 * (x.quantile(0.75) - x.quantile(0.25))) & \n                                    (x <= x.quantile(0.75) + 1.5 * (x.quantile(0.75) - x.quantile(0.25))), axis=0)).all(axis=1)]\n</code>"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing embeddings represented as a 3D tensor, which denotes a batch of sequences. Your task is to perform a similarity computation between two sets of embeddings via cosine similarity. The embeddings of two sequences are given in the following tensors:\n\n```python\nimport torch\n\nembeddings_a = torch.randn(50, 30, 128)  # 50 sequences, 30 time steps, 128 features\nembeddings_b = torch.randn(50, 30, 128)  # Another set of embeddings\n```\n\nYou need to compute the cosine similarity between the corresponding sequences from `embeddings_a` and `embeddings_b` across the last dimension (feature dimension). The result should be a tensor of shape `[50, 30]` where each element at position `(i, j)` represents the cosine similarity between the embeddings of sequence `i` at time step `j` from both tensors.\n\nYou can use the following commands to calculate the similarities. Fill in the blank for the `similarity` variable.\n\nA:\n\n```python\nimport torch\n\nembeddings_a = torch.randn(50, 30, 128)  # 50 sequences, 30 time steps, 128 features\nembeddings_b = torch.randn(50, 30, 128)  # Another set of embeddings\n\n# Normalize the embeddings\nnorm_a = embeddings_a / embeddings_a.norm(dim=-1, keepdim=True)\nnorm_b = embeddings_b / embeddings_b.norm(dim=-1, keepdim=True)\n\nsimilarity = ...  # put solution in this variable\n```\n\nBEGIN SOLUTION\n```python\nsimilarity = torch.bmm(norm_a, norm_b.transpose(1, 2)).squeeze(-1)\n```",
    "output": "\n```python\nsimilarity = torch.bmm(norm_a, norm_b.transpose(1, 2)).squeeze(-1)\n```"
  },
  {
    "input": "Problem:\nI am working with two DataFrames, X and Y, that represent sales data over time. The structure of the DataFrames is as follows:\n\nX:\n```\n   Product  Sales        Date\n0       A    200  2023-01-01\n1       B    150  2023-01-02\n2       C    300  2023-01-03\n3       A    250  2023-01-01\n```\n\nY:\n```\n   Product  Sales        Date\n0       A    180  2023-01-02\n1       D    330  2023-01-04\n2       B    160  2023-01-03\n```\n\nI need to merge these DataFrames to create a consolidated view of sales data, ensuring that any duplicate entries for a Product on the same Date are merged such that their Sales values are summed. The resulting DataFrame should have the unique combinations of Product and Date, while maintaining the original data integrity and sorting by Date first and then by Product. The expected structure of the output DataFrame is:\n\n```\n   Product  Sales        Date\n0       A    200  2023-01-01\n1       A    180  2023-01-02\n2       B    150  2023-01-02\n3       B    320  2023-01-03 \n4       C    300  2023-01-03\n5       D    330  2023-01-04\n```\n\nCurrently, I am trying to use `pd.concat` but I am unsure how to handle the duplicates and sum their Sales properly.\n\nA:\n<code>\nimport pandas as pd\n\nX = pd.DataFrame({\n    'Product': ['A', 'B', 'C', 'A'],\n    'Sales': [200, 150, 300, 250],\n    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-01']\n})\n\nY = pd.DataFrame({\n    'Product': ['A', 'D', 'B'],\n    'Sales': [180, 330, 160],\n    'Date': ['2023-01-02', '2023-01-04', '2023-01-03']\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.concat([X, Y]).groupby(['Product', 'Date'], as_index=False).sum().sort_values(by=['Date', 'Product'])\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset containing information about customer purchases from an online retail store. The dataset is stored in a CSV file called `purchases.csv` and has the following columns: `customer_id`, `purchase_amount`, and `purchase_date`. You want to calculate the monthly purchasing trends by aggregating the total purchase amounts for each customer and each month. How can you get the aggregated totals for each customer per month in a new DataFrame?\n\nAssuming that you have already loaded the data into a DataFrame called `df`, please fill in the blank below to create the new aggregated DataFrame:\n\nA:\n```python\nimport pandas as pd\n\ndf = pd.read_csv('purchases.csv')\ndf['purchase_date'] = pd.to_datetime(df['purchase_date'])\ndf['month'] = df['purchase_date'].dt.to_period('M')\naggregated_df = df.groupby(['customer_id', 'month'])['purchase_amount'].sum().reset_index()\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = aggregated_df\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing information about various individuals, including their age, income, and whether they have purchased a particular product (1 for yes, 0 for no). You need to preprocess this data by applying a MinMaxScaler to the 'age' and 'income' columns, then use the preprocessed data to train a logistic regression model to predict the purchase outcome. Given the following code:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\n\ndata = load_data()  # Assume this loads a DataFrame with 'age', 'income', and 'purchase' columns\nX = data[['age', 'income']]\ny = data['purchase']\n\nscaler = MinMaxScaler()\n# Apply the scaler to X\n```\n\nWhat line of code should fill in the blank for scaling the features?\n\nA:\n\n```python\nX_scaled = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nX_scaled = scaler.fit_transform(X)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames, `sales` and `returns`, that need to be merged and processed for analysis. The `sales` DataFrame looks like this:\n\n```\n    OrderID     Date     Amount\n0      1001 2023-01-01     250.0\n1      1002 2023-01-02     150.0\n2      1003 2023-01-02     300.0\n3      1004 2023-01-03     200.0\n```\n\nAnd the `returns` DataFrame looks like this:\n\n```\n    OrderID     Date    Amount\n0      1002 2023-01-02      50.0\n1      1003 2023-01-03     100.0\n2      1005 2023-01-03     20.0\n```\n\nYour task is to combine these two DataFrames, ensuring that you include all sales records and the respective return amounts, such that if a sale has a corresponding return, the return amount should be deducted from the total sale amount. Furthermore, ensure that the `Date` format in the resulting DataFrame is simplified to \"DD-MMM-YYYY\". \n\nThe expected output should look like this:\n```\n    OrderID     Date     NetAmount\n0      1001 01-Jan-2023     250.0\n1      1002 02-Jan-2023     100.0\n2      1003 02-Jan-2023     200.0\n3      1004 03-Jan-2023     200.0\n4      1005 03-Jan-2023       NaN\n```\n\nCurrently, you have tried merging them like this:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({'OrderID': [1001, 1002, 1003, 1004],\n                      'Date': ['2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03'],\n                      'Amount': [250.0, 150.0, 300.0, 200.0]})\n\nreturns = pd.DataFrame({'OrderID': [1002, 1003, 1005],\n                        'Date': ['2023-01-02', '2023-01-03', '2023-01-03'],\n                        'Amount': [50.0, 100.0, 20.0]})\n```\n\nAnd you are unsure how to calculate the `NetAmount` and format the dates correctly.\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nmerged = pd.merge(sales, returns, on='OrderID', how='left', suffixes=('_sale', '_return'))\nmerged['NetAmount'] = merged['Amount_sale'] - merged['Amount_return'].fillna(0)\nmerged['Date'] = pd.to_datetime(merged['Date_sale']).dt.strftime('%d-%b-%Y')\nresult = merged[['OrderID', 'Date', 'NetAmount']]\n```"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame representing sales data with two columns: 'Product' and 'Quantity'. Some entries in the 'Quantity' column are missing (NaN). I want to create a new column 'Adjusted_Quantity' where I fill in the missing values with the median of the 'Quantity' column. Then, I want to apply a discount of 10% to products with an 'Adjusted_Quantity' greater than 50, and those quantities should be reflected in another new column 'Discounted_Quantity'. \n\nHere is a portion of the DataFrame:\n```\n  Product  Quantity\n0       A      60.0\n1       B       NaN\n2       C      80.0\n3       D      40.0\n4       E       NaN\n```\n\nAfter performing the above operations, the DataFrame should look something like this with 'Adjusted_Quantity' and 'Discounted_Quantity':\n```\n  Product  Quantity  Adjusted_Quantity  Discounted_Quantity\n0       A      60.0                60.0                 54.0\n1       B       NaN                60.0                 54.0\n2       C      80.0                80.0                 72.0\n3       D      40.0                40.0                  NaN\n4       E       NaN                60.0                 54.0\n```\n\nHow can I implement this using pandas? \n\nA:\n<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Product': ['A', 'B', 'C', 'D', 'E'], 'Quantity': [60.0, None, 80.0, 40.0, None]})\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf['Adjusted_Quantity'] = df['Quantity'].fillna(df['Quantity'].median())\ndf['Discounted_Quantity'] = df['Adjusted_Quantity'].apply(lambda x: x * 0.9 if x > 50 else None)\n</code>"
  },
  {
    "input": "Problem:\nI have a dataset containing information about customers and their purchasing behavior. I want to preprocess this data to better analyze the factors influencing their purchase decisions. Specifically, I'd like to fill missing values in the 'Income' column with the median income of the age group the customer belongs to. Assuming I have a pandas DataFrame `df` that includes 'Age' and 'Income' columns, how can I implement this column-wise transformation using the 'Age' groups (e.g., 18-25, 26-35, etc.) to fill the missing 'Income' values? \n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'Age': [22, 29, 35, 22, 44, 29, 19, 38, 45, 26],\n    'Income': [30000, None, 50000, 32000, None, None, 28000, 60000, 70000, 35000]\n}\ndf = pd.DataFrame(data)\n\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nage_bins = [18, 25, 35, 45, 100]\nage_labels = ['18-25', '26-35', '36-45', '46+']\ndf['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n\nmedian_income = df.groupby('AgeGroup')['Income'].transform('median')\ndf['Income'] = df['Income'].fillna(median_income)\nresult = df\n</code>"
  },
  {
    "input": "Problem:\nYou are given a DataFrame containing sales data for different products over various months, structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product_A', 'Q1'): [200, 250, 300],\n    ('Product_A', 'Q2'): [220, 240, 320],\n    ('Product_B', 'Q1'): [150, 175, 200],\n    ('Product_B', 'Q2'): [180, 190, 210],\n}\n\nindex = ['January', 'February', 'March']\ndf = pd.DataFrame(data, index=index)\n```\n\nYou need to normalize the sales data for each product (column-wise normalization) and then create a new DataFrame that retains the original multi-index structure while also appending a new row at the bottom with the normalized sums for each product across all months. How can you accomplish this?\n\nA:\n<code>\ndf_normalized = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf_normalized = df.div(df.sum(axis=0), axis=1)\nnormalized_sums = df_normalized.sum(axis=0).to_frame().T\nnormalized_sums.index = ['Normalized_Sum']\ndf_normalized = pd.concat([df_normalized, normalized_sums])\n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset containing sales records with missing entries for some products, and I want to fill in those missing values using the mean of the existing sales for their respective categories. The dataset is structured like this:\n\n```\n+------------+-----------+--------+\n| category   | product   | sales  |\n+------------+-----------+--------+\n| electronics| phone     | 150    |\n| electronics| laptop    | NaN    |\n| groceries  | apple     | 30     |\n| groceries  | banana    | NaN    |\n| groceries  | orange    | 25     |\n| furniture   | chair     | 75     |\n| furniture   | table     | NaN    |\n+------------+-----------+--------+\n```\n\nI attempted to use a groupby operation, but I'm unsure how to fill the NaN values correctly while maintaining the mean for each category.\n\nI'd like to know how to calculate the mean sales per category and fill in the missing sales values for the products accordingly. \n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'category': ['electronics', 'electronics', 'groceries', 'groceries', 'groceries', 'furniture', 'furniture'],\n    'product': ['phone', 'laptop', 'apple', 'banana', 'orange', 'chair', 'table'],\n    'sales': [150, np.nan, 30, np.nan, 25, 75, np.nan]\n})\n\nmean_sales_per_category = df.groupby('category')['sales'].mean()\n```\ndf['sales'] = ... # put solution in this variable",
    "output": "\n```python\ndf['sales'] = df['sales'].fillna(df.groupby('category')['sales'].transform('mean'))\n```"
  },
  {
    "input": "## Problem:\n\nYou are working with a dataset of image features represented as a 4D tensor, where the dimensions correspond to: batch size x channels x height x width. \n\nAssume you have the following tensor:\n\n```python\nimport torch\nfeatures = torch.rand((8, 3, 32, 32))  # 8 images, 3 channels, 32x32 each\n```\n\nYou want to perform a boolean mask operation to conditionally modify specific channels of the tensor based on a condition provided by another tensor, which denotes a threshold for each channel.\n\nYou have a threshold tensor that specifies a value for each channel as follows:\n\n```python\nthresholds = torch.tensor([0.5, 0.6, 0.4])  # Threshold for each channel\n```\n\nYour goal is to set all pixel values in `features` to `0` for channels that do not meet their respective threshold values across the batch dimension.\n\nHow would you write the code to achieve the desired masking operation on the `features` tensor?\n\n```python\n# Replace values in features based on the condition on thresholds\nmask = features.mean(dim=(2, 3)) > ...  # fill in the solution here\nfeatures[~mask[:, :, None, None]] = 0\n```\n\nA:\n```python\n...  # put solution in this variable\n```",
    "output": "\n```python\nthresholds\n```"
  },
  {
    "input": "### Problem:\nI have a dataset containing information about movies with features such as title, genre, and user ratings from different platforms. I'd like to preprocess this dataset to prepare it for machine learning. Specifically, I want to one-hot encode the genre column and scale the user ratings between 0 and 1. \n\nHere's a sample of my DataFrame:\n\n+-----------------------+----------------+--------------+\n| title                 | genre          | user_rating  |\n+-----------------------+----------------+--------------+\n| The Shawshank Redemption | Drama       | 9.3          |\n| The Godfather         | Crime, Drama   | 9.2          |\n| The Dark Knight       | Action, Crime   | 9.0          |\n| Schindler's List      | Biography, Drama | 8.9         |\n| Pulp Fiction          | Crime, Drama   | 8.9          |\n+-----------------------+----------------+--------------+\n\nI want to transform the DataFrame to include one-hot encoded columns for each genre and scale the user ratings. The resulting DataFrame should look like this:\n\n+-----------------------+--------------+--------------+----------------+-----------------+\n| title                 | user_rating  | Action       | Biography       | Crime           | Drama          |\n+-----------------------+--------------+--------------+----------------+-----------------+\n| The Shawshank Redemption | 0.9       | 0            | 0              | 0               | 1              |\n| The Godfather         | 0.8          | 0            | 0              | 1               | 1              |\n| The Dark Knight       | 0.7          | 1            | 0              | 1               | 0              |\n| Schindler's List      | 0.6          | 0            | 1              | 0               | 1              |\n| Pulp Fiction          | 0.6          | 0            | 0              | 1               | 1              |\n+-----------------------+--------------+--------------+----------------+-----------------+\n\nYou can assume you have already imported necessary libraries and have the DataFrame `df` loaded with the movie data. Fill in the blank for the transformation step:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n\n# Sample DataFrame\ndata = {\n    'title': ['The Shawshank Redemption', 'The Godfather', 'The Dark Knight', 'Schindler\\'s List', 'Pulp Fiction'],\n    'genre': ['Drama', 'Crime, Drama', 'Action, Crime', 'Biography, Drama', 'Crime, Drama'],\n    'user_rating': [9.3, 9.2, 9.0, 8.9, 8.9]\n}\ndf = pd.DataFrame(data)\n\n# Begin transformation\nencoder = OneHotEncoder(sparse=False)\nscaler = MinMaxScaler()\n\n# _______ (Fill in the blank with the correct transformation code)\n\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\n# One-hot encode the genre column\ngenre_encoded = encoder.fit_transform(df['genre'].str.get_dummies(sep=', '))\ngenre_df = pd.DataFrame(genre_encoded, columns=encoder.get_feature_names_out(['genre']))\n\n# Scale the user ratings\ndf['user_rating'] = scaler.fit_transform(df[['user_rating']])\n\n# Concatenate the original DataFrame with the one-hot encoded genres\ndf = pd.concat([df[['title', 'user_rating']], genre_df], axis=1)\n</code>\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame that contains sales data for different products across multiple regions, and it looks like this:\n\n```\n   Product      Region    Sales\n0      A       North    100\n1      A       South    200\n2      B       North    NaN\n3      B       South    150\n4      C       North    300\n5      C       South    NaN\n```\n\nI want to create a new column that calculates the average sales for each product while ignoring NaN values. If a product has no sales data (i.e., all values are NaN), the average should be recorded as 0. \n\nThe expected output is:\n\n```\n   Product      Region    Sales      Average_Sales\n0      A       North    100          150.0\n1      A       South    200          150.0\n2      B       North    NaN          150.0\n3      B       South    150          150.0\n4      C       North    300          150.0\n5      C       South    NaN          150.0\n```\n\nHere's the DataFrame definition in my code:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Product': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'Region': ['North', 'South', 'North', 'South', 'North', 'South'],\n    'Sales': [100, 200, None, 150, 300, None]\n})\n```\nHow can I achieve this?\n\nA:\n<code>\ndf['Average_Sales'] = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['Average_Sales'] = df.groupby('Product')['Sales'].transform(lambda x: x.mean() if x.notna().any() else 0)\n</code>"
  },
  {
    "input": "Problem:\nYou have a dataset containing customer information and their purchasing patterns, which includes the columns 'Age', 'Annual_Income', and 'Spending_Score'. You want to segment the customers into clusters based on these features using KMeans clustering from scikit-learn. However, before applying the clustering, you need to standardize the feature values to ensure the clustering algorithm works effectively.\n\nHere is a sample of your data:\n\n```python\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'Age': [25, 45, 35, 50, 23, 40, 28, 55],\n    'Annual_Income': [50000, 80000, 62000, 70000, 53000, 91000, 68000, 43000],\n    'Spending_Score': [60, 30, 40, 70, 50, 90, 80, 20]\n})\n```\n\nYour task is to write one line of code that fits a KMeans model with 3 clusters after standardizing the features. Assume that `StandardScaler` and `KMeans` are already imported.\n\nA:\n```python\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n... # put solution in this variable\n```",
    "output": "\n```python\nkmeans_model = KMeans(n_clusters=3).fit(StandardScaler().fit_transform(data))\n```"
  },
  {
    "input": "Problem:\n\nI am working with a dataset containing user ratings for multiple items, represented as a PyTorch tensor. I need to compute the cosine similarity between each pair of user rating vectors to identify which users have similar preferences. \n\nGiven the ratings tensor, I want to create a boolean mask that filters out user pairs with a cosine similarity below a certain threshold (e.g., 0.5) and returns a tensor that contains only the pairs meeting this criteria.\n\nThe ratings tensor is given below:\n```\nratings = torch.tensor([[5, 1, 0], \n                        [4, 0, 2], \n                        [0, 3, 4], \n                        [0, 0, 5]])\n```\n\nHow can I generate a mask for this filtering operation based on the cosine similarity calculation? \n\nA:\n<code>\nimport torch\nratings = torch.tensor([[5, 1, 0], \n                        [4, 0, 2], \n                        [0, 3, 4], \n                        [0, 0, 5]])\n</code>\nmask = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom torch.nn.functional import cosine_similarity\n\n# Compute cosine similarity between each pair of users\nsimilarity_matrix = cosine_similarity(ratings.unsqueeze(1), ratings.unsqueeze(0), dim=2)\n\n# Create a boolean mask for similarity above the threshold\nthreshold = 0.5\nmask = similarity_matrix > threshold\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working on a recommendation system using PyTorch, and you want to compute the similarity between user and item embeddings. Your user and item representations are stored in tensors where the shape of the user tensor is [num_users, embedding_dim] and the item tensor is [num_items, embedding_dim]. After computing the similarity scores, you want to apply a boolean mask to select only the scores above a certain threshold for further processing.\n\nGiven a user embedding tensor and an item embedding tensor, how can you calculate the cosine similarity between the user embeddings and the item embeddings, then apply a mask to keep only similarities that are greater than a defined threshold?\n\nFor example, let\u2019s assume you have the following user and item embeddings:\n\nuser_embeddings = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nitem_embeddings = torch.tensor([[0.7, 0.8, 0.9], [0.1, 0.1, 0.1]])\nthreshold = 0.5\n\nYou need to return a tensor containing the similarities that exceed the threshold.\n\nA:\n\n<code>\nimport torch\n\nuser_embeddings = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nitem_embeddings = torch.tensor([[0.7, 0.8, 0.9], [0.1, 0.1, 0.1]])\nthreshold = 0.5\n</code>\nsimilarities = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nsimilarities = torch.nn.functional.cosine_similarity(user_embeddings.unsqueeze(1), item_embeddings.unsqueeze(0), dim=2)\nsimilarities = similarities[similarities > threshold]\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame that contains customer transaction data with several columns, including customer ID, transaction date, and transaction amount. The DataFrame looks like this:\n\n```\ncustomer_id | transaction_date | amount\n------------|------------------|-------\n      101   |   2023-01-15     |  200\n      102   |   2023-01-16     |  150\n      101   |   2023-01-17     |  300\n      103   |   2023-01-18     |  400\n      102   |   2023-01-19     |  250\n```\n\nI want to transform this DataFrame to have a multi-index defined by `customer_id` and `transaction_date`, and also normalize the `amount` column such that the values are transformed to be between 0 and 1 based on the max transaction amount per customer.\n\nThe final DataFrame should look like this:\n\n```\n                     amount\ncustomer_id transaction_date      \n      101   2023-01-15   0.00\n             2023-01-17   1.00\n      102   2023-01-16   0.00\n             2023-01-19   1.00\n      103   2023-01-18   1.00\n```\n\nWhat would be the most idiomatic way to achieve this in Pandas?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'customer_id': [101, 102, 101, 103, 102],\n    'transaction_date': ['2023-01-15', '2023-01-16', '2023-01-17', '2023-01-18', '2023-01-19'],\n    'amount': [200, 150, 300, 400, 250]\n}\n\ndf = pd.DataFrame(data)\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf = (df.set_index(['customer_id', 'transaction_date'])\n        .groupby(level=0)\n        .apply(lambda x: x['amount'] / x['amount'].max())\n        .reset_index(name='amount')\n        .set_index(['customer_id', 'transaction_date']))\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing customer information, and you want to perform data normalization and dimensionality reduction in preparation for a machine learning model. Given your data in a pandas DataFrame `df` with numerical features, you are using the `StandardScaler` for normalization followed by `PCA` for dimensionality reduction.\n\nHere is an example code setup:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# Assume df is your input DataFrame with numerical columns only\n```\n\nYou want to extract the transformed features after applying normalization but before PCA is applied. How can you achieve this?\n\nStore the output of the transformation from `StandardScaler` into the variable `scaled_data`.\n\nscaled_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n```"
  },
  {
    "input": "Problem:\n\nI am working with a PyTorch model that takes a tensor of shape (batch_size, sequence_length, feature_dimension) as input. I need to compute the cosine similarity between the first and second entries across the batch, and I want to return a tensor of shape (batch_size, 1) that contains the cosine similarity for each pair.\n\nThe formula for cosine similarity is given by:\n\\[ \\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]\n\nFor instance, if my tensor looks like this:\n```python\ninput_tensor = torch.tensor([\n    [[1, 2, 3], [4, 5, 6]],\n    [[7, 8, 9], [1, 0, 0]]\n])\n```\nThen I want a tensor that contains the cosine similarity of the first and second elements of the sequences from each batch.\n\nHow can I achieve this in PyTorch?\n\nA:\n<code>\nimport torch\n\ninput_tensor = torch.tensor([\n    [[1, 2, 3], [4, 5, 6]],\n    [[7, 8, 9], [1, 0, 0]]\n])\n\ndef compute_cosine_similarity(tensor):\n    # return the cosine similarity in this function\n    # similarity = compute_cosine_similarity(tensor)\n    ### BEGIN SOLUTION\n",
    "output": "\n<code>\n    A = tensor[:, 0, :]  # First entry across the batch\n    B = tensor[:, 1, :]  # Second entry across the batch\n\n    dot_product = (A * B).sum(dim=1)  # Compute dot product\n    norm_A = A.norm(dim=1)  # Compute norm of A\n    norm_B = B.norm(dim=1)  # Compute norm of B\n\n    cosine_similarity = dot_product / (norm_A * norm_B)  # Compute cosine similarity\n    return cosine_similarity.unsqueeze(1)  # Return shape (batch_size, 1)\n\nsimilarity = compute_cosine_similarity(input_tensor)\n</code>"
  },
  {
    "input": "Problem:\nYou are given a dataset containing customer transaction records stored in a Pandas DataFrame. Each record includes a customer ID, purchase date, product category, and transaction amount. Your goal is to normalize the transaction amounts across each product category to understand how individual transactions compare within their category.\n\nThe DataFrame is structured as follows:\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': [1, 2, 1, 3, 2, 3],\n    'purchase_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06'],\n    'product_category': ['A', 'A', 'B', 'B', 'A', 'B'],\n    'transaction_amount': [100, 200, 150, 300, 250, 130]\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to calculate the normalized transaction amount for each transaction, where normalization is done using the mean and standard deviation of the transaction amounts within each product category.\n\nFill in the blank in the following code to achieve this normalization:\n```python\ndf['normalized_amount'] = (df['transaction_amount'] - df.groupby('product_category')['transaction_amount'].transform('mean')) / df.groupby('product_category')['transaction_amount'].transform('std')\n```\nHow would you write the assignment statement to correctly compute the `normalized_amount` column?\n\nA:\n<code>\ndf['normalized_amount'] = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\ndf['normalized_amount'] = (df['transaction_amount'] - df.groupby('product_category')['transaction_amount'].transform('mean')) / df.groupby('product_category')['transaction_amount'].transform('std')\n</code>"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame that contains the scores of different students across multiple subjects, structured as follows:\n\n```python\nimport pandas as pd\ndf = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Charlie'],\n    'Math': [88, 92, 75],\n    'Science': [90, 95, 80],\n    'English': [85, 89, 70]\n})\n```\n\nI want to normalize the scores for each subject to be between 0 and 1, and I would like the resulting DataFrame to maintain the same structure, with scores replaced by their normalized values.\n\nCan you provide the correct code that fills in the blank below to accomplish this normalization using the Min-Max Scaling technique? \n\nA:\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnormalized_scores = ... # put solution in this variable\n```",
    "output": "\n```python\nnormalized_scores = scaler.fit_transform(df[['Math', 'Science', 'English']])\ndf[['Math', 'Science', 'English']] = normalized_scores\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products across several regions over multiple years. The DataFrame is structured with a multi-index consisting of 'Region' and 'Product', and columns for 'Year', 'Sales', and 'Returns'. Your goal is to normalize the 'Sales' column by converting it to a z-score within each product category across all regions for the year 2020.\n\nConsider the following DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('North', 'Widget A'): {2019: 100, 2020: 150, 2021: 200},\n    ('North', 'Widget B'): {2019: 120, 2020: 110, 2021: 130},\n    ('South', 'Widget A'): {2019: 90, 2020: 160, 2021: 190},\n    ('South', 'Widget B'): {2019: 160, 2020: 180, 2021: 210}\n}\n\ndf = pd.DataFrame(data).T\ndf.index.names = ['Region', 'Product']\ndf.columns.name = 'Year'\n```\n\nYou need to calculate the z-score for 'Sales' in 2020 and store the result in a new column 'Sales_Z'. The resulting DataFrame should keep the original structure with the additional 'Sales_Z' column.\n\nHow can you accomplish this by filling in the blank below?\n\nA:\n```python\ndf['Sales_Z'] = (df.xs(2020, level='Year') - df.xs(2020, level='Year').mean()) / df.xs(2020, level='Year').std()\n```",
    "output": "\n```python\ndf['Sales_Z'] = (df.xs(2020, level='Year')['Sales'] - df.xs(2020, level='Year')['Sales'].mean()) / df.xs(2020, level='Year')['Sales'].std()\n```"
  },
  {
    "input": "## Problem:\n\nI have a set of images in the form of a 4D tensor where the dimensions are (batch_size, channels, height, width). My task is to apply a threshold to the pixel values to create a binary mask of the images, where pixels greater than a specific threshold are set to 1, and those less than or equal to the threshold are set to 0.\n\nGiven a tensor of images `images_tensor` and a threshold value `threshold`, I need to create the binary mask using boolean masking. \n\nThe current setup looks like this:\n\n```python\nimport torch\n\n# Assume images_tensor is your input tensor with shape (batch_size, channels, height, width)\nimages_tensor = load_data()  # example: shape (32, 3, 64, 64)\nthreshold = 0.5\n```\n\nWhat code should I use to generate the binary mask?\n\nA:\n\n```python\nbinary_mask = ... # put solution in this variable\n```",
    "output": "\n```python\nbinary_mask = (images_tensor > threshold).float()\n```"
  },
  {
    "input": "Problem:\nContext  \nI have two DataFrames representing sales data and returns data. The sales DataFrame contains transactions, while the returns DataFrame contains records of returned items.\n\nSales DataFrame (sales_df):\n```\n   order_id  product_id  quantity  sale_date\n0        1         101         2  2023-01-01\n1        2         102         1  2023-01-02\n2        3         101         1  2023-01-03\n3        4         103         4  2023-01-04\n4        5         104         3  2023-01-05\n```\n\nReturns DataFrame (returns_df):\n```\n   return_id  product_id  quantity  return_date\n0         1         101         1  2023-01-03\n1         2         102         1  2023-01-05\n2         3         104         2  2023-01-06\n```\n\nI want to create a new DataFrame that combines both sales and returns, with the quantity of each product adjusted based on the returns. If a product is returned, its sold quantity should be decreased accordingly in the final DataFrame. The new DataFrame should look like this:\n\nFinal DataFrame:\n```\n   product_id  sold_quantity  return_quantity\n0         101              1                1\n1         102              0                1\n2         103              4                0\n3         104              1                2\n```\n\nThe `sold_quantity` is calculated as the total quantity sold minus the total returned quantity for each product.\n\nHow can I achieve this using Pandas?  \nHere's the setup:\n```python\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'order_id': [1, 2, 3, 4, 5],\n    'product_id': [101, 102, 101, 103, 104],\n    'quantity': [2, 1, 1, 4, 3],\n    'sale_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']\n})\n\nreturns_df = pd.DataFrame({\n    'return_id': [1, 2, 3],\n    'product_id': [101, 102, 104],\n    'quantity': [1, 1, 2],\n    'return_date': ['2023-01-03', '2023-01-05', '2023-01-06']\n})\n```\n\nresult = ... # put solution in this variable  \nBEGIN SOLUTION  \n```",
    "output": "\n```python\nsales_summary = sales_df.groupby('product_id')['quantity'].sum().reset_index()\nreturns_summary = returns_df.groupby('product_id')['quantity'].sum().reset_index()\n\nmerged_df = pd.merge(sales_summary, returns_summary, on='product_id', how='outer', suffixes=('_sold', '_returned')).fillna(0)\nmerged_df['sold_quantity'] = merged_df['quantity_sold'] - merged_df['quantity_returned']\nmerged_df = merged_df[['product_id', 'sold_quantity', 'quantity_returned']]\nmerged_df.columns = ['product_id', 'sold_quantity', 'return_quantity']\n\nresult = merged_df\n```"
  },
  {
    "input": "Problem:\nI have a PyTorch tensor that represents a batch of user ratings for different movies. The shape of the tensor is (number_of_users, number_of_movies). I need to implement a function that computes the cosine similarity between two specific users based on their ratings. The output should be a single float value representing their cosine similarity.\n\nHere is a portion of my code:\n\n```python\nimport torch\n\nratings = torch.tensor([[5.0, 0.0, 3.0], \n                        [4.0, 1.0, 0.0], \n                        [0.0, 0.0, 2.0]])\n\nuser1_index = 0  # First user's index\nuser2_index = 1  # Second user's index\n```\n\nHow can I calculate the cosine similarity between user 1 and user 2 using this ratings tensor? Please fill in the blank below.\n\nA:\n<code>\ndef cosine_similarity(ratings, user1_index, user2_index):\n    user1_ratings = ratings[user1_index, :]\n    user2_ratings = ratings[user2_index, :]\n    \n    # Calculate cosine similarity here\n</code>\nsimilarity = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>\nsimilarity = torch.nn.functional.cosine_similarity(user1_ratings.unsqueeze(0), user2_ratings.unsqueeze(0)).item()\n</code>",
    "output": "\n<code>\nsimilarity = torch.nn.functional.cosine_similarity(user1_ratings.unsqueeze(0), user2_ratings.unsqueeze(0)).item()\n</code>"
  },
  {
    "input": "### Problem\nYou are working with two datasets containing sales records. The first DataFrame, `sales_data`, contains sales information with possible duplicate entries. The second DataFrame, `discounts_data`, holds discount percentages applicable based on item categories. Your goal is to combine these datasets in a way that calculates the final sales amount after applying the respective discounts.\n\n`sales_data`:\n```\n   item_id    category  amount\n0       A      toys      100\n1       B      books     200\n2       A      toys      150\n3       C      books      300\n4       A      toys      200\n```\n\n`discounts_data`:\n```\n   category  discount\n0      toys      0.10\n1      books     0.15\n```\n\n1. First, remove duplicate entries in the `sales_data` based on the `item_id`, keeping the last occurrence.\n2. Calculate the final sales amount for each category after applying the discount. \n3. After merging, your final DataFrame should include `item_id`, `category`, `amount`, and `final_amount` (i.e., amount after discount).\n\nPlease write the appropriate code to complete the following task:\n```python\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'item_id': ['A', 'B', 'A', 'C', 'A'],\n    'category': ['toys', 'books', 'toys', 'books', 'toys'],\n    'amount': [100, 200, 150, 300, 200]\n})\n\ndiscounts_data = pd.DataFrame({\n    'category': ['toys', 'books'],\n    'discount': [0.10, 0.15]\n})\n\nresult = ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nsales_data = sales_data.drop_duplicates(subset='item_id', keep='last')\nmerged_data = pd.merge(sales_data, discounts_data, on='category')\nmerged_data['final_amount'] = merged_data['amount'] * (1 - merged_data['discount'])\nresult = merged_data[['item_id', 'category', 'amount', 'final_amount']]\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset representing sales data for different products over several months. The dataframe includes duplicate entries for some products in certain months, and I need to process this data to keep only the most recent sales record for each product, while also replacing any missing month entries with the most recent available month. Afterward, I want to sort the final dataframe by month.\n\nHere's my initial dataframe:\n\n```python\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\n    'product_id': [1, 1, 2, 2, 3, 3, 1],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-03', '2023-01', np.nan, '2023-01'],\n    'sales': [100, 150, 200, 250, 300, 350, 200]\n})\n```\n\nI executed the following operations to find the most recent record per product:\n\n```python\nlatest = df.loc[df['month'].notnull()].sort_values(by='month').drop_duplicates(subset='product_id', keep='last')\n```\n\nNow, I want to fill the missing month values with the latest available month for each product, and finally sort the dataframe by 'month'.\n\nHow can I achieve the final result with a single line of code that fulfills this requirement?\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'product_id': [1, 1, 2, 2, 3, 3, 1],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-03', '2023-01', np.nan, '2023-01'],\n    'sales': [100, 150, 200, 250, 300, 350, 200]\n})\nfinal_result = ... # your solution here\n### BEGIN SOLUTION",
    "output": "\n<code>\nfinal_result = df.groupby('product_id').apply(lambda x: x.ffill().sort_values(by='month')).reset_index(drop=True).sort_values(by='month')\n</code>"
  },
  {
    "input": "Problem:\nI have two dataframes that contain a list of products and their sales over time. The first dataframe `df_sales` contains columns for `product_id`, `date`, and `sales_amount`, while the second dataframe `df_details` contains columns for `product_id`, `product_name`, and `category`. I need to merge them based on `product_id`, ensuring that we also handle any missing product names from `df_details`. After that, I want to create a new column `sales_status` that categorizes sales into 'High' if sales are greater than 1000, 'Medium' if sales are between 500 and 1000, and 'Low' for sales below 500. Finally, I would like the resultant dataframe to be sorted by `date` in ascending order.\n\nHere are the two dataframes:\n\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3, 2],\n    'date': ['2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03'],\n    'sales_amount': [1000, 2000, 300, 500]\n})\n\ndf_details = pd.DataFrame({\n    'product_id': [1, 2, 3],\n    'product_name': ['Product A', 'Product B', None],\n    'category': ['Category 1', 'Category 2', 'Category 1']\n})\n```\n\nAfter merging, how can I generate the final dataframe as described? Use the following placeholder for the result:\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df_sales.merge(df_details, on='product_id', how='left')\nresult['sales_status'] = pd.cut(result['sales_amount'], bins=[-1, 500, 1000, float('inf')], labels=['Low', 'Medium', 'High'])\nresult = result.sort_values(by='date')\n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset containing information about various houses, including features such as size (in square feet), number of bedrooms, and sale price. I want to preprocess this dataset by applying normalization to the features, which is crucial before training a regression model. I already have the data loaded into a pandas DataFrame called `houses`, and I want to use MinMaxScaler from sklearn to scale the features between 0 and 1.\n\nHere is the initial code I've set up:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nhouses = load_data()  # This function loads the DataFrame with house data\n```\n\nWhat should I fill in the blank to apply the normalization to the 'size' and 'bedrooms' columns of the DataFrame, saving the scaled features back into the same DataFrame?\n\nA:\n\n```python\nscaler = MinMaxScaler()\nhouses[['size', 'bedrooms']] = ...  # put solution in this variable\n```",
    "output": "\n```python\nscaler.fit_transform(houses[['size', 'bedrooms']])\n```"
  },
  {
    "input": "Problem:\n\nI have a tensor `data` representing features of different samples in a machine learning task, with the shape of (100, 20). I also have a boolean mask `mask` of shape (100, 20) that indicates which features should be considered (True for selected features, False for ignored ones). My goal is to apply this mask to the tensor such that only selected features remain in the resulting tensor, and I need to count how many features were selected for each sample.\n\nHere's a snippet of how my tensors look:\n\n```python\ndata = torch.randn(100, 20)  # Shape (100, 20)\nmask = (torch.rand(100, 20) > 0.5)  # Random True/False values\n```\n\nAfter applying the mask to `data`, I want to obtain a tensor that represents the count of selected features for each of the 100 samples.\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\ndata, mask = load_data()\n</code>\nselected_counts = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nselected_counts = mask.sum(dim=1)\n</code>"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing customer transaction records, stored in a pandas DataFrame. The dataset includes information on the transaction date, customer ID, and the purchase amount. The goal is to analyze the spending behavior of customers by creating a new column that categorizes customers into \"low,\" \"medium,\" or \"high\" spenders based on the average transaction amount.\n\nGiven the following DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'transaction_date': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-05', '2023-01-06', '2023-01-07'],\n    'customer_id': ['C1', 'C2', 'C1', 'C2', 'C1', 'C3'],\n    'purchase_amount': [20.0, 45.0, 30.0, 55.0, 10.0, 100.0]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to create a new column called \"spending_category\" that is determined as follows:\n- \"low\" if the average transaction amount is less than $25,\n- \"medium\" if the average transaction amount is between $25 and $50 (inclusive),\n- \"high\" if the average transaction amount is greater than $50.\n\nA:\n```python\n# Fill in the blank with the appropriate code to achieve the task.\ndf['spending_category'] = ...\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf['spending_category'] = pd.cut(df.groupby('customer_id')['purchase_amount'].transform('mean'), \n                                  bins=[-float('inf'), 25, 50, float('inf')], \n                                  labels=['low', 'medium', 'high'])\n```"
  },
  {
    "input": "Problem:\nI have two pandas DataFrames that represent sales data and returns data. The sales DataFrame contains the following columns: 'order_id', 'customer_id', 'date', 'amount'. The returns DataFrame contains 'order_id', 'return_date', and 'amount_returned'. I need to merge these two DataFrames on 'order_id', ensuring that if an order has been returned, the 'amount' in the sales DataFrame is updated to reflect the return. Additionally, I want to keep the original columns intact while adding a new 'final_amount' column that shows the amount after returns have been deducted.\n\nHere are my DataFrames:\n\nsales_df:\n   order_id  customer_id       date  amount\n0        1           A 2023-01-01     100\n1        2           B 2023-01-02     150\n2        3           A 2023-01-03     200\n3        4           C 2023-01-04     300\n\nreturns_df:\n   order_id return_date  amount_returned\n0        2  2023-01-05               50\n1        3  2023-01-06              100\n\nI want the final DataFrame to look like this:\n   order_id  customer_id       date  amount  final_amount\n0        1           A 2023-01-01     100          100\n1        2           B 2023-01-02     150          100\n2        3           A 2023-01-03     200          100\n3        4           C 2023-01-04     300          300\n\nHow can I accomplish this with pandas? \n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'order_id': [1, 2, 3, 4],\n    'customer_id': ['A', 'B', 'A', 'C'],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']),\n    'amount': [100, 150, 200, 300]\n})\n\nreturns_df = pd.DataFrame({\n    'order_id': [2, 3],\n    'return_date': pd.to_datetime(['2023-01-05', '2023-01-06']),\n    'amount_returned': [50, 100]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmerged_df = sales_df.merge(returns_df, on='order_id', how='left')\nmerged_df['final_amount'] = merged_df['amount'] - merged_df['amount_returned'].fillna(0)\nresult = merged_df[['order_id', 'customer_id', 'date', 'amount', 'final_amount']]\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing the sales information of various products along with their sales regions. The dataset is structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product', 'Name'): ['A', 'B', 'C', 'A', 'B'],\n    ('Product', 'Category'): ['Electronics', 'Furniture', 'Electronics', 'Electronics', 'Furniture'],\n    ('Region', 'West'): [100, 200, 150, 300, None],\n    ('Region', 'East'): [None, 250, 100, 400, 150],\n    ('Region', 'North'): [50, 75, None, None, 50]\n}\ndf = pd.DataFrame(data)\n```\n\nThe DataFrame `df` has a multi-level column structure with two levels: `'Product'` and `'Region'`. \n\nYour goal is to normalize the sales data such that you extract a DataFrame containing only the total sales for each product across all regions (summing up the sales values). The resulting DataFrame should have the following format:\n\n```\n    Product   Total_Sales\n0      A          ?\n1      B          ?\n2      C          ?\n```\n\nTo accomplish this, you need to fill in the blank below with the appropriate code:\n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby(('Product', 'Name')).sum().reset_index()\nresult.columns = ['Product', 'Total_Sales']\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset containing information about houses, including features such as square footage, number of bedrooms, and age of the house. You need to preprocess the data by normalizing the numerical features and encoding the categorical features before fitting a linear regression model. The normalization should be done using Min-Max scaling, and the categorical features should be one-hot encoded. You have the following dummy DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'square_footage': [1000, 1500, 2000, 2500],\n    'bedrooms': [2, 3, 4, 5],\n    'age': [10, 15, 20, 25],\n    'location': ['suburb', 'city', 'suburb', 'city']\n}\n\ndf = pd.DataFrame(data)\n```\n\nTo prepare the DataFrame for modeling, you want to apply normalization to the numerical features and one-hot encoding to the 'location' feature. What code should you use to perform these transformations and store the result in a variable called `processed_data`?\n\nA:\n<code>\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nimport pandas as pd\n\ndata = {\n    'square_footage': [1000, 1500, 2000, 2500],\n    'bedrooms': [2, 3, 4, 5],\n    'age': [10, 15, 20, 25],\n    'location': ['suburb', 'city', 'suburb', 'city']\n}\n\ndf = pd.DataFrame(data)\n\nnumerical_features = df[['square_footage', 'bedrooms', 'age']]\ncategorical_features = df[['location']]\n\nscaler = MinMaxScaler()\nencoder = OneHotEncoder()\n\n# Apply transformations\nnormalized_numerical = scaler.fit_transform(numerical_features)\nencoded_categorical = encoder.fit_transform(categorical_features).toarray()\n\n# Combine the results into a single DataFrame\nprocessed_data = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nprocessed_data = pd.DataFrame(normalized_numerical, columns=numerical_features.columns)\nencoded_columns = encoder.get_feature_names_out(categorical_features.columns)\nencoded_df = pd.DataFrame(encoded_categorical, columns=encoded_columns)\nprocessed_data = pd.concat([processed_data, encoded_df], axis=1)\n</code>"
  },
  {
    "input": "Problem:\nYou have two DataFrames, `sales_data` and `returns_data`, that track daily sales and return transactions for a retail store.\n\n`sales_data`:\n```\n+------------+-------+\n|    Date    | Sales |\n+------------+-------+\n| 2023-10-01 |  200  |\n| 2023-10-02 |  150  |\n| 2023-10-03 |  300  |\n| 2023-10-04 |  250  |\n+------------+-------+\n```\n\n`returns_data`:\n```\n+------------+-------+\n|    Date    | Returns|\n+------------+-------+\n| 2023-10-01 |   20  |\n| 2023-10-02 |   10  |\n| 2023-10-03 |   30  |\n| 2023-10-05 |   15  |\n+------------+-------+\n```\n\nYour objective is to merge these two DataFrames on the \"Date\" column, ensuring to align the return values with the respective sales on the same date. For dates available in `sales_data` but not in `returns_data`, the return should be marked as 0. Additionally, you must add a column \"Net_Sales\" to reflect the actual sales after subtracting returns. The resulting DataFrame should not contain any duplicates in the date column, and should be sorted by date.\n\nThe expected output should look like this:\n```\n+------------+-------+---------+-----------+\n|    Date    | Sales | Returns | Net_Sales |\n+------------+-------+---------+-----------+\n| 2023-10-01 |  200  |   20    |    180    |\n| 2023-10-02 |  150  |   10    |    140    |\n| 2023-10-03 |  300  |   30    |    270    |\n| 2023-10-04 |  250  |   0     |    250    |\n| 2023-10-05 |   0   |   15    |   -15     |\n+------------+-------+---------+-----------+\n```\n\nCan you provide the code to achieve this result?\n\nA:\n<code>\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'Date': pd.to_datetime(['2023-10-01', '2023-10-02', '2023-10-03', '2023-10-04']),\n    'Sales': [200, 150, 300, 250]\n})\n\nreturns_data = pd.DataFrame({\n    'Date': pd.to_datetime(['2023-10-01', '2023-10-02', '2023-10-03', '2023-10-05']),\n    'Returns': [20, 10, 30, 15]\n})\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.merge(sales_data, returns_data, on='Date', how='outer').fillna(0)\nresult['Net_Sales'] = result['Sales'] - result['Returns']\nresult = result.sort_values(by='Date').reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\nI am working with a DataFrame that tracks store sales data with potentially missing values, structured like this:\n```\n   Store  Sales\n0      A  2000\n1      A  3000\n2      B  4000\n3      B  null\n4      C  5000\n5      C  6000\n6      C  null\n```\nThe DataFrame was created using:\n```python\ndf = pd.DataFrame({'Store': ['A', 'A', 'B', 'B', 'C', 'C', 'C'], \n                   'Sales': [2000, 3000, 4000, None, 5000, 6000, None]})\n```\nI want to compute the average sales for each store but ignore null values. Additionally, I want to label stores with an average greater than 3500 as \"High Performer\" and others as \"Regular\". The expected output should look like this:\n```\n   Store          Average      Label\n0     A           2500.0     Regular\n1     B           4000.0    High Performer\n2     C           5500.0    High Performer\n```\nI started by grouping and calculating the mean, but I want to know how to apply this custom labeling to the results. What is the correct way to accomplish this?\n\nA:\n<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Store': ['A', 'A', 'B', 'B', 'C', 'C', 'C'], \n                   'Sales': [2000, 3000, 4000, None, 5000, 6000, None]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby('Store', as_index=False).agg(Average=('Sales', 'mean'))\nresult['Label'] = result['Average'].apply(lambda x: 'High Performer' if x > 3500 else 'Regular')\n</code>"
  },
  {
    "input": "Problem:\nYou have a multi-index DataFrame containing sales data for different products sold across various regions, formatted as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\narrays = [\n    ['North', 'North', 'South', 'South'],\n    ['A', 'B', 'A', 'B']\n]\nindex = pd.MultiIndex.from_arrays(arrays, names=('Region', 'Product'))\ndata = np.random.randint(10, 100, size=(4, 3))\ncolumns = ['Q1', 'Q2', 'Q3']\ndf = pd.DataFrame(data, index=index, columns=columns)\n```\n\nThe `df` would look something like this:\n\n```\n             Q1  Q2  Q3\nRegion Product         \nNorth  A     30  50  60\n       B     70  80  90\nSouth  A     20  40  30\n       B     10  60  50\n```\n\nYou need to normalize the values in the DataFrame by the maximum value for each product across all regions, and then store the normalized values in a new DataFrame while preserving the multi-index. \n\nWhat code would you use to achieve this, ensuring the structure of the DataFrame remains the same?\n\n```python\nnormalized_df = ...  # put solution in this variable\n```",
    "output": "\n```python\nnormalized_df = df.div(df.groupby(level='Product').transform('max'))\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames E and F as follows:\nE\n    ID    Value\n0   1     10\n1   2     20\n2   3     30\nF\n    ID    Value\n0   2     25\n1   3     35\n2   4     45\n\nI need to perform a merge between both DataFrames on the 'ID' column, but I want to replace the 'Value' in DataFrame E with the 'Value' from DataFrame F when IDs match. Also, if there are IDs in DataFrame F that do not exist in DataFrame E, they should be added as new rows while maintaining the order of IDs from DataFrame E.\n\nThe resulting DataFrame should look like this:\n    ID    Value\n0   1     10\n1   2     25\n2   3     35\n3   4     45\n\nI used the following code but it did not give the expected result:\n>>> pd.merge(E, F, how='outer', on='ID')\n\nWhat should I use to achieve the desired outcome?\n\nA:\n<code>\nimport pandas as pd\n\nE = pd.DataFrame({\"ID\": [1, 2, 3], \"Value\": [10, 20, 30]})\nF = pd.DataFrame({\"ID\": [2, 3, 4], \"Value\": [25, 35, 45]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = pd.merge(E, F, on='ID', how='outer').fillna(0).assign(Value=lambda x: x['Value_y'].where(x['Value_y'] != 0, x['Value_x'])).drop(columns=['Value_x', 'Value_y'])</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset represented as a 3D Tensor in PyTorch with dimensions (batch_size, channels, height). I want to compute a similarity score between each pair of channels within each batch. Specifically, I want to calculate the cosine similarity of each channel with every other channel. The output should be a 2D Tensor containing the similarity scores for each pair of channels per batch.\n\nHere is what I have so far:\n\n```python\nimport torch\n\n# Assume the following is our input tensor\ninput_tensor = torch.randn(4, 3, 5)  # 4 batches, 3 channels, height 5\n```\n\nWhat should I put in place of `...` to compute the cosine similarity across the channels?\n\nA:\n\n```python\nsimilarity_scores = ...  # put solution in this variable\n```",
    "output": "\n```python\nsimilarity_scores = torch.nn.functional.cosine_similarity(input_tensor.unsqueeze(2), input_tensor.unsqueeze(1), dim=3)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames E and F as follows:\nE\n    product_id   price    date\n0          101   250.0  2023-01-01\n1          102   150.0  2023-01-01\n2          103   300.0  2023-01-02\nF\n    product_id   price    date\n0          101   260.0  2023-01-02\n1          102   150.0  2023-01-02\n2          104   180.0  2023-01-03\n\n\nI need to merge these DataFrames on the `product_id` column while keeping all records from both DataFrames and ensuring that for the same `product_id`, the latest `date` is maintained in the merged DataFrame. The `price` value from the DataFrame F should replace the corresponding value in E where applicable. If a `product_id` exists in F but not in E, it should still be included. The final output should be sorted by `date` and `product_id` in ascending order.\n\nYour merging attempt using pd.merge() has resulted in duplicates and incorrect ordering. Here is my partial code:\n```python\nimport pandas as pd\n\nE = pd.DataFrame({\"product_id\": [101, 102, 103], \"price\": [250.0, 150.0, 300.0], \"date\": [\"2023-01-01\", \"2023-01-01\", \"2023-01-02\"]})\nF = pd.DataFrame({\"product_id\": [101, 102, 104], \"price\": [260.0, 150.0, 180.0], \"date\": [\"2023-01-02\", \"2023-01-02\", \"2023-01-03\"]})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = pd.concat([E, F]).sort_values(by=['date', 'product_id']).drop_duplicates(subset=['product_id'], keep='last').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nContext\nI have two DataFrames representing sales data and product information. The sales DataFrame contains duplicates for products sold in different transactions, while the product DataFrame has relevant product details. \n\nSales DataFrame (sales_df):\n```\nProductID    Date       Quantity \n-----------------------------------\n101          2023-01-01 2\n102          2023-01-01 4\n101          2023-01-02 1\n103          2023-01-01 3\n102          2023-01-01 2\n```\n\nProduct DataFrame (product_df):\n```\nProductID    ProductName    Price\n----------------------------------------\n101          Widget A       25.00\n102          Widget B       15.00\n103          Widget C       20.00\n```\n\nI need to combine these DataFrames to create a new DataFrame that shows the total quantity sold for each product along with the product name and total sales value. The expected final DataFrame should look like this:\n```\nProductID    ProductName    TotalQuantity   TotalSales\n-------------------------------------------------------\n101          Widget A       3               75.00\n102          Widget B       6               90.00\n103          Widget C       3               60.00\n```\n\nTo achieve this, I will refer to the sales DataFrame, group by `ProductID`, sum the `Quantity`, and then merge with the product DataFrame to calculate `TotalSales`.\n\nSo far, I have done the following steps:\n\n1. Group the sales_df by `ProductID` using `groupby()` and calculate the sum of `Quantity`.\n2. Merge the result with product_df to bring in `ProductName` and `Price`.\n\nI am unsure how to calculate `TotalSales` based on the final merged DataFrame.\n\nA:\n```python\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'ProductID': [101, 102, 101, 103, 102],\n    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-01', '2023-01-01'],\n    'Quantity': [2, 4, 1, 3, 2]\n})\n\nproduct_df = pd.DataFrame({\n    'ProductID': [101, 102, 103],\n    'ProductName': ['Widget A', 'Widget B', 'Widget C'],\n    'Price': [25.00, 15.00, 20.00]\n})\n\n# group sales_df by 'ProductID' and sum 'Quantity'\ntotal_quantity = sales_df.groupby('ProductID').agg({'Quantity': 'sum'}).reset_index()\n\n# merge with product_df\nresult = pd.merge(total_quantity, product_df, on='ProductID')\n\n# calculate TotalSales\nresult['TotalSales'] = result['Quantity'] * result['Price']\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = result[['ProductID', 'ProductName', 'Quantity', 'TotalSales']].rename(columns={'Quantity': 'TotalQuantity'})\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data from a retail store with missing values. Each row represents a transaction, including columns for the product category, transaction amount, and discount applied. Your task is to compute a new column called 'final_price' that represents the price after applying the discount, ensuring that you handle cases where either the discount or the transaction amount is null (treat null discounts as 0 and null amounts as 0). \n\nThe DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'C', 'B'],\n    'amount': [100, np.nan, 200, 150, 300],\n    'discount': [10, 20, np.nan, 5, np.nan]\n})\n```\n\nYou are asked to complete the following line of code to create the `final_price` column using a custom function:\n\n```python\ndf['final_price'] = df.apply(lambda row: row['amount'] * (1 - (row['discount'] / 100)) if pd.notnull(row['amount']) and pd.notnull(row['discount']) else np.nan, axis=1)\n```\n\nHowever, using the above code, you find it repetitive and want a more elegant solution for handling null values gracefully using DataFrame methods. \n\nFill in the blank to replace the lambda function:\n\n```python\ndf['final_price'] = df.apply(__________, axis=1)\n```\n\nDesired output when applied:\n```python\n   category  amount  discount  final_price\n0        A   100.0      10.0          90.0\n1        B     NaN      20.0           NaN\n2        A   200.0       NaN         200.0\n3        C   150.0       5.0         142.5\n4        B   300.0       NaN         300.0\n```",
    "output": "\n```python\ndf['final_price'] = df.apply(lambda row: row['amount'] * (1 - (row['discount'] / 100)) if pd.notnull(row['amount']) else 0, axis=1).fillna(0)\n```"
  },
  {
    "input": "Problem:\n\nI am working with a dataset that contains time series data for electric consumption. I've already managed to load the data into a Pandas DataFrame, and I want to create new features by extracting the day of the week and whether it's a weekend or a weekday. I also want to drop any missing values afterward. Here's my current setup:\n\n```python\nimport pandas as pd\n\ndf = pd.read_csv('electric_consumption.csv', parse_dates=['timestamp'])\ndf['day_of_week'] = df['timestamp'].dt.dayofweek\n# Add code to create a 'is_weekend' column that assigns 1 for Saturday and Sunday,\n# and 0 for Monday through Friday.\n```\n\nA:\n\n```python\nis_weekend = ... # put solution in this variable\ndf = df.dropna() # drop any rows with missing values\n```",
    "output": "\n```python\ndf['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset containing customer information with various features such as age, income, and purchase history. Some of these features have missing values, and I want to preprocess them before feeding them into a machine learning model. Specifically, I need to apply the following transformations:\n\n1. Fill missing values in the 'age' column with the median age.\n2. Normalize the 'income' column using Min-Max scaling.\n3. One-hot encode the 'purchase_history' categorical column.\n\nHow can I achieve this using Pandas and Scikit-Learn?\n\nA:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = load_data()\n```\npipeline = ...  # put solution in this variable\nBEGIN SOLUTION\n```python\n\n",
    "output": "\n```python\npipeline = Pipeline(steps=[\n    ('preprocessor', ColumnTransformer(\n        transformers=[\n            ('age_imputer', 'passthrough', ['age']),\n            ('income_scaler', MinMaxScaler(), ['income']),\n            ('purchase_encoder', OneHotEncoder(), ['purchase_history'])\n        ],\n        remainder='drop'\n    )),\n    ('age_median_imputer', SimpleImputer(strategy='median'))\n])\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales information for various products across different departments. The DataFrame looks like this:\n\n```\n               department product  sales\n0            Electronics      TV   2000\n1            Electronics    Radio   1500\n2                 Furniture    Chair   1200\n3                 Furniture  Table   3000\n4              Clothing    Shirt   2500\n5              Clothing    Pants   1800\n```\n\nYou want to encode the 'department' and 'product' columns into categorical variables. After encoding, you wish to create a new DataFrame that shows total sales per department represented in one-hot encoded format with sales amounts normalized between 0 and 1. How can I obtain this transformation?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'department': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Clothing', 'Clothing'],\n    'product': ['TV', 'Radio', 'Chair', 'Table', 'Shirt', 'Pants'],\n    'sales': [2000, 1500, 1200, 3000, 2500, 1800]\n}\n\ndf = pd.DataFrame(data)\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\n# Encode 'department' and 'product' columns\ndf['department'] = df['department'].astype('category')\ndf['product'] = df['product'].astype('category')\n\n# Normalize sales\ndf['normalized_sales'] = df['sales'] / df['sales'].max()\n\n# Create one-hot encoded DataFrame for departments\none_hot = pd.get_dummies(df['department'])\n\n# Combine one-hot encoded DataFrame with normalized sales\nresult = one_hot.mul(df['normalized_sales'], axis=0)\n</code>"
  },
  {
    "input": "Problem:\nI'm using PyTorch 1.12.0. I have a tensor representing user-item ratings, where missing ratings are indicated by NaN values. I want to compute the cosine similarity between users' ratings, only considering the ratings that are not NaN. How can I achieve this?\n\nA:\n<code>\nimport torch\nimport numpy as np\n\nratings = torch.tensor([[5.0, 3.0, np.nan, 1.0],\n                        [4.0, np.nan, np.nan, 1.0],\n                        [1.0, 1.0, np.nan, 5.0],\n                        [np.nan, 1.0, 5.0, 4.0],\n                        [np.nan, 2.0, 1.0, 4.0]])\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = torch.nn.functional.cosine_similarity(ratings[~torch.isnan(ratings)].view(-1, ratings.size(1)), dim=0)"
  },
  {
    "input": "Problem:\nI have a DataFrame that contains sales data for a retail company over several months. The DataFrame looks like this:\n\n| Month | Sales | Returns |\n|-------|-------|---------|\n| Jan   | 200   | 10      |\n| Feb   | NaN   | 5       |\n| Mar   | 150   | NaN     |\n| Apr   | NaN   | NaN     |\n| May   | 300   | 15      |\n\nI want to calculate the net sales for each month, which is defined as the sales minus the returns. However, if either the sales or returns value is NaN, the net sales should also be NaN for that month. \n\nHere's the code I have so far:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May'],\n    'Sales': [200, np.nan, 150, np.nan, 300],\n    'Returns': [10, 5, np.nan, np.nan, 15]\n}\n\ndf = pd.DataFrame(data)\ndf.set_index('Month', inplace=True)\n```\n\nHow can I calculate the net sales for each month and add it as a new column called 'Net Sales'?\n\nA:\n<code>\ndf['Net Sales'] = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['Net Sales'] = df['Sales'] - df['Returns']\n</code>"
  },
  {
    "input": "Problem:\n\nYou are tasked with implementing a custom similarity calculation method using PyTorch for comparing two sets of embeddings. Each embedding is a tensor of shape (batch_size, embedding_dim). You need to compute the cosine similarity between two batches of embeddings, `embeddings_a` and `embeddings_b`. The result should be a tensor of shape (batch_size,) containing the cosine similarity scores for each corresponding pair of embeddings.\n\nGiven the following code snippet:\n\n```python\nimport torch\nfrom torch.nn.functional import cosine_similarity\n\nbatch_size = 5\nembedding_dim = 10\n\nembeddings_a = torch.rand((batch_size, embedding_dim))\nembeddings_b = torch.rand((batch_size, embedding_dim))\n```\n\nYou need to fill in the blank to compute the cosine similarities:\n\n```python\nsimilarities = ...\n```\n\nEnsure that your solution leverages PyTorch's capabilities effectively.",
    "output": "\n```python\nsimilarities = cosine_similarity(embeddings_a, embeddings_b)\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame representing sales data of a store over several months. The DataFrame has the following structure:\n\n```\n          Month  Sales\n0       January   100\n1      February   200\n2         March   NaN\n3         April   150\n4          May     300\n5         June    NaN\n6         July     250\n7       August    400\n8    September   200\n9      October   100\n10  November     NaN\n11  December    300\n```\n\nYou need to calculate a new column `Adjusted_Sales` where:\n- The value is the sales amount for that month.\n- If the sales amount is missing (NaN), it should be replaced by the average of the previous and the next month's sales values.\n- If both neighboring sales are NaN, the `Adjusted_Sales` should also be NaN.\n\nHere is the starting DataFrame:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Month': ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'],\n    'Sales': [100, 200, None, 150, 300, None, 250, 400, 200, 100, None, 300]\n})\n```\n\nFill in the blank with the appropriate pandas code to create the `Adjusted_Sales` column as described.\n\n```python\ndf['Adjusted_Sales'] = ...\n```",
    "output": "\n```python\ndf['Adjusted_Sales'] = df['Sales'].fillna((df['Sales'].shift() + df['Sales'].shift(-1)) / 2)\n```"
  },
  {
    "input": "Problem:\n\nI'm working with a PyTorch model that outputs a tensor of shape (batch_size, features). I want to find the cosine similarity between each pair of feature vectors within the same batch. However, I need to ignore any features that correspond to a masked value that I've defined. The mask is a boolean tensor of the same shape, where `True` indicates valid entries to be considered in the similarity computation, and `False` indicates masked values.\n\nGiven a tensor `X` and its corresponding boolean mask `mask`, how can I compute the cosine similarity for the valid entries while ignoring the masked values? The resulting tensor should have a shape of (batch_size, batch_size) representing the cosine similarity between valid feature vectors.\n\nExample input:\n```python\nX = torch.tensor([[1.0, 0.0, 2.0],\n                  [2.0, 3.0, 1.0],\n                  [0.0, 3.0, 0.0]])\nmask = torch.tensor([[True, False, True],\n                     [True, True, True],\n                     [False, True, False]])\n```\n\nA:\n\n<code>\nimport torch\n\nX = torch.tensor([[1.0, 0.0, 2.0],\n                  [2.0, 3.0, 1.0],\n                  [0.0, 3.0, 0.0]])\nmask = torch.tensor([[True, False, True],\n                     [True, True, True],\n                     [False, True, False]])\n</code>\nsimilarity = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nimport torch.nn.functional as F\n\ndef masked_cosine_similarity(X, mask):\n    # Apply the mask to X\n    masked_X = X * mask.float()\n    \n    # Normalize the masked feature vectors\n    norms = torch.norm(masked_X, dim=1, keepdim=True)\n    normalized_X = masked_X / norms\n    \n    # Compute cosine similarity\n    similarity = torch.mm(normalized_X, normalized_X.t())\n    \n    # Create a mask for the similarity matrix\n    similarity_mask = mask.float() @ mask.float().t()\n    \n    # Zero out the masked entries in the similarity matrix\n    similarity *= similarity_mask\n    \n    return similarity\n\nsimilarity = masked_cosine_similarity(X, mask)\n</code>"
  },
  {
    "input": "Problem:\nYou are working with a sales DataFrame containing daily transactions of products in different regions. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Date': pd.date_range(start='2023-01-01', periods=6),\n    'Region': ['East', 'West', 'East', 'West', 'East', 'West'],\n    'Sales': [200, np.nan, 150, 250, np.nan, 300],\n    'Returns': [np.nan, 50, 20, np.nan, 10, 70]\n})\n```\n\nYour goal is to calculate the 'Net Sales' for each region, considering that 'Net Sales' is defined as 'Sales' minus 'Returns'. If there are any NaN values in either 'Sales' or 'Returns', those rows should be excluded from the calculation of 'Net Sales'. The expected result for 'Net Sales' should be stored in a new column in the DataFrame.\n\nHow can I achieve this?\n\n```python\ndf = ... # put solution in this variable\n```",
    "output": "\n```python\ndf = df.dropna(subset=['Sales', 'Returns'])\ndf['Net Sales'] = df['Sales'] - df['Returns']\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing information about sales data, structured as follows:\n\n| product_id | region   | sales      |\n|------------|----------|------------|\n| 1          | North    | 100        |\n| 2          | South    | 150        |\n| 3          | East     | 90         |\n| 1          | West     | <NA>      |\n| 2          | North    | 200        |\n| 3          | South    | NaN        |\n| 1          | East     | 110        |\n| 2          | West     | 160        |\n| 3          | North    | <NA>      |\n\nYour goal is to calculate the average sales per product_id for each region and fill any missing values with the overall average sales across all products. The desired output is a DataFrame that shows the average sales with no missing values.\n\nThe current code you are working with is:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product_id': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North'],\n    'sales': [100, 150, 90, pd.NA, 200, None, 110, 160, pd.NA]\n})\n\nresult = df.groupby(['product_id', 'region']).mean()\n```\n\nHowever, you are encountering issues with handling null values and cannot directly perform operations that include those null entries. How can you modify your code to fill in the missing values correctly and achieve the desired result?\n\nA:\n<code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['sales'] = df['sales'].astype('float')  # Ensure sales are float for mean calculation\nresult = df.groupby(['product_id', 'region']).mean().fillna(df['sales'].mean())\n</code>\n"
  },
  {
    "input": "Problem:\nI have two pandas DataFrames representing sales data from two different regions, each containing a timestamp for when the data was recorded. The first DataFrame `sales_a` contains sales data from Region A, and the second DataFrame `sales_b` contains sales data from Region B. I want to merge these DataFrames such that for each timestamp in `sales_b`, I include the total sales from `sales_a` that occurred within a 5-minute window before that timestamp. If no sales are found in that time window, I want to record a `None` in the sales column.\n\nHere is the setup of the two DataFrames:\n\n```python\nimport pandas as pd\n\nsales_a = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:00:00', '2023-10-01 10:02:00', '2023-10-01 10:05:00', '2023-10-01 10:10:00'],\n    'TotalSales': [100, 200, 150, 300]\n})\n\nsales_b = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:01:00', '2023-10-01 10:06:00', '2023-10-01 10:11:00']\n})\n```\n\nNow, after merging, I want the resulting DataFrame to look like this:\n\n```\n+---------------------+------------+\n|      Timestamp      | TotalSales |\n+---------------------+------------+\n| 2023-10-01 10:01:00 |     100    |\n| 2023-10-01 10:06:00 |     200    |\n| 2023-10-01 10:11:00 |     150    |\n+---------------------+------------+\n```\n\nIf no sales were recorded in the 5-minute window, `TotalSales` should be `None`.\n\nTo achieve this, write the code that fills in the blank for the variable `result` below, ensuring that the merged DataFrame keeps the order of `sales_b` and aligns the sales data from `sales_a` correctly.\n\n```python\nimport pandas as pd\n\nsales_a = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:00:00', '2023-10-01 10:02:00', '2023-10-01 10:05:00', '2023-10-01 10:10:00'],\n    'TotalSales': [100, 200, 150, 300]\n})\n\nsales_b = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:01:00', '2023-10-01 10:06:00', '2023-10-01 10:11:00']\n})\n\n# Convert Timestamp columns to datetime\nsales_a['Timestamp'] = pd.to_datetime(sales_a['Timestamp'])\nsales_b['Timestamp'] = pd.to_datetime(sales_b['Timestamp'])\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = sales_b.copy()\nresult['TotalSales'] = result['Timestamp'].apply(\n    lambda x: sales_a[sales_a['Timestamp'] <= x].loc[\n        sales_a['Timestamp'] > (x - pd.Timedelta(minutes=5)), 'TotalSales'\n    ].sum() or None\n)\n```"
  },
  {
    "input": "Problem:\nYou are given two DataFrames representing product sales over different months:\n\ndf_sales:\n   product_id    sale_date  quantity\n0          1  2023-01-01       100\n1          2  2023-01-01       200\n2          1  2023-01-15       150\n3          3  2023-02-01       300\n\ndf_returns:\n   product_id     return_date  quantity\n0          1  2023-01-05          50\n1          2  2023-02-01         100\n2          1  2023-01-10          20\n3          3  2023-02-02          30\n\nYou want to create a new DataFrame that includes all products with their total sales quantity minus the total returns. The outcome should maintain the product_id, sale_date aligned to the earliest sale date for each product and return_date in an appropriate format (e.g., '01-Jan-2023').\n\nThe resulting DataFrame should have the following columns: product_id, sale_date, total_sales, return_date, total_returns.\n\nYour current attempt with `pd.concat()` does not yield the desired results because of mismatched dates. You need to first merge the DataFrames on product_id while summing the quantities appropriately.\n\nA:\n<code>\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3],\n    'sale_date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-15', '2023-02-01']),\n    'quantity': [100, 200, 150, 300]\n})\n\ndf_returns = pd.DataFrame({\n    'product_id': [1, 2, 1, 3],\n    'return_date': pd.to_datetime(['2023-01-05', '2023-02-01', '2023-01-10', '2023-02-02']),\n    'quantity': [50, 100, 20, 30]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df_sales.groupby('product_id').agg(\n    sale_date=('sale_date', 'min'),\n    total_sales=('quantity', 'sum')\n).reset_index()\n\nreturns_summary = df_returns.groupby('product_id').agg(\n    return_date=('return_date', 'max'),\n    total_returns=('quantity', 'sum')\n).reset_index()\n\nresult = pd.merge(result, returns_summary, on='product_id', how='left')\nresult['return_date'] = result['return_date'].dt.strftime('%d-%b-%Y')\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working on a machine learning model that requires computing similarity between feature vectors stored in a tensor. You have a 2D tensor `feature_vectors` of shape (8, 128), representing 8 samples with 128 features each. You need to compute the cosine similarity between each pair of samples (rows) and store the results in a new tensor `similarities` of shape (8, 8).\n\nYou have started with the following code:\n\n```python\nimport torch\n\nfeature_vectors = torch.randn(8, 128)\n```\n\nFill in the blank in the code below to compute the cosine similarity matrix:\n\n```python\nsimilarities = torch.nn.functional.cosine_similarity(______ , dim=1)\n```\n\nA:\n\n<code>\nimport torch\n\nfeature_vectors = torch.randn(8, 128)\nsimilarities = torch.nn.functional.cosine_similarity(______ , dim=1)\n</code>",
    "output": "\n<code>\nimport torch\n\nfeature_vectors = torch.randn(8, 128)\nsimilarities = torch.mm(feature_vectors, feature_vectors.t()) / (torch.norm(feature_vectors, dim=1).unsqueeze(1) * torch.norm(feature_vectors, dim=1).unsqueeze(0))\n</code>"
  },
  {
    "input": "Problem:\nI'm using PyTorch for a machine learning task. I have a predictions tensor that contains the softmax scores of shape [50, 5], which corresponds to 50 samples and 5 classes. Additionally, I have a tensor containing the actual class indices, `labels`, of shape [50]. My aim is to compute the loss for these predictions using PyTorch's cross-entropy loss function, which expects raw logits. To achieve this, I need to convert my softmax scores into logits, since cross-entropy cannot directly work with probabilities.\n\nGiven the softmax scores tensor looks like this:\n```python\nimport torch\n\nscores = torch.tensor([[0.1, 0.1, 0.6, 0.1, 0.1],\n                       [0.3, 0.3, 0.1, 0.1, 0.2],\n                       [0.5, 0.1, 0.3, 0.1, 0.0]])\n```\nThe `labels` tensor corresponds to the following actual classes:\n```python\nlabels = torch.tensor([2, 0, 1])\n```\nI want to revert the softmax scores back to logits by applying the appropriate transformation. How would I do that?\n\nA:\n<code>\nimport torch\n\nscores = torch.tensor([[0.1, 0.1, 0.6, 0.1, 0.1],\n                       [0.3, 0.3, 0.1, 0.1, 0.2],\n                       [0.5, 0.1, 0.3, 0.1, 0.0]])\nlabels = torch.tensor([2, 0, 1])\n\ndef f(scores=scores):\n    # return the solution in this function\n    # logits = f(scores)\n    ### BEGIN SOLUTION",
    "output": "\n<code>\n    logits = torch.log(scores) - torch.logsumexp(torch.log(scores), dim=1, keepdim=True)\n    return logits\n### END SOLUTION\n</code>"
  },
  {
    "input": "Problem:\nYou have two DataFrames, `df1` and `df2`, representing sales data for two different years:\n\n`df1`:\n```\n   Product  Sales  Year\n0  A       100    2020\n1  B       150    2020\n2  A       200    2021\n3  C       300    2021\n```\n\n`df2`:\n```\n   Product  Sales  Year\n0  B       250    2020\n1  C       350    2021\n2  D       400    2022\n3  A       100    2022\n```\n\nYou need to combine these DataFrames on the 'Product' column. During this merge, if a product exists in both DataFrames for the same year, the sales values from `df2` should overwrite those from `df1`. For products not present in `df1`, their sales should simply be added. The final DataFrame should be sorted first by year and then by product name.\n\nAfter merging, the DataFrame should also include a column named 'Duplicate' that indicates whether a product's record was updated from `df1` or is a new entry from `df2` (True if updated, False if new).\n\nYour current attempt with pd.merge results in duplicated entries in different rows instead of merging properly.\n\nHere's your starting code:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'Product': ['A', 'B', 'A', 'C'],\n    'Sales': [100, 150, 200, 300],\n    'Year': [2020, 2020, 2021, 2021]\n})\n\ndf2 = pd.DataFrame({\n    'Product': ['B', 'C', 'D', 'A'],\n    'Sales': [250, 350, 400, 100],\n    'Year': [2020, 2021, 2022, 2022]\n})\n```\n\nYou need to fill in the blank for the final DataFrame.\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = (pd.concat([df1, df2])\n          .groupby(['Product', 'Year'], as_index=False)\n          .agg({'Sales': 'sum'})\n          .merge(df2[['Product', 'Year']], on=['Product', 'Year'], how='left', indicator=True)\n          .assign(Duplicate=lambda x: x['_merge'] == 'both')\n          .drop(columns=['_merge'])\n          .sort_values(by=['Year', 'Product']))\n```"
  },
  {
    "input": "## Problem\nYou have a DataFrame that contains sales data for different products over several months. The DataFrame is structured as follows:\n\n```plaintext\n         Product  Jan  Feb  Mar  Apr  May  Jun\n0        A       10  15   0   0   25  30\n1        B       20   0   0   10  15  20\n2        C        0  25  30  0   5   0\n3        D       40   5   0   0  10  12\n```\n\nThe zeros represent months with no sales data, and you want to calculate the rolling sum of sales for each product, excluding any months with zero sales. For each product, if there are consecutive non-zero sales, they should be included in the sum, while skipping zero values.\n\nYour task is to write the code to generate a new DataFrame that replaces each product's sales for each month with this calculated rolling sum. Please create your DataFrame with the following code:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Product': ['A', 'B', 'C', 'D'],\n    'Jan': [10, 20, 0, 40],\n    'Feb': [15, 0, 25, 5],\n    'Mar': [0, 0, 30, 0],\n    'Apr': [0, 10, 0, 0],\n    'May': [25, 15, 5, 10],\n    'Jun': [30, 20, 0, 12]\n}\ndf = pd.DataFrame(data)\n```\n\nFill in the blank with the appropriate code to compute the rolling sum while excluding zeros:\n\n```python\nrolling_sum = df.set_index('Product').rolling(window=len(df.columns)-1, min_periods=1, axis=1).apply(lambda x: x[x != 0].sum(), raw=False)\n```\n\nMake sure your final DataFrame looks like this when printed:\n\n```plaintext\n         Product  Jan  Feb  Mar  Apr  May  Jun\n0        A       10  25   25  25  50  80\n1        B       20  20   20  30  45  65\n2        C        0  25  55  55  60  60\n3        D       40  45  45  45  55  67\n```\n\nYour task is to complete the code where indicated.",
    "output": "\n```python\nrolling_sum = df.set_index('Product').rolling(window=len(df.columns)-1, min_periods=1, axis=1).apply(lambda x: x[x != 0].sum(), raw=False).reset_index()\nrolling_sum.columns = df.columns\nprint(rolling_sum)\n```"
  },
  {
    "input": "Problem:\nContext\nI have two DataFrames that need to be combined while also ensuring data integrity and managing duplicates effectively. \n\nThe first DataFrame is as follows:\n```python\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n```\n\nAnd the second DataFrame is:\n```python\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\n\nI want to merge these two DataFrames on the `product_id` column, but I need to handle the case where `product_id` values in `df2` may result in duplicate rows in the final merged DataFrame, and keep only the first occurrence. The output should include all products and their corresponding stores, with an added column for availability.\n\nFinally, I would like to make sure the final DataFrame is sorted by `product_id`, in ascending order. The expected output should resemble:\n```python\n   product_id        products  price    store  availability\n0         101          Widget  20.99      NaN           NaN\n1         102          Gadget  25.50      NaN           NaN\n2         103          Doodad  15.75  Store A          True\n3         104  Thingamajig  22.10  Store B         False\n4         105           NaN     NaN  Store D          True\n```\n\nYou can find below the beginning of my implementation:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2.drop_duplicates(subset='product_id'), on='product_id', how='outer').sort_values(by='product_id').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nYou are analyzing a dataset of customer transactions for a retail store, which includes information such as `customer_id`, `purchase_amount`, and `purchase_date`. Your goal is to preprocess the data to eliminate outliers based on the purchase amounts using the Interquartile Range (IQR) method, and then group the remaining data by `customer_id` to calculate the total purchase amount per customer.\n\nGiven a pandas DataFrame named `transactions`, which contains the following columns:\n- `customer_id`: The ID of the customer (string).\n- `purchase_amount`: The amount spent in a transaction (float).\n- `purchase_date`: The date of the transaction (datetime).\n\nYou need to calculate the IQR, remove the outliers (purchase amounts less than Q1 - 1.5*IQR or greater than Q3 + 1.5*IQR), and return a new DataFrame containing `customer_id` and the total `purchase_amount` for each customer after the outlier removal.\n\nHere is how the initial code looks:\n\n```python\nimport pandas as pd\n\n# Sample data for transactions\ndata = {\n    'customer_id': ['C1', 'C2', 'C1', 'C3', 'C2', 'C1', 'C3'],\n    'purchase_amount': [150, 300, 5000, 250, 280, 800, 8000],\n    'purchase_date': pd.to_datetime(['2021-01-01', '2021-01-02', '2021-01-03',\n                                       '2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07'])\n}\ntransactions = pd.DataFrame(data)\n\n# Calculate Q1 and Q3\nQ1 = transactions['purchase_amount'].quantile(0.25)\nQ3 = transactions['purchase_amount'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Remove outliers\nfiltered_transactions = transactions[(transactions['purchase_amount'] >= Q1 - 1.5 * IQR) &\n                                     (transactions['purchase_amount'] <= Q3 + 1.5 * IQR)]\n\n# Group by customer and sum purchase amounts\nresult = ... # put solution in this variable\n```\n\nFill in the blank with the appropriate code to achieve the desired grouping and summation.",
    "output": "\n```python\nresult = filtered_transactions.groupby('customer_id', as_index=False)['purchase_amount'].sum()\n```"
  },
  {
    "input": "Problem:\nYou are working on a classification problem with a dataset that includes various features, some of which are categorical. You need to preprocess the dataset before applying machine learning algorithms. Specifically, you want to:\n\n1. Encode the categorical variables using one-hot encoding.\n2. Impute missing values in the numeric features with the median of each feature.\n3. Scale the numeric features to a range of [0, 1].\n\nGiven a pandas DataFrame `df` with both numerical and categorical features, fill in the following code to perform these preprocessing steps. \n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Sample data creation (omit this part in actual code)\n# df = pd.DataFrame({\n#     'Age': [25, 30, 35, None],\n#     'Salary': [50000, 60000, None, 80000],\n#     'City': ['New York', 'Los Angeles', None, 'Chicago']\n# })\n\n# Step 1: Encode categorical variables\nencoder = OneHotEncoder(sparse=False, drop='first')\nencoded_features = encoder.fit_transform(df[['City']])  # Adjust column name accordingly\n\n# Step 2: Impute missing values for numerical features\nimputer = SimpleImputer(strategy='median')\nnumeric_features = imputer.fit_transform(df[['Age', 'Salary']])  # Adjust column names accordingly\n\n# Step 3: Scale the numeric features\nscaler = MinMaxScaler()\nscaled_numeric_features = scaler.fit_transform(numeric_features)\n\n# Combine the processed features back into a single DataFrame\nprocessed_df = pd.DataFrame(np.hstack([scaled_numeric_features, encoded_features]))\n\n# What should be the final processed DataFrame?\nfinal_processed_df = ... # put solution in this variable\n```",
    "output": "\n```python\nfinal_processed_df = pd.DataFrame(np.hstack([scaled_numeric_features, encoded_features]), \n                                   columns=[*df[['Age', 'Salary']].columns, *encoder.get_feature_names_out(['City'])])\n```"
  },
  {
    "input": "Problem:\nI am working with two DataFrames, `sales_data` and `returns_data`, which contain sales and return information, respectively. Below are the structures of both DataFrames:\n\n`sales_data`\n```\n   OrderID     Date   Amount\n0        1 2023-01-01  150.00\n1        2 2023-01-01  200.00\n2        3 2023-01-02  250.00\n3        4 2023-01-03  300.00\n```\n\n`returns_data`\n```\n   OrderID     Date   Amount\n0        2 2023-01-01   50.00\n1        3 2023-01-02  100.00\n2        5 2023-01-04  200.00\n```\n\nI want to create a new DataFrame, `result`, where I merge `sales_data` with `returns_data` on `OrderID`. The `Amount` in the `returns_data` should subtract from the corresponding `Amount` in the `sales_data` where `OrderID` matches. If there is no matching `OrderID` in `returns_data`, the `Amount` from `sales_data` should remain unchanged. \n\nAdditionally, the `result` DataFrame should have the following structure, including an indicator column called `Returned` to specify if the item was returned:\n\n```\n   OrderID     Date   Amount  Returned\n0        1 2023-01-01  150.00     False\n1        2 2023-01-01  150.00      True\n2        3 2023-01-02  150.00      True\n3        4 2023-01-03  300.00     False\n```\n\nCan you help me fill in the code to perform this operation?\n\nA:\n<code>\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'OrderID': [1, 2, 3, 4],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03']),\n    'Amount': [150.00, 200.00, 250.00, 300.00]\n})\n\nreturns_data = pd.DataFrame({\n    'OrderID': [2, 3, 5],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-04']),\n    'Amount': [50.00, 100.00, 200.00]\n})\n\nresult = ... # fill in your solution here\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = sales_data.merge(returns_data, on='OrderID', how='left', suffixes=('', '_return'))\nresult['Amount'] = result['Amount'] - result['Amount_return'].fillna(0)\nresult['Returned'] = result['Amount_return'].notna()\nresult = result[['OrderID', 'Date', 'Amount', 'Returned']]\n</code>"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame representing sales data from a store with the following two columns: 'product' and 'sales'. Some sales values are missing (represented as pd.NA), and you want to replace these missing values with the average sales for each product category, but only for categories that have non-missing values. Next, create a new column 'adjusted_sales' which will be equal to sales if it is not missing, and adjusted to the computed average for those that are missing. The original dataframe is defined as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B'],\n    'sales': [200, 150, pd.NA, 300, 100, pd.NA, 500, 250]\n}\ndf = pd.DataFrame(data)\n```\n\nAfter performing this operation, your DataFrame should look something like this:\n\n```\n  product sales adjusted_sales\n0       A   200           200\n1       B   150           150\n2       A  <NA>           300\n3       C   300           300\n4       B   100           100\n5       C  <NA>           300\n6       A   500           500\n7       B   250           250\n```\n\nHow can you achieve this? \n\nPlease fill in the blank with the correct code to calculate and add the 'adjusted_sales' column:\n\n```python\ndf['adjusted_sales'] = ...\n```",
    "output": "\n```python\ndf['adjusted_sales'] = df['sales'].fillna(df.groupby('product')['sales'].transform('mean'))\n```"
  },
  {
    "input": "Problem:\nI have a dataset containing information about customer purchases in a retail store. Each purchase record includes the customer's ID, the product ID, the quantity bought, and the purchase amount. I want to aggregate that data to find the total amount spent by each customer. To prepare for this, I've imported the data into a pandas DataFrame. However, I also want to include only those customers who have spent more than a specified threshold.\n\nMy DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': [1, 2, 3, 1, 2, 3, 1],\n    'product_id': [101, 102, 103, 101, 102, 103, 104],\n    'quantity': [1, 2, 1, 1, 1, 1, 2],\n    'amount': [10.00, 20.00, 15.00, 5.00, 25.00, 15.00, 30.00]\n}\n\ndf = pd.DataFrame(data)\n```\n\nI\u2019ve tried to aggregate the total amount each customer has spent, but I\u2019m unsure how to filter out customers who spent less than a given threshold. Here\u2019s the code I have so far:\n\n```python\nthreshold = 50.00\ntotal_spent = df.groupby('customer_id')['amount'].sum()\n# filter to only include customers over the threshold\nfiltered_customers = total_spent[total_spent > threshold]\n```\n\nI'm looking to complete this code to obtain the resulting DataFrame `filtered_customers`.\n\nA:\n```python\nfiltered_customers = ... # put solution in this variable\n```",
    "output": "\n```python\nfiltered_customers = total_spent[total_spent > threshold].reset_index()\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame representing sales data for different products in different regions. The DataFrame looks like this:\n\n```python\nimport pandas as pd\ndata = {\n    'Product': ['A', 'B', 'A', 'B', 'C', 'C', 'A'],\n    'Region': ['North', 'South', 'East', 'West', 'North', 'South', 'West'],\n    'Sales': [100, None, 200, 300, None, 400, 150]\n}\ndf = pd.DataFrame(data)\n```\n\nI want to calculate the total sales for each product, but I need to replace any missing sales values (NaN) with the average sales for that product before computing the total. \n\nThe final result should be a new DataFrame that shows the total sales for each product:\n\n```python\n  Product  Total_Sales\n0       A         450.0\n1       B         300.0\n2       C         400.0\n```\n\nHow can I fill in the following line to achieve this?\n\n```python\nresult = df.groupby('Product').agg({'Sales': lambda x: x.fillna(x.mean()).sum()}).reset_index()\n```\nFill in the blank:\n```python\ndf['Sales'] = df['Sales'] ... # put solution in this variable\n```\nBEGIN SOLUTION\n<code>df['Sales'] = df['Sales'].fillna(df.groupby('Product')['Sales'].transform('mean'))</code>",
    "output": "\n<code>df['Sales'] = df['Sales'].fillna(df.groupby('Product')['Sales'].transform('mean'))</code>"
  },
  {
    "input": "## Problem\n\nI am working with a dataset that contains various features about houses, including their prices. I want to preprocess the data to better prepare it for a machine learning model. Specifically, I need to handle missing values in the 'price' column, standardize the 'size' column, and encode the categorical 'location' column.\n\nI have the following code snippet to start with:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Load dataset\ndf = pd.read_csv(\"houses.csv\")\n\n# Define columns\nnumeric_features = ['size']\ncategorical_features = ['location']\ntarget_column = 'price'\n\n# Imputer for numerical data\nnum_imputer = SimpleImputer(strategy='mean')\n# Encoder for categorical data\ncat_encoder = OneHotEncoder()\n\n# Create a preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_imputer, numeric_features),\n        ('cat', cat_encoder, categorical_features)\n    ]\n)\n\n# Apply preprocessing\nX = df[numeric_features + categorical_features]\ny = df[target_column]  # target variable\n\n# I need to fit and transform the features using preprocessor and also use StandardScaler on 'size'.\nX_transformed = ... # put solution in this variable\n\n```\nWhat code should I place where the comment indicates to preprocess `X` correctly?\n\nA:\n```python\nX_transformed = ... # put solution in this variable\n```",
    "output": "\n```python\nX_transformed = preprocessor.fit_transform(X)\nX_transformed[:, 0] = StandardScaler().fit_transform(X_transformed[:, 0].reshape(-1, 1)).flatten()\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a machine learning project and have a dataset contained within a pandas DataFrame that includes categorical features. Your task is to apply one-hot encoding to these categorical features while also ensuring to drop the original columns from the DataFrame afterwards. \n\nGiven the following DataFrame:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'city': ['New York', 'San Francisco', 'Los Angeles', 'New York', 'Los Angeles'],\n    'weather': ['Sunny', 'Foggy', 'Sunny', 'Rainy', 'Foggy'],\n    'temperature': [85, 72, 78, 80, 75]\n})\n```\n\nYou want to transform the 'city' and 'weather' categorical fields into one-hot encoded format and store the result in a variable.\n\nFill in the code to achieve this transformation:\n\n```python\ndf_encoded = ... # put solution in this variable\n```",
    "output": "\n```python\ndf_encoded = pd.get_dummies(df, columns=['city', 'weather'], drop_first=True)\n```"
  },
  {
    "input": "Problem:  \nYou are tasked with comparing two sets of multi-dimensional tensor embeddings to identify which embeddings are similar based on a cosine similarity threshold. You have two sets of embeddings, `embeddings_a` and `embeddings_b`, both represented as 2D tensors (where each row represents an embedding). Your goal is to create a boolean mask indicating which rows in `embeddings_a` are similar to any row in `embeddings_b` based on a cosine similarity above a specified threshold (let's say `0.8`). \n\nHere's the initial setup for your code:\n\n```python\nimport torch\n\nembeddings_a = torch.rand(5, 128)  # 5 embeddings of dimension 128\nembeddings_b = torch.rand(7, 128)  # 7 embeddings of dimension 128\nthreshold = 0.8\n```\n\nYou need to fill in the blank to compute the boolean mask indicating which embeddings from `embeddings_a` are similar to those in `embeddings_b`:\n\n```python\ncosine_similarity = torch.nn.functional.cosine_similarity(embeddings_a.unsqueeze(1), embeddings_b.unsqueeze(0), dim=2)\nmask = (cosine_similarity > threshold)\n```\n\nWhat should you assign to `mask` so that it indicates whether each embedding in `embeddings_a` is similar to at least one embedding in `embeddings_b`? \n\nA:\n```python\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nmask = mask.any(dim=1)\n```"
  }
]