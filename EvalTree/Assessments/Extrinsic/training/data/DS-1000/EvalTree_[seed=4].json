[
  {
    "input": "### Problem:\nYou have a dataset containing information about various products and their sales in different regions. The data is presented in a DataFrame with a multi-index consisting of ('Region', 'Product'). Here's a sample of the DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('North', 'A'): [100, 150, 200],\n    ('North', 'B'): [80, 60, 90],\n    ('South', 'A'): [110, 130, 120],\n    ('South', 'B'): [70, 100, 80],\n}\nindex = pd.date_range(start='2023-01-01', periods=3, freq='D')\nsales_df = pd.DataFrame(data, index=index)\n```\n\nI want to normalize the sales values for each product across the regions, where the normalized value is calculated as `(value - min) / (max - min)`. \n\nIn order to do this, you'll need to reshape the DataFrame to handle the multi-index appropriately, perform the normalization, and then reshape it back to the original multi-index format.\n\nPlease fill in the following line to perform this operation and obtain the resulting normalized DataFrame:\n\n```python\nnormalized_sales = ...  # put solution in this variable\n```",
    "output": "\n```python\nnormalized_sales = (sales_df - sales_df.min()) / (sales_df.max() - sales_df.min())\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames representing sales data from two different regions. The first DataFrame, `sales_region1`, contains sales figures from Region 1, and the second DataFrame, `sales_region2`, contains sales figures from Region 2. Each DataFrame has an 'id' column and a 'sales' column. I want to combine these DataFrames into a single DataFrame that contains the total sales for each id, ensuring that if an id appears in both DataFrames, the sales figures are summed. If an id appears in one DataFrame only, it should still be included. The 'id' should be sorted in ascending order.\n\nHere are the DataFrames:\n```python\nimport pandas as pd\n\nsales_region1 = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'sales': [100, 200, 300, 400]\n})\n\nsales_region2 = pd.DataFrame({\n    'id': [2, 3, 5],\n    'sales': [150, 250, 100]\n})\n```\n\nI'd like to merge these DataFrames while summing the sales, and output the result sorted by 'id'. What is the code to achieve this?\n\nA:\n```python\nimport pandas as pd\n\nsales_region1 = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'sales': [100, 200, 300, 400]\n})\n\nsales_region2 = pd.DataFrame({\n    'id': [2, 3, 5],\n    'sales': [150, 250, 100]\n})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.concat([sales_region1, sales_region2]).groupby('id', as_index=False).sum().sort_values('id')\n```"
  },
  {
    "input": "Problem:\nI have a dataset containing sales transactions that includes information about the product, sales amount, and region. Some of the sales amounts are missing (NaN). Here's a sample DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'product': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B'],\n    'sales_amount': [200, 150, np.nan, 300, np.nan, 400, 250, 100],\n    'region': ['North', 'South', 'North', 'East', 'South', 'East', 'North', 'South']\n}\ndf = pd.DataFrame(data)\n```\n\nMy task is to calculate the average sales amount per product and region. However, if any of the sales amounts in a group are NaN, the average for that group should also be NaN. \n\nI want to use the `groupby` function combined with a method to enforce this condition, and then create a new DataFrame that shows the average sales amount.\n\nPlease fill in the blank for the code snippet below to achieve this:\n\n```python\nresult = df.groupby(['product', 'region'])['sales_amount']._________\n```\nDesired output when printed:\n```\nproduct  region\nA       East      NaN\n        North     225.0\n        South      200.0\nB       East      NaN\n        North      NaN\n        South     125.0\nC       East      300.0\n        North      NaN\n        South      NaN\nName: sales_amount, dtype: float64\n```",
    "output": "\n```python\nresult = df.groupby(['product', 'region'])['sales_amount'].mean(skipna=False)\n```"
  },
  {
    "input": "### Problem:\n\nYou are tasked with building a predictive model to analyze credit card transaction data. The dataset contains various features including transaction amount, time of transaction, merchant category, and a binary label indicating if the transaction was fraudulent. \n\nTo preprocess the data, you need to handle missing values, normalize numerical features, and one-hot encode categorical features before fitting a machine learning model.\n\nGiven the following example DataFrame `df`, implement a function `preprocess_data` that does the following:\n\n1. Fills any missing values in the 'transaction_amount' column with the median value of the column.\n2. Normalizes the 'transaction_amount' column to a scale of 0 to 1.\n3. One-hot encodes the 'merchant_category' column.\n\nHere is the example DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'transaction_amount': [20, 55, None, 30, 75],\n    'merchant_category': ['grocery', 'electronics', 'grocery', 'electronics', 'clothing'],\n    'is_fraud': [0, 1, 0, 1, 0]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYour function definition should look like this:\n\n```python\ndef preprocess_data(df):\n    # Your preprocessing code goes here\n    ...\n```\n\nAfter implementing the preprocessing steps, the function should return a new DataFrame, ready for model training.",
    "output": "\n```python\ndef preprocess_data(df):\n    df['transaction_amount'].fillna(df['transaction_amount'].median(), inplace=True)\n    df['transaction_amount'] = (df['transaction_amount'] - df['transaction_amount'].min()) / (df['transaction_amount'].max() - df['transaction_amount'].min())\n    df = pd.get_dummies(df, columns=['merchant_category'], drop_first=True)\n    return df\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset of housing prices with features such as square footage, number of bedrooms, and age of the house. Before fitting a linear regression model, you need to handle missing values by filling them with the median of each respective feature. Also, you must scale the features to a standard normal distribution (mean=0, standard deviation=1). Given the following DataFrame `df`, which already contains missing values:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndata = {\n    'square_footage': [1500, 1600, None, 1800, 2000, None],\n    'num_bedrooms': [3, 3, 4, None, 4, 5],\n    'age': [10, 15, 20, 25, None, 30]\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to fill in the missing values and scale the features. Complete the following code snippet to achieve that:\n\n```python\n# Fill missing values with the median of each column\ndf.fillna(df.median(), inplace=True)\n\n# Scale the features\nscaler = StandardScaler()\nscaled_features = __________\n```\n\nA: \n<code>\nscaled_features = scaler.fit_transform(df)\n</code>",
    "output": "\n<code>\nscaled_features = scaler.fit_transform(df)\n</code>"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame that contains the scores of different students across multiple subjects, structured as follows:\n\n```python\nimport pandas as pd\ndf = pd.DataFrame({\n    'Student': ['Alice', 'Bob', 'Charlie'],\n    'Math': [88, 92, 75],\n    'Science': [90, 95, 80],\n    'English': [85, 89, 70]\n})\n```\n\nI want to normalize the scores for each subject to be between 0 and 1, and I would like the resulting DataFrame to maintain the same structure, with scores replaced by their normalized values.\n\nCan you provide the correct code that fills in the blank below to accomplish this normalization using the Min-Max Scaling technique? \n\nA:\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnormalized_scores = ... # put solution in this variable\n```",
    "output": "\n```python\nnormalized_scores = scaler.fit_transform(df[['Math', 'Science', 'English']])\ndf[['Math', 'Science', 'English']] = normalized_scores\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for various products over a week, including some records with missing values. The structure of the DataFrame is as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'B', 'C', 'A', 'C', 'B', 'C', 'A'],\n    'Sales': [150, np.nan, 200, 250, np.nan, 300, 350, np.nan, 400, 450],\n    'Date': pd.date_range(start='2023-01-01', periods=10)\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to compute the total sales for each product, but you need to ensure that if any sales figures for a product are missing (NaN), the total sales for that product also becomes NaN. You start with the following code for summing the sales:\n\n```python\ntotal_sales = df.groupby('Product')['Sales'].sum()\n```\n\nHowever, this does not yield the desired result with NaN handling. How can you modify your code to get the correct total sales that meet this requirement?\n\nHere is what I want to achieve:\n\n```\nProduct\nA      NaN\nB      NaN\nC      NaN\nName: Sales, dtype: float64\n```\n\nFill in the blank with the correct code:\n```python\ntotal_sales = ...\n```",
    "output": "\n```python\ntotal_sales = df.groupby('Product')['Sales'].apply(lambda x: x.sum() if x.isnull().sum() == 0 else np.nan)\n```"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame representing sales data from a store with the following two columns: 'product' and 'sales'. Some sales values are missing (represented as pd.NA), and you want to replace these missing values with the average sales for each product category, but only for categories that have non-missing values. Next, create a new column 'adjusted_sales' which will be equal to sales if it is not missing, and adjusted to the computed average for those that are missing. The original dataframe is defined as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B'],\n    'sales': [200, 150, pd.NA, 300, 100, pd.NA, 500, 250]\n}\ndf = pd.DataFrame(data)\n```\n\nAfter performing this operation, your DataFrame should look something like this:\n\n```\n  product sales adjusted_sales\n0       A   200           200\n1       B   150           150\n2       A  <NA>           300\n3       C   300           300\n4       B   100           100\n5       C  <NA>           300\n6       A   500           500\n7       B   250           250\n```\n\nHow can you achieve this? \n\nPlease fill in the blank with the correct code to calculate and add the 'adjusted_sales' column:\n\n```python\ndf['adjusted_sales'] = ...\n```",
    "output": "\n```python\ndf['adjusted_sales'] = df['sales'].fillna(df.groupby('product')['sales'].transform('mean'))\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame that contains information about sales transactions as shown below:\n\n| Transaction_ID | Product   | Quantity | Price | Discount |\n|----------------|-----------|----------|-------|----------|\n| 1              | A         | 2        | 30    | NaN      |\n| 2              | B         | 1        | 20    | 5        |\n| 3              | A         | NaN      | 30    | 3        |\n| 4              | C         | 1        | 50    | 10       |\n| 5              | B         | 3        | NaN   | NaN      |\n| 6              | A         | 4        | 30    | 0        |\n\nI need to calculate the total sales for each product, where total sales are defined as `(Quantity * Price) - Discount`. If `Quantity` or `Price` is NaN for any transaction, it should be ignored in the total calculation. Any `Discount` value that is NaN should be treated as 0 for the calculations. The expected output should show the total sales per product as follows:\n\n| Product | Total_Sales |\n|---------|-------------|\n| A       | ?           |\n| B       | ?           |\n| C       | ?           |\n\nA:\n<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Transaction_ID': [1, 2, 3, 4, 5, 6],\n    'Product': ['A', 'B', 'A', 'C', 'B', 'A'],\n    'Quantity': [2, 1, None, 1, 3, 4],\n    'Price': [30, 20, 30, 50, None, 30],\n    'Discount': [None, 5, 3, 10, None, 0]\n})\n</code>\ntotal_sales = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\ndf['Discount'] = df['Discount'].fillna(0)\ntotal_sales = df.dropna(subset=['Quantity', 'Price']).groupby('Product').apply(lambda x: (x['Quantity'] * x['Price'] - x['Discount']).sum()).reset_index(name='Total_Sales')\n</code>"
  },
  {
    "input": "Problem:\n\nI am working on a healthcare dataset where I want to analyze the relationship between multiple features and the target variable, which is a binary classification indicating whether a patient has a certain condition (1) or not (0). Before using any machine learning model, I want to preprocess the data by normalizing the numeric features to a range between 0 and 1 and applying one-hot encoding to the categorical features. I have a DataFrame called `df` with several numeric columns and one categorical column named 'condition_type'.\n\nI already imported the necessary libraries, but I'm unsure how to do the preprocessing steps efficiently. After normalization and one-hot encoding, I want to get the preprocessed DataFrame.\n\nHere\u2019s how I started:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndf = load_data()  # Assuming this function loads the DataFrame\nnumeric_features = ['age', 'blood_pressure', 'cholesterol']  # Example numeric features\ncategorical_features = ['condition_type']  # Categorical feature\n\n# You need to create a transformation pipeline that handles both numeric and categorical preprocessing.\n```\n\ntransformed_df = ...  # put solution in this variable  \nBEGIN SOLUTION\n<code>",
    "output": "\n```python\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', MinMaxScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ]\n)\n\ntransformed_df = pd.DataFrame(preprocessor.fit_transform(df))\n```"
  },
  {
    "input": "Problem:\nI'm using PyTorch for a machine learning task. I have a predictions tensor that contains the softmax scores of shape [50, 5], which corresponds to 50 samples and 5 classes. Additionally, I have a tensor containing the actual class indices, `labels`, of shape [50]. My aim is to compute the loss for these predictions using PyTorch's cross-entropy loss function, which expects raw logits. To achieve this, I need to convert my softmax scores into logits, since cross-entropy cannot directly work with probabilities.\n\nGiven the softmax scores tensor looks like this:\n```python\nimport torch\n\nscores = torch.tensor([[0.1, 0.1, 0.6, 0.1, 0.1],\n                       [0.3, 0.3, 0.1, 0.1, 0.2],\n                       [0.5, 0.1, 0.3, 0.1, 0.0]])\n```\nThe `labels` tensor corresponds to the following actual classes:\n```python\nlabels = torch.tensor([2, 0, 1])\n```\nI want to revert the softmax scores back to logits by applying the appropriate transformation. How would I do that?\n\nA:\n<code>\nimport torch\n\nscores = torch.tensor([[0.1, 0.1, 0.6, 0.1, 0.1],\n                       [0.3, 0.3, 0.1, 0.1, 0.2],\n                       [0.5, 0.1, 0.3, 0.1, 0.0]])\nlabels = torch.tensor([2, 0, 1])\n\ndef f(scores=scores):\n    # return the solution in this function\n    # logits = f(scores)\n    ### BEGIN SOLUTION",
    "output": "\n<code>\n    logits = torch.log(scores) - torch.logsumexp(torch.log(scores), dim=1, keepdim=True)\n    return logits\n### END SOLUTION\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames E and F as follows:\nE\n    Timestamp       Temperature\n0  2023-01-01 10:00:00       22.5\n1  2023-01-01 11:00:00       23.0\n2  2023-01-01 12:00:00       21.5\n\nF\n    Timestamp       Temperature\n0  2023-01-01 10:30:00       24.0\n1  2023-01-01 11:30:00       23.5\n\nI need to combine these two DataFrames based on the 'Timestamp', ensuring that if there are duplicate timestamps, the value from DataFrame F replaces the value from DataFrame E. Additionally, I want to add a new column 'Source' indicating whether the temperature came from DataFrame E or F. If the timestamp is original to DataFrame E, it should read \"E\", otherwise \"F\". The resulting DataFrame should be sorted by 'Timestamp'.\n\nOutput\n    Timestamp          Temperature Source\n0  2023-01-01 10:00:00       22.5      E\n1  2023-01-01 10:30:00       24.0      F\n2  2023-01-01 11:00:00       23.0      E\n3  2023-01-01 11:30:00       23.5      F\n4  2023-01-01 12:00:00       21.5      E\n\nA:\n<code>\nimport pandas as pd\n\n\nE = pd.DataFrame({\n    \"Timestamp\": pd.to_datetime([\"2023-01-01 10:00:00\", \"2023-01-01 11:00:00\", \"2023-01-01 12:00:00\"]),\n    \"Temperature\": [22.5, 23.0, 21.5]\n})\nF = pd.DataFrame({\n    \"Timestamp\": pd.to_datetime([\"2023-01-01 10:30:00\", \"2023-01-01 11:30:00\"]),\n    \"Temperature\": [24.0, 23.5]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.concat([E.assign(Source='E'), F.assign(Source='F')]).drop_duplicates(subset='Timestamp', keep='last').sort_values(by='Timestamp').reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data from a fictional online store as follows:\n\n| Category | Product | Sales  | Returns | Profit |\n|----------|---------|--------|---------|--------|\n| A        | P1     | 100    | 5       | 70     |\n| A        | P2     | 150    | 10      | 80     |\n| B        | P3     | 200    | 0       | 120    |\n| B        | P4     | 50     | 5       | 20     |\n| C        | P5     | 300    | 15      | 200    |\n| A        | P6     | NaN    | 5       | NaN    |\n| C        | P7     | 400    | NaN     | 300    |\n\nYou want to compute the following for each category:\n1. The total sales, ignoring NaN values.\n2. The total profit, replacing any NaN values with 0.\n3. The returns as a percentage of total sales, with NaN values handled appropriately.\n\nThe expected output should look like this:\n\n| Category | Total_Sales | Total_Profit | Returns_Percent |\n|----------|-------------|---------------|------------------|\n| A        | 250         | 150           | 4.0              |\n| B        | 250         | 140           | 2.0              |\n| C        | 700         | 500           | 2.14             |\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Category': ['A', 'A', 'B', 'B', 'C', 'A', 'C'],\n    'Product': ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7'],\n    'Sales': [100, 150, 200, 50, 300, np.nan, 400],\n    'Returns': [5, 10, 0, 5, 15, 5, np.nan],\n    'Profit': [70, 80, 120, 20, 200, np.nan, 300]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df.groupby('Category').agg(\n    Total_Sales=('Sales', 'sum'),\n    Total_Profit=('Profit', lambda x: x.fillna(0).sum()),\n    Returns_Percent=('Returns', lambda x: (x.sum() / df['Sales'].sum()) * 100 if df['Sales'].sum() > 0 else 0)\n).reset_index()"
  },
  {
    "input": "Problem:\nYou are working with a dataset containing sales data for multiple products across different stores, structured as follows:\n\n```python\ndata = {\n    ('Store1', 'ProductA'): [100, 150, 200],\n    ('Store1', 'ProductB'): [200, 250, 300],\n    ('Store2', 'ProductA'): [150, 250, 350],\n    ('Store2', 'ProductB'): [300, 400, 500],\n}\ndf = pd.DataFrame(data, index=[2021, 2022, 2023])\n```\n\nYou must normalize the values in the DataFrame on a per-store basis, transforming them into a range between 0 and 1. Your goal is to output a new DataFrame with normalized values, keeping the multi-index structure intact. \n\nHow would you implement this normalization step?\n\nPlease provide your solution in the variable `normalized_df`.",
    "output": "\n```python\n<code>\nnormalized_df = df.groupby(level=0).apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing embeddings represented as a 3D tensor, which denotes a batch of sequences. Your task is to perform a similarity computation between two sets of embeddings via cosine similarity. The embeddings of two sequences are given in the following tensors:\n\n```python\nimport torch\n\nembeddings_a = torch.randn(50, 30, 128)  # 50 sequences, 30 time steps, 128 features\nembeddings_b = torch.randn(50, 30, 128)  # Another set of embeddings\n```\n\nYou need to compute the cosine similarity between the corresponding sequences from `embeddings_a` and `embeddings_b` across the last dimension (feature dimension). The result should be a tensor of shape `[50, 30]` where each element at position `(i, j)` represents the cosine similarity between the embeddings of sequence `i` at time step `j` from both tensors.\n\nYou can use the following commands to calculate the similarities. Fill in the blank for the `similarity` variable.\n\nA:\n\n```python\nimport torch\n\nembeddings_a = torch.randn(50, 30, 128)  # 50 sequences, 30 time steps, 128 features\nembeddings_b = torch.randn(50, 30, 128)  # Another set of embeddings\n\n# Normalize the embeddings\nnorm_a = embeddings_a / embeddings_a.norm(dim=-1, keepdim=True)\nnorm_b = embeddings_b / embeddings_b.norm(dim=-1, keepdim=True)\n\nsimilarity = ...  # put solution in this variable\n```\n\nBEGIN SOLUTION\n```python\nsimilarity = torch.bmm(norm_a, norm_b.transpose(1, 2)).squeeze(-1)\n```",
    "output": "\n```python\nsimilarity = torch.bmm(norm_a, norm_b.transpose(1, 2)).squeeze(-1)\n```"
  },
  {
    "input": "Problem:\nI have a PyTorch tensor that represents a batch of user ratings for different movies. The shape of the tensor is (number_of_users, number_of_movies). I need to implement a function that computes the cosine similarity between two specific users based on their ratings. The output should be a single float value representing their cosine similarity.\n\nHere is a portion of my code:\n\n```python\nimport torch\n\nratings = torch.tensor([[5.0, 0.0, 3.0], \n                        [4.0, 1.0, 0.0], \n                        [0.0, 0.0, 2.0]])\n\nuser1_index = 0  # First user's index\nuser2_index = 1  # Second user's index\n```\n\nHow can I calculate the cosine similarity between user 1 and user 2 using this ratings tensor? Please fill in the blank below.\n\nA:\n<code>\ndef cosine_similarity(ratings, user1_index, user2_index):\n    user1_ratings = ratings[user1_index, :]\n    user2_ratings = ratings[user2_index, :]\n    \n    # Calculate cosine similarity here\n</code>\nsimilarity = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>\nsimilarity = torch.nn.functional.cosine_similarity(user1_ratings.unsqueeze(0), user2_ratings.unsqueeze(0)).item()\n</code>",
    "output": "\n<code>\nsimilarity = torch.nn.functional.cosine_similarity(user1_ratings.unsqueeze(0), user2_ratings.unsqueeze(0)).item()\n</code>"
  },
  {
    "input": "Problem:\nYou have two pandas DataFrames representing sales transactions from two different months. The first DataFrame, `df_january`, includes sales data for January and looks like this:\n\n```python\n   transaction_id  product_id  quantity  sale_date   revenue\n0              1          101        2  2023-01-01       20\n1              2          102        1  2023-01-05       30\n2              3          103        5  2023-01-10      150\n3              4          101        3  2023-01-15       60\n4              5          104        1  2023-01-20       25\n```\n\nThe second DataFrame, `df_february`, gives data for February and looks like this:\n\n```python\n   transaction_id  product_id  quantity  sale_date   revenue\n0              6          103        4  2023-02-05      120\n1              7          104        2  2023-02-10       50\n2              8          105        1  2023-02-20       15\n3              9          101        1  2023-02-25       10\n4             10          102        2  2023-02-28       60\n```\n\nYou want to merge these two DataFrames such that:\n- All transactions from both months display together.\n- For the merged DataFrame, replace any missing `quantity`, `sale_date`, or `revenue` in February with the values from January, using the `product_id` as the key to look up these values.\n- Finally, ensure the DataFrame is sorted by `sale_date` and retains all transactions.\n\nHowever, you're noticing that there may be duplicate entries for `product_id` within one month when compared to the other month, and you want to drop the duplicates from the merged DataFrame based on the `transaction_id`, keeping the first occurrence.\n\nGiven the two DataFrames, how would you implement the merge and relevant processing?\n\nA:\n<code>\nimport pandas as pd\n\ndf_january = pd.DataFrame({\n    'transaction_id': [1, 2, 3, 4, 5],\n    'product_id': [101, 102, 103, 101, 104],\n    'quantity': [2, 1, 5, 3, 1],\n    'sale_date': pd.to_datetime(['2023-01-01', '2023-01-05', '2023-01-10', '2023-01-15', '2023-01-20']),\n    'revenue': [20, 30, 150, 60, 25]\n})\n\ndf_february = pd.DataFrame({\n    'transaction_id': [6, 7, 8, 9, 10],\n    'product_id': [103, 104, 105, 101, 102],\n    'quantity': [4, 2, 1, 1, 2],\n    'sale_date': pd.to_datetime(['2023-02-05', '2023-02-10', '2023-02-20', '2023-02-25', '2023-02-28']),\n    'revenue': [120, 50, 15, 10, 60]\n})\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = pd.merge(df_february, df_january, on='product_id', how='outer', suffixes=('_feb', '_jan'))\nresult['quantity'] = result['quantity_feb'].combine_first(result['quantity_jan'])\nresult['sale_date'] = result['sale_date_feb'].combine_first(result['sale_date_jan'])\nresult['revenue'] = result['revenue_feb'].combine_first(result['revenue_jan'])\nresult = result[['transaction_id_feb', 'product_id', 'quantity', 'sale_date', 'revenue']].rename(columns={'transaction_id_feb': 'transaction_id'})\nresult = result.drop_duplicates(subset='transaction_id').sort_values(by='sale_date').reset_index(drop=True)\n</code>"
  },
  {
    "input": "### Problem:\nYou have two DataFrames representing sales data for different products. The first DataFrame (`df1`) contains historical sales data, while the second DataFrame (`df2`) contains updated sales information for the same products, but with some new sales entries and updated quantities. \n\n**df1:**\n```\n+---------+-----------+----------+-----------+\n| Product | SaleDate  | Quantity | Sales     |\n+---------+-----------+----------+-----------+\n| A       | 2022-01-01| 10       | 100       |\n| B       | 2022-01-05| 5        | 50        |\n| A       | 2022-01-10| 8        | 80        |\n| C       | 2022-01-15| 12       | 120       |\n+---------+-----------+----------+-----------+\n```\n\n**df2:**\n```\n+---------+-----------+----------+-----------+\n| Product | SaleDate  | Quantity | Sales     |\n+---------+-----------+----------+-----------+\n| A       | 2022-01-01| 5        | 50        |\n| A       | 2022-01-10| 10       | 100       |\n| D       | 2022-01-20| 7        | 70        |\n| B       | 2022-01-05| 6        | 60        |\n+---------+-----------+----------+-----------+\n```\n\nYour goal is to merge `df1` and `df2` into a single DataFrame such that:\n- For products that exist in both DataFrames, the `Quantity` and `Sales` should be summed up.\n- For products that exist only in one of the DataFrames, they should be included as is.\n- The resulting DataFrame should be sorted by `SaleDate` in ascending order.\n\nThe expected resultant format should look like this after the merge:\n```\n+---------+-----------+----------+-----------+\n| Product | SaleDate  | Quantity | Sales     |\n+---------+-----------+----------+-----------+\n| A       | 2022-01-01| 15       | 150       |\n| B       | 2022-01-05| 11       | 110       |\n| A       | 2022-01-10| 18       | 180       |\n| C       | 2022-01-15| 12       | 120       |\n| D       | 2022-01-20| 7        | 70        |\n+---------+-----------+----------+-----------+\n```\n\nHere's a partial implementation:\n```python\nimport pandas as pd\n\n# Creating the DataFrames df1 and df2\ndf1 = pd.DataFrame({\n    'Product': ['A', 'B', 'A', 'C'],\n    'SaleDate': ['2022-01-01', '2022-01-05', '2022-01-10', '2022-01-15'],\n    'Quantity': [10, 5, 8, 12],\n    'Sales': [100, 50, 80, 120]\n})\n\ndf2 = pd.DataFrame({\n    'Product': ['A', 'A', 'D', 'B'],\n    'SaleDate': ['2022-01-01', '2022-01-10', '2022-01-20', '2022-01-05'],\n    'Quantity': [5, 10, 7, 6],\n    'Sales': [50, 100, 70, 60]\n})\n\n# Implementation should go here\nresult = ...\n```",
    "output": "\n```python\nresult = pd.concat([df1, df2]).groupby(['Product', 'SaleDate'], as_index=False).sum().sort_values('SaleDate')\n```"
  },
  {
    "input": "Problem:\nYou are analyzing a dataset of users' purchase transactions stored in a Pandas DataFrame with the following structure:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('user_id', 'user'): [1, 1, 2, 2, 3],\n    ('transaction', 'transaction_id'): [101, 102, 103, 104, 105],\n    ('purchase', 'item'): ['A', 'B', 'A', 'C', 'B'],\n    ('purchase', 'amount'): [25, 30, 15, 45, 20]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to calculate the total amount spent by each user per item category and normalize the resulting amounts by the maximum amount for each user. Your goal is to fill in the following code that will provide a normalized DataFrame with a multi-index, where the outer index is `user_id` and the inner index is the item purchased. The resulting DataFrame should look something like:\n\n```\n              amount\nuser_id item        \n1       A     1.00\n        B     1.20\n2       A     0.33\n        C     1.00\n3       B     1.00\n```\n\nTo achieve this, you must calculate the total purchase amounts and normalize them. Fill in the solution below to get the desired output:\n\n```python\nresult = ...\n```",
    "output": "\n```python\nresult = df.groupby(('user_id', 'user', 'purchase', 'item')).sum().reset_index()\nresult = result.set_index((('user_id', 'user'), ('purchase', 'item')))\nresult['amount'] = result['purchase', 'amount'] / result.groupby(level=0)['purchase', 'amount'].transform('max')\nresult = result[['amount']]\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for a retail store, which includes columns for 'Product', 'Category', 'Sales', and 'Discount'. The 'Discount' column may contain null values. You want to compute a new column 'Net_Sales', which represents the total sales after applying the discount. If the 'Discount' is null, assume it to be 0. \n\nHere is the DataFrame:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Product': ['A', 'B', 'C', 'D'],\n    'Category': ['Electronics', 'Furniture', 'Electronics', 'Furniture'],\n    'Sales': [300, 150, 500, 200],\n    'Discount': [20, np.nan, 10, 30]\n})\n```\n\nYour current attempt is as follows:\n```python\ndf['Net_Sales'] = df['Sales'] - df['Discount']\n```\n\nHowever, this results in an error due to the null values in the 'Discount' column.\n\nHow can you adjust your code to calculate 'Net_Sales' correctly, ensuring that null discounts are treated as no discount applied?\n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Product': ['A', 'B', 'C', 'D'],\n    'Category': ['Electronics', 'Furniture', 'Electronics', 'Furniture'],\n    'Sales': [300, 150, 500, 200],\n    'Discount': [20, np.nan, 10, 30]\n})\n```\ndf['Net_Sales'] = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf['Net_Sales'] = df['Sales'] - df['Discount'].fillna(0)\n```"
  },
  {
    "input": "Problem:\nYou are provided with two DataFrames: `df1` and `df2`.\n\n`df1`:\n```\n   product_id  category     date  sales\n0           1      A 2023/01/01    100\n1           2      B 2023/01/01    150\n2           3      A 2023/01/02    200\n3           4      C 2023/01/01    50\n4           5      B 2023/01/03    300\n```\n\n`df2`:\n```\n   product_id     date  sales\n0           1 2023/01/01     80\n1           2 2023/01/03    120\n2           3 2023/01/02    210\n3           6 2023/01/04    90\n4           7 2023/01/02     60\n```\n\nYou need to concatenate `df1` and `df2` based on `product_id`, filling in the missing `category` information from `df1` into `df2`. Ensure the results are sorted by `product_id` and the dates are formatted as \"DD-Mon-YYYY\". Duplicate entries should be kept in the final DataFrame.\n\nThe final DataFrame should look like this:\n```\n   product_id category         date  sales\n0           1        A  01-Jan-2023    100\n1           1        A  01-Jan-2023     80\n2           2        B  01-Jan-2023    150\n3           2        B  01-Jan-2023    120\n4           3        A  02-Jan-2023    200\n5           3        A  02-Jan-2023    210\n6           4        C  01-Jan-2023     50\n7           5        B  03-Jan-2023    300\n8           6      NaN  04-Jan-2023     90\n9           7      NaN  02-Jan-2023     60\n```\n\nSo far, the result generated with `pd.concat([df1, df2], axis=0)` is like this:\n```\n   product_id     date category  sales\n0           1 2023/01/01        A    100\n1           2 2023/01/01        B    150\n2           3 2023/01/02        A    200\n3           4 2023/01/01        C     50\n4           5 2023/01/03        B    300\n0           1 2023/01/01      NaN     80\n1           2 2023/01/03      NaN    120\n2           3 2023/01/02      NaN    210\n3           6 2023/01/04      NaN     90\n4           7 2023/01/02      NaN     60\n```\n\nYour task is to complete the following code to achieve the desired transformation and formatting:\n\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'product_id': [1, 2, 3, 4, 5],\n    'category': ['A', 'B', 'A', 'C', 'B'],\n    'date': ['2023/01/01', '2023/01/01', '2023/01/02', '2023/01/01', '2023/01/03'],\n    'sales': [100, 150, 200, 50, 300]\n})\n\ndf2 = pd.DataFrame({\n    'product_id': [1, 2, 3, 6, 7],\n    'date': ['2023/01/01', '2023/01/03', '2023/01/02', '2023/01/04', '2023/01/02'],\n    'sales': [80, 120, 210, 90, 60]\n})\n\n# Your solution should populate this variable\nresult = ...\n```",
    "output": "\n```python\nresult = pd.concat([df1, df2.assign(category=df2['product_id'].map(df1.set_index('product_id')['category']))], axis=0)\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result.sort_values(by='product_id').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nYou are working on a dataset containing information about customer transactions, and you have the following DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'CustomerID': [1, 1, 2, 2, 3, 3, 3],\n    'Transaction': ['T1', 'T2', 'T1', 'T3', 'T1', 'T2', 'T3'],\n    'Amount': [250, 150, 300, 500, 100, 200, 300],\n    'Date': ['2023-01-10', '2023-01-15', '2023-01-12', '2023-01-20', '2023-01-05', '2023-01-07', '2023-01-09']\n}\n\ndf = pd.DataFrame(data)\ndf['Date'] = pd.to_datetime(df['Date'])\n```\n\nYou want to create a new DataFrame that aggregates the total spent (`Amount`) by each customer across all their transactions, while also including the latest transaction date per customer. The resulting DataFrame should be structured with 'CustomerID' as the index, and columns should include 'TotalSpent' and 'LastTransactionDate'. \n\nHow can you achieve this?\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby('CustomerID').agg(TotalSpent=('Amount', 'sum'), LastTransactionDate=('Date', 'max')).reset_index().set_index('CustomerID')\n</code>"
  },
  {
    "input": "## Problem\nYou have the following pandas DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Category': ['A', 'A', 'B', 'B', 'C', 'C', 'C'],\n    'Value': [10, np.nan, 20, 30, np.nan, 25, np.nan],\n    'Count': [1, 2, 1, 2, 1, 1, 2]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to calculate the weighted average of the 'Value' column for each 'Category'. However, any NaN values in 'Value' should be ignored, and for categories where all values are NaN, the result should be NaN as well. Moreover, the weights for the average should be the corresponding 'Count' values.\n\nThe expected output should have the following structure:\n\n```plaintext\nCategory  Weighted_Average\nA         10.0\nB         25.0\nC         25.0\n```\n\nYou know that you can use `groupby` along with a custom aggregation function, but you're unsure how to implement the specific logic for calculating the weighted average while handling NaN values.\n\nHow can you do this?\n\n```python\nresult = ...\n```",
    "output": "\n```python\nresult = df.groupby('Category').apply(lambda x: np.average(x['Value'], weights=x['Count']) if x['Value'].notna().any() else np.nan).reset_index(name='Weighted_Average')\n```"
  },
  {
    "input": "Problem:\nI am working with two DataFrames, `sales_data` and `returns_data`, which contain sales and return information, respectively. Below are the structures of both DataFrames:\n\n`sales_data`\n```\n   OrderID     Date   Amount\n0        1 2023-01-01  150.00\n1        2 2023-01-01  200.00\n2        3 2023-01-02  250.00\n3        4 2023-01-03  300.00\n```\n\n`returns_data`\n```\n   OrderID     Date   Amount\n0        2 2023-01-01   50.00\n1        3 2023-01-02  100.00\n2        5 2023-01-04  200.00\n```\n\nI want to create a new DataFrame, `result`, where I merge `sales_data` with `returns_data` on `OrderID`. The `Amount` in the `returns_data` should subtract from the corresponding `Amount` in the `sales_data` where `OrderID` matches. If there is no matching `OrderID` in `returns_data`, the `Amount` from `sales_data` should remain unchanged. \n\nAdditionally, the `result` DataFrame should have the following structure, including an indicator column called `Returned` to specify if the item was returned:\n\n```\n   OrderID     Date   Amount  Returned\n0        1 2023-01-01  150.00     False\n1        2 2023-01-01  150.00      True\n2        3 2023-01-02  150.00      True\n3        4 2023-01-03  300.00     False\n```\n\nCan you help me fill in the code to perform this operation?\n\nA:\n<code>\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'OrderID': [1, 2, 3, 4],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-03']),\n    'Amount': [150.00, 200.00, 250.00, 300.00]\n})\n\nreturns_data = pd.DataFrame({\n    'OrderID': [2, 3, 5],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-04']),\n    'Amount': [50.00, 100.00, 200.00]\n})\n\nresult = ... # fill in your solution here\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = sales_data.merge(returns_data, on='OrderID', how='left', suffixes=('', '_return'))\nresult['Amount'] = result['Amount'] - result['Amount_return'].fillna(0)\nresult['Returned'] = result['Amount_return'].notna()\nresult = result[['OrderID', 'Date', 'Amount', 'Returned']]\n</code>"
  },
  {
    "input": "## Problem:\nContext\nI am working with sales data stored in two different DataFrames and need to combine them. The first DataFrame contains overall sales details per transaction, while the second contains some additional metrics like discounts applied. I need to ensure that when merging these DataFrames, I only keep relevant columns and handle any missing values appropriately.\n\nGiven the following DataFrames:\n```python\ndf_sales = pd.DataFrame({\n    'TransactionID': [1001, 1002, 1003, 1004],\n    'Product': ['A', 'B', 'A', 'C'],\n    'Amount': [250, 150, 200, 300],\n    'Date': ['2023-10-01', '2023-10-02', '2023-10-01', '2023-10-03']\n})\n\ndf_discounts = pd.DataFrame({\n    'TransactionID': [1001, 1003, 1004, 1005],\n    'Discount': [25, 30, 50, 20],\n    'PromoCode': ['SAVE10', 'SAVE20', '', 'SAVE15']\n})\n```\n\nI want to merge `df_sales` with `df_discounts` based on the `TransactionID` column, while only keeping the `TransactionID`, `Product`, `Amount`, and `Discount` columns in the final result. Also, for any missing discounts, I want to fill with 0, and the resulting `Date` should be formatted as \"DD-MMM-YYYY\".\n\nThe expected result should look like:\n```\n   TransactionID Product  Amount  Discount        Date\n0           1001      A     250        25  01-Oct-2023\n1           1002      B     150         0  02-Oct-2023\n2           1003      A     200        30  01-Oct-2023\n3           1004      C     300        50  03-Oct-2023\n```\n\nHow do I accomplish this merge and manipulate the data accordingly? \n\nA:\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'TransactionID': [1001, 1002, 1003, 1004],\n    'Product': ['A', 'B', 'A', 'C'],\n    'Amount': [250, 150, 200, 300],\n    'Date': ['2023-10-01', '2023-10-02', '2023-10-01', '2023-10-03']\n})\n\ndf_discounts = pd.DataFrame({\n    'TransactionID': [1001, 1003, 1004, 1005],\n    'Discount': [25, 30, 50, 20],\n    'PromoCode': ['SAVE10', 'SAVE20', '', 'SAVE15']\n})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = (\n    pd.merge(df_sales, df_discounts[['TransactionID', 'Discount']], on='TransactionID', how='left')\n    .fillna({'Discount': 0})\n    .assign(Date=lambda x: pd.to_datetime(x['Date']).dt.strftime('%d-%b-%Y'))\n    [['TransactionID', 'Product', 'Amount', 'Discount', 'Date']]\n)\n```"
  },
  {
    "input": "### Problem:\nYou are given a multi-indexed DataFrame containing sales data for different products over various time periods. The DataFrame has the following structure:\n\n```python\nimport pandas as pd\n\nindex = pd.MultiIndex.from_tuples([(2022, 'Q1'), (2022, 'Q2'), (2022, 'Q3'), (2022, 'Q4'),\n                                    (2023, 'Q1'), (2023, 'Q2'), (2023, 'Q3'), (2023, 'Q4')],\n                                   names=['Year', 'Quarter'])\ndata = {'Product_A': [100, 150, 130, 160, 110, 145, 120, 170],\n        'Product_B': [90, 120, 115, 130, 100, 140, 125, 150]}\ndf = pd.DataFrame(data, index=index)\n```\n\nYour task is to normalize the sales data for each product within the given time periods and create a new DataFrame that preserves the multi-index structure. The normalization should be done using the Min-Max scaling method, where each value is scaled to a range between 0 and 1.\n\nYou need to fill in the following code to achieve this normalization, ensuring that your result retains the original multi-index:\n\n```python\nnormalized_df = df .___fill_in_the_blank___\n```\n\nMake sure to use the appropriate methods from Pandas and NumPy to perform the normalization.",
    "output": "\n```python\nnormalized_df = (df - df.min()) / (df.max() - df.min())\n```"
  },
  {
    "input": "Problem:\nYou are given a dataset with user engagement metrics for an online platform over a period of time. The data looks like this:\n\n```python\nimport pandas as pd\nimport io\n\ndata = \"\"\"user,date,session_length,activity\nuser1,2023-01-01,30,click\nuser1,2023-01-02,45,scroll\nuser2,2023-01-01,20,click\nuser2,2023-01-03,35,click\nuser3,2023-01-01,50,scroll\nuser1,2023-01-03,60,scroll\nuser3,2023-01-02,40,click\"\"\"\ndf = pd.read_csv(io.StringIO(data))\n```\n\nYou want to pivot this DataFrame to show the total session length for each user by date, while also maintaining a multi-index where the first level is the user and the second level is the date. The activity column is not needed for the pivot operation.\n\nHow can you achieve this in Python using pandas? Fill in the following code snippet to create the desired pivot table:\n\n```python\nresult = df.pivot_table(index='user', columns='date', values='session_length', aggfunc='sum')\n```\n\nThis code should set the multi-index where the first level is the user and second level is the date, with total session lengths as the values.",
    "output": "\n```python\nresult = df.pivot_table(index='user', columns='date', values='session_length', aggfunc='sum').reset_index().set_index(['user'])\n```"
  },
  {
    "input": "Problem:\n\nContext:\nI have two DataFrames representing product sales data from two different regions. I need to combine these datasets while ensuring that products from the second region replace the sales figures of the first region wherever they coincide. Additionally, I want to add a column that denotes whether a product's sales data from the second region is filling in a duplicate entry.\n\nDataFrames:\ndf1:\n```\n    Product    Sales\n0   A          150\n1   B          200\n2   C          300\n```\n\ndf2:\n```\n    Product    Sales\n0   B          250\n1   D          400\n```\n\nThe goal is to achieve a final DataFrame like this, maintaining the order of products from df1:\n```\n    Product    Sales     is_duplicated\n0   A          150      False\n1   B          250      True\n2   C          300      False\n3   D          400      True\n```\n\nYou can start with this code snippet:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'Product': ['A', 'B', 'C'], 'Sales': [150, 200, 300]})\ndf2 = pd.DataFrame({'Product': ['B', 'D'], 'Sales': [250, 400]})\n```\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nmerged = pd.merge(df1, df2, on='Product', how='outer', suffixes=('_df1', '_df2'))\nmerged['Sales'] = merged['Sales_df2'].combine_first(merged['Sales_df1'])\nmerged['is_duplicated'] = merged['Sales_df2'].notnull()\nresult = merged[['Product', 'Sales', 'is_duplicated']]\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a tensor representing images with a shape of (num_images, height, width, channels), where you have a batch of images and their respective confidences. You want to apply a thresholding operation to filter out images with a confidence score below a certain value. \n\nGiven the following tensors:\n\n```python\nconfidences = torch.tensor([[0.9, 0.8, 0.85], [0.5, 0.1, 0.97], [0.65, 0.88, 0.9]])\nimages = torch.rand((3, 128, 128, 3))  # Random images\nthreshold = 0.7 \n```\n\nThe goal is to create a new tensor that retains only the images with confidence scores above the provided threshold. Your solution should apply boolean masking based on the condition.\n\nHow would you fill in the code to achieve this?\n\n```python\nimport torch\n\nconfidences = torch.tensor([[0.9, 0.8, 0.85], [0.5, 0.1, 0.97], [0.65, 0.88, 0.9]])\nimages = torch.rand((3, 128, 128, 3))  # Random images\nthreshold = 0.7\n```\nfiltered_images = ... # put solution in this variable",
    "output": "\n```python\nfiltered_images = images[confidences > threshold]\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames, `sales_data` and `returns_data`, as follows:\n\n`sales_data`\n```\n    order_id   product_id  quantity  sale_date\n0        001         P01        10  2023-01-01\n1        002         P02        5   2023-01-03\n2        003         P01        8   2023-01-05\n3        004         P03        20  2023-01-07\n4        005         P01        15  2023-01-09\n```\n\n`returns_data`\n```\n    return_id   product_id  return_quantity  return_date\n0          R01         P01               3  2023-01-06\n1          R02         P02               1  2023-01-07\n2          R03         P01               2  2023-01-10\n```\n\nYour goal is to create a new DataFrame called `net_sales` that summarizes the total sales for each product, while adjusting for returns. If a product has been returned, you should subtract the returned quantity from the total sold quantity. The resulting DataFrame should have columns for `product_id`, `total_sold`, and `net_sales`, formatted in such a way that `net_sales` can't be negative.\n\nHere\u2019s the starting code you have:\n\n```python\nimport pandas as pd\n\nsales_data = pd.DataFrame({\n    'order_id': ['001', '002', '003', '004', '005'],\n    'product_id': ['P01', 'P02', 'P01', 'P03', 'P01'],\n    'quantity': [10, 5, 8, 20, 15],\n    'sale_date': pd.to_datetime(['2023-01-01', '2023-01-03', '2023-01-05', '2023-01-07', '2023-01-09'])\n})\n\nreturns_data = pd.DataFrame({\n    'return_id': ['R01', 'R02', 'R03'],\n    'product_id': ['P01', 'P02', 'P01'],\n    'return_quantity': [3, 1, 2],\n    'return_date': pd.to_datetime(['2023-01-06', '2023-01-07', '2023-01-10'])\n})\n\n# Your code for net_sales calculation starts here:\nnet_sales = sales_data.groupby('product_id')['quantity'].sum().reset_index()\nnet_sales = ... # put solution in this variable\n```\n\nFill in the blank with code that accomplishes the task of computing `net_sales`. Ensure that negative values are handled correctly.",
    "output": "\n```python\nreturns_summary = returns_data.groupby('product_id')['return_quantity'].sum().reset_index()\nnet_sales = net_sales.merge(returns_summary, on='product_id', how='left').fillna(0)\nnet_sales['net_sales'] = net_sales['quantity'] - net_sales['return_quantity']\nnet_sales['net_sales'] = net_sales['net_sales'].clip(lower=0)\nnet_sales = net_sales[['product_id', 'quantity', 'net_sales']].rename(columns={'quantity': 'total_sold'})\n```"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame that includes sales information from various stores, structured as follows:\n```\n     store    sales      date\n0   Store A   200.0  2023-01-01\n1   Store B   NaN    2023-01-02\n2   Store A   150.0  2023-01-03\n3   Store C   300.0  2023-01-04\n4   Store B   250.0  2023-01-05\n5   Store A   NaN    2023-01-06\n```\nThis DataFrame has some missing values in the 'sales' column, which I want to handle before performing any operations.\n\nMy goal is to create a new DataFrame that contains only the total sales per store for sales made after January 1, 2023, while replacing any missing values with the average sales for that store before aggregating.\n\nHere are my code snippets:\n```\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'store': ['Store A', 'Store B', 'Store A', 'Store C', 'Store B', 'Store A'],\n    'sales': [200.0, np.nan, 150.0, 300.0, 250.0, np.nan],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06'])\n})\n```\nHow can I achieve this aggregation and handle null values appropriately?\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\ndf['sales'] = df.groupby('store')['sales'].transform(lambda x: x.fillna(x.mean()))\nresult = df[df['date'] > '2023-01-01'].groupby('store')['sales'].sum().reset_index()\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames with the following structure:\n\ndf1:\n```\n   employee_id   department   date               score\n0           1         HR      2022-01-01         88\n1           2         IT      2022-01-01         91\n2           3         IT      2022-01-01         85\n3           1         HR      2022-02-01         82\n4           2         IT      2022-02-01         90\n```\n\ndf2:\n```\n   employee_id   date               score\n0           3      2022-01-05          85\n1           1      2022-01-05          90\n2           1      2022-03-01          92\n3           4      2022-01-05          75\n4           5      2022-01-05          88\n```\n\nThe task is to merge these two DataFrames into a single DataFrame such that:\n1. Rows are aligned by `employee_id` and `date`.\n2. If there are duplicate entries for an employee on a specific date in either DataFrame, keep the one with the highest score.\n3. Fill missing department information in df2 from df1 where applicable.\n\nThe expected DataFrame after merging and deduplication should look like this:\n```\n   employee_id   department   date               score\n0           1         HR      2022-01-01         88\n1           1         HR      2022-01-05         90\n2           1         HR      2022-02-01         82\n3           1         HR      2022-03-01         92\n4           2         IT      2022-01-01         91\n5           2         IT      2022-02-01         90\n6           3         IT      2022-01-01         85\n7           3         NaN     2022-01-05        85\n8           4         NaN     2022-01-05         75\n9           5         NaN     2022-01-05         88\n```\n\nSo far I have attempted the following code to start the merging process:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'employee_id': [1, 2, 3, 1, 2],\n                    'department': ['HR', 'IT', 'IT', 'HR', 'IT'],\n                    'date': ['2022-01-01', '2022-01-01', '2022-01-01', '2022-02-01', '2022-02-01'],\n                    'score': [88, 91, 85, 82, 90]})\n\ndf2 = pd.DataFrame({'employee_id': [3, 1, 1, 4, 5],\n                    'date': ['2022-01-05', '2022-01-05', '2022-03-01', '2022-01-05', '2022-01-05'],\n                    'score': [85, 90, 92, 75, 88]})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nmerged = pd.concat([df1, df2], ignore_index=True)\nmerged = merged.sort_values(['employee_id', 'date']).drop_duplicates(subset=['employee_id', 'date'], keep='last')\nresult = merged.merge(df1[['employee_id', 'department']], on='employee_id', how='left').fillna(method='ffill')\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset containing multiple features, and wish to apply a Min-Max scaling to normalize the features. The dataset is represented as a pandas DataFrame with some features having missing values. Before scaling, you need to handle these missing values by replacing them with the median of each feature. After dealing with the missing values, you will perform the Min-Max scaling. For example, consider the following DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'feature1': [2, np.nan, 5, 7, 9],\n    'feature2': [1, 2, np.nan, 4, 5],\n    'feature3': [np.nan, 2, 3, np.nan, 5]\n}\ndf = pd.DataFrame(data)\n```\n\nYou need to compute the Min-Max scaled DataFrame and store it in a variable called `scaled_df`. The Min-Max scaling should transform the features to a range between 0 and 1. There is already a method to perform Min-Max scaling in sklearn.\n\nA:\n```python\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n```\nscaled_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nimputer = SimpleImputer(strategy='median')\ndf_imputed = imputer.fit_transform(df)\n\nscaler = MinMaxScaler()\nscaled_df = scaler.fit_transform(df_imputed)\nscaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n</code>\n```"
  },
  {
    "input": "Problem:\n\nI am working with a dataset containing user ratings for multiple items, represented as a PyTorch tensor. I need to compute the cosine similarity between each pair of user rating vectors to identify which users have similar preferences. \n\nGiven the ratings tensor, I want to create a boolean mask that filters out user pairs with a cosine similarity below a certain threshold (e.g., 0.5) and returns a tensor that contains only the pairs meeting this criteria.\n\nThe ratings tensor is given below:\n```\nratings = torch.tensor([[5, 1, 0], \n                        [4, 0, 2], \n                        [0, 3, 4], \n                        [0, 0, 5]])\n```\n\nHow can I generate a mask for this filtering operation based on the cosine similarity calculation? \n\nA:\n<code>\nimport torch\nratings = torch.tensor([[5, 1, 0], \n                        [4, 0, 2], \n                        [0, 3, 4], \n                        [0, 0, 5]])\n</code>\nmask = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom torch.nn.functional import cosine_similarity\n\n# Compute cosine similarity between each pair of users\nsimilarity_matrix = cosine_similarity(ratings.unsqueeze(1), ratings.unsqueeze(0), dim=2)\n\n# Create a boolean mask for similarity above the threshold\nthreshold = 0.5\nmask = similarity_matrix > threshold\n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a DataFrame containing user activity data for a mobile application. The DataFrame has the following columns: `UserID`, `Activity`, and `Timestamp`. Each user may have multiple activities recorded at different timestamps. Your goal is to transform this data to analyze the frequency of each activity per user, while normalizing the counts. The sample DataFrame looks like this:\n\n```\n  UserID    Activity              Timestamp\n0      1      Login      2023-09-01 10:00:00\n1      1      Logout     2023-09-01 12:00:00\n2      1      Purchase   2023-09-01 11:30:00\n3      2      Login      2023-09-01 09:00:00\n4      2      Purchase   2023-09-01 09:30:00\n5      2      Logout     2023-09-01 11:00:00\n6      3      Login      2023-09-01 08:30:00\n7      3      Purchase   2023-09-01 09:00:00\n```\n\nYou need to pivot this DataFrame such that each activity type has its own column, and the values indicate the frequency a user performed that activity. Additionally, normalize the counts per user so that they sum to 1 for each user. The resulting DataFrame should look like this:\n\n```\n  UserID   Login   Logout   Purchase\n0      1     0.33     0.33     0.33\n1      2     0.33     0.33     0.33\n2      3     0.50     0.00     0.50\n```\n\nHow can you achieve this?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'UserID': [1, 1, 1, 2, 2, 2, 3, 3],\n    'Activity': ['Login', 'Logout', 'Purchase', 'Login', 'Purchase', 'Logout', 'Login', 'Purchase'],\n    'Timestamp': pd.to_datetime(['2023-09-01 10:00:00', '2023-09-01 12:00:00', '2023-09-01 11:30:00',\n                                  '2023-09-01 09:00:00', '2023-09-01 09:30:00', '2023-09-01 11:00:00',\n                                  '2023-09-01 08:30:00', '2023-09-01 09:00:00'])\n}\n\ndf = pd.DataFrame(data)\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby(['UserID', 'Activity']).size().unstack(fill_value=0)\nresult = result.div(result.sum(axis=1), axis=0)\n</code>"
  },
  {
    "input": "Problem:\n\nYou have a multi-index Pandas DataFrame representing sales data for different products across various regions and years. The DataFrame, named `sales_df`, looks like this:\n\n```\nsales_df = pd.DataFrame({\n    'ProductA': [100, 150, 200],\n    'ProductB': [80, 120, 160],\n    'ProductC': [90, 110, 140]\n}, index=pd.MultiIndex.from_tuples([\n    ('North', 2021),\n    ('North', 2022),\n    ('South', 2021)\n], names=['Region', 'Year']))\n```\n\nYour task is to normalize the sales data, so each product's sales figures are transformed to be between 0 and 1 based on the max value within its product category across all regions and years. You want to store the normalized DataFrame into a variable called `normalized_sales`.\n\nHow can you achieve this?\n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'ProductA': [100, 150, 200],\n    'ProductB': [80, 120, 160],\n    'ProductC': [90, 110, 140]\n}, index=pd.MultiIndex.from_tuples([\n    ('North', 2021),\n    ('North', 2022),\n    ('South', 2021)\n], names=['Region', 'Year']))\n</code>\nnormalized_sales = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nnormalized_sales = sales_df / sales_df.max()"
  },
  {
    "input": "Problem:\n\nI have a Pandas DataFrame containing sales data that includes a hierarchical index representing year, quarter, and product category, along with the sales figures. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples([\n    (2021, 'Q1', 'Electronics'),\n    (2021, 'Q1', 'Furniture'),\n    (2021, 'Q2', 'Electronics'),\n    (2021, 'Q2', 'Furniture'),\n    (2022, 'Q1', 'Electronics'),\n    (2022, 'Q1', 'Furniture')\n], names=['Year', 'Quarter', 'Category'])\n\ndata = {\n    'Sales': [2000, 1500, 3000, 2500, 3200, 2600]\n}\n\ndf = pd.DataFrame(data, index=index)\n```\n\nNow, I want to normalize the sales figures across the different product categories within each quarter. How can I achieve that using Pandas? Specifically, I want to modify the DataFrame such that each sales figure is expressed as a fraction of the total sales for that quarter, i.e. for each `(Year, Quarter)` combination, the sales for each category should be divided by the total sales for that quarter.\n\nI would do this in a generalizable way using the following code:\n\n```python\nnormalized_sales = ...\n```\n",
    "output": "\n```python\nnormalized_sales = df.groupby(level=['Year', 'Quarter']).transform(lambda x: x / x.sum())\n```"
  },
  {
    "input": "Problem:\nI am working with a dataset containing information about houses, with features like size, number of rooms, and age. I want to preprocess this data before training a machine learning model. Specifically, I need to one-hot encode the categorical feature 'location' and normalize the 'size' feature. After applying the transformations, I want to create a new DataFrame with the transformed features ready for modeling.\n\nHere is my example DataFrame:\n\n```python\nimport pandas as pd\n\ndata = {\n    'size': [1500, 2000, 1700, 2500, 1800],\n    'num_rooms': [3, 4, 3, 5, 4],\n    'age': [10, 20, 15, 5, 12],\n    'location': ['urban', 'suburban', 'urban', 'rural', 'suburban']\n}\ndf = pd.DataFrame(data)\n```\n\nI want to fill in the code that applies the transformations and creates a transformed DataFrame called `transformed_df`. \n\nThe transformation steps are:\n1. One-hot encode the 'location' column.\n2. Normalize the 'size' column so that it has a mean of 0 and a standard deviation of 1.\n\nA:\n<code>\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Your code here\nencoder = OneHotEncoder()\nscaler = StandardScaler()\n\n# Fit and transform the necessary columns\nlocation_encoded = encoder.fit_transform(df[['location']]).toarray()\nsize_normalized = scaler.fit_transform(df[['size']])\n\n# Combine the transformed features with the original numeric columns\ntransformed_df = pd.DataFrame(data=size_normalized, columns=['size_normalized'])\ntransformed_df = pd.concat([transformed_df, df[['num_rooms', 'age']].reset_index(drop=True), \n                            pd.DataFrame(location_encoded, columns=encoder.get_feature_names_out(['location']))], axis=1)\n\ntransformed_df\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nencoder = OneHotEncoder()\nscaler = StandardScaler()\n\nlocation_encoded = encoder.fit_transform(df[['location']]).toarray()\nsize_normalized = scaler.fit_transform(df[['size']])\n\ntransformed_df = pd.DataFrame(data=size_normalized, columns=['size_normalized'])\ntransformed_df = pd.concat([transformed_df, df[['num_rooms', 'age']].reset_index(drop=True), \n                            pd.DataFrame(location_encoded, columns=encoder.get_feature_names_out(['location']))], axis=1)\n\ntransformed_df\n</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset representing sales data for different products over several months. The dataframe includes duplicate entries for some products in certain months, and I need to process this data to keep only the most recent sales record for each product, while also replacing any missing month entries with the most recent available month. Afterward, I want to sort the final dataframe by month.\n\nHere's my initial dataframe:\n\n```python\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\n    'product_id': [1, 1, 2, 2, 3, 3, 1],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-03', '2023-01', np.nan, '2023-01'],\n    'sales': [100, 150, 200, 250, 300, 350, 200]\n})\n```\n\nI executed the following operations to find the most recent record per product:\n\n```python\nlatest = df.loc[df['month'].notnull()].sort_values(by='month').drop_duplicates(subset='product_id', keep='last')\n```\n\nNow, I want to fill the missing month values with the latest available month for each product, and finally sort the dataframe by 'month'.\n\nHow can I achieve the final result with a single line of code that fulfills this requirement?\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'product_id': [1, 1, 2, 2, 3, 3, 1],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-03', '2023-01', np.nan, '2023-01'],\n    'sales': [100, 150, 200, 250, 300, 350, 200]\n})\nfinal_result = ... # your solution here\n### BEGIN SOLUTION",
    "output": "\n<code>\nfinal_result = df.groupby('product_id').apply(lambda x: x.ffill().sort_values(by='month')).reset_index(drop=True).sort_values(by='month')\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products over several months. Each row represents a single product and its sales figures for multiple months. The DataFrame also contains some NaN values for products that did not sell in specific months. You want to create a new column that computes the total sales for each product while disregarding the NaN values, and then apply a custom function to categorize the products based on their total sales\u2014if the total is above 500, categorize it as 'High', otherwise 'Low'. \n\nHere is the DataFrame structure you'll start with:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'C', 'D'],\n    'Jan': [150, np.nan, 300, 100],\n    'Feb': [200, 300, np.nan, np.nan],\n    'Mar': [np.nan, 250, 150, 200],\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou want to achieve the following output after processing:\n\n```\n  Product  Total Sales Category\n0       A          350     Low\n1       B          550    High\n2       C          450     Low\n3       D          300     Low\n```\n\nWhat code fills in the blank to achieve this desired output?\n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'C', 'D'],\n    'Jan': [150, np.nan, 300, 100],\n    'Feb': [200, 300, np.nan, np.nan],\n    'Mar': [np.nan, 250, 150, 200],\n}\n\ndf = pd.DataFrame(data)\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf['Total Sales'] = df[['Jan', 'Feb', 'Mar']].sum(axis=1, skipna=True)\ndf['Category'] = df['Total Sales'].apply(lambda x: 'High' if x > 500 else 'Low')\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames, df_sales and df_products, that I need to work with. The df_sales DataFrame contains sales data, including product IDs and quantities sold, while df_products contains information about the products. The goal is to combine these two DataFrames on the product ID, remove any duplicate sales entries (keeping the first occurrence), and replace any missing product details in the merged DataFrame with placeholder values.\n\nHere are the DataFrames:\n\ndf_sales:\n```\n   product_id  quantity\n0           1        10\n1           1        15\n2           2        5\n3           3        20\n4           2        10\n```\n\ndf_products:\n```\n   product_id    product_name   category\n0           1  Product A       Type X\n1           2  Product B       Type Y\n2           3  Product C       Type Z\n3           4  Product D       Type W\n```\n\nAfter merging, the expected DataFrame should look like this:\n```\n   product_id  quantity   product_name   category\n0           1        10      Product A   Type X\n1           1        15      Product A   Type X\n2           2         5      Product B   Type Y\n3           3        20      Product C   Type Z\n4           2        10      Product B   Type Y\n```\n\nI want to make sure that any missing product details (like product_id 4 in df_products) are replaced with \"Missing Product\" in the `product_name` and \"Unknown Category\" in the `category`.\n\nSo far, I attempted the following:\n\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({'product_id': [1, 1, 2, 3, 2],\n                         'quantity': [10, 15, 5, 20, 10]})\n\ndf_products = pd.DataFrame({'product_id': [1, 2, 3, 4],\n                            'product_name': ['Product A', 'Product B', 'Product C', 'Product D'],\n                            'category': ['Type X', 'Type Y', 'Type Z', 'Type W']})\n\nmerged_df = pd.merge(df_sales, df_products, on='product_id')\nmerged_df = merged_df.drop_duplicates()\nmerged_df['product_name'].fillna(\"Missing Product\", inplace=True)\nmerged_df['category'].fillna(\"Unknown Category\", inplace=True)\n```\n\nI need to complete the merging and handle the duplicates properly. What should I put in the following placeholder for completing the code?\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = merged_df.fillna({'product_name': 'Missing Product', 'category': 'Unknown Category'})\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames containing sales data. The first DataFrame has sales records with columns for 'product_id', 'store_id', and 'sale_amount'. The second one contains product information with 'product_id' and 'product_category'. I want to calculate the total sales amount per product category, but also modify sales records to handle negative sales as zero and ignore any entries with missing values. \n\nHere are my DataFrames:\n\n**Sales DataFrame:**\n```\nproduct_id | store_id | sale_amount\n--------------------------------------\n1          | 101      | 150\n2          | 101      | -50\n1          | 102      | 200\n3          | 103      | None\n4          | 101      | 300\n```\n\n**Product DataFrame:**\n```\nproduct_id | product_category\n-------------------------------\n1          | Electronics\n2          | Accessories\n3          | Electronics\n4          | Apparel\n```\n\nI want to get a new DataFrame that shows the total sales amount for each product category with the following structure:\n\n```\nproduct_category | total_sales\n------------------------------\nElectronics      | 350\nAccessories      | 0\nApparel          | 300\n```\n\nHow can I achieve this using Pandas? \n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'product_id': [1, 2, 1, 3, 4],\n    'store_id': [101, 101, 102, 103, 101],\n    'sale_amount': [150, -50, 200, None, 300]\n})\n\nproduct_df = pd.DataFrame({\n    'product_id': [1, 2, 3, 4],\n    'product_category': ['Electronics', 'Accessories', 'Electronics', 'Apparel']\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\nsales_df['sale_amount'] = sales_df['sale_amount'].fillna(0).clip(lower=0)\nmerged_df = sales_df.merge(product_df, on='product_id', how='left')\nresult = merged_df.groupby('product_category')['sale_amount'].sum().reset_index()\nresult.columns = ['product_category', 'total_sales']\n</code>"
  },
  {
    "input": "Problem:\nYou have two DataFrames representing sales records for two months:\n\ndf_january:\n```\n   id  product quantity    date  revenue\n0   1  apples       30  2023-01-01       300\n1   2  bananas      50  2023-01-01       200\n2   3  cherries     20  2023-01-01       400\n```\n\ndf_february:\n```\n   id  product quantity    date  revenue\n0   3  cherries     15  2023-02-01       300\n1   4  dates        10  2023-02-01       150\n2   5  elderberries 25  2023-02-01       600\n```\n\nAfter merging these two DataFrames on the 'id' column, you want to ensure that for products sold in January but not in February, the quantity and revenue reflect the January values. For products sold in February that weren't in January, you want to keep the February values. Additionally, any duplicate entries (same id for January and February) should show the February 'quantity' and 'revenue', while aligning the dates properly in 'dd-MMM-yyyy' format.\n\nThe expected merged result should look like this:\n```\n   id       product quantity    date      revenue\n0   1       apples      30  01-Jan-2023       300\n1   2      bananas      50  01-Jan-2023       200\n2   3     cherries      15  01-Feb-2023       300\n3   4         dates      10  01-Feb-2023       150\n4   5 elderberries      25  01-Feb-2023       600\n```\n\nYou initially concatenated the two DataFrames, resulting in duplicates and misaligned dates due to direct concatenation. How can you do this correctly? \n\nFill in the code below to achieve this:\n```\nimport pandas as pd\n\ndf_january = pd.DataFrame({'id': [1, 2, 3],\n                            'product': ['apples', 'bananas', 'cherries'],\n                            'quantity': [30, 50, 20],\n                            'date': ['2023-01-01', '2023-01-01', '2023-01-01'],\n                            'revenue': [300, 200, 400]})\n\ndf_february = pd.DataFrame({'id': [3, 4, 5],\n                             'product': ['cherries', 'dates', 'elderberries'],\n                             'quantity': [15, 10, 25],\n                             'date': ['2023-02-01', '2023-02-01', '2023-02-01'],\n                             'revenue': [300, 150, 600]})\n\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```\nresult = pd.merge(df_january, df_february, on='id', how='outer', suffixes=('_jan', '_feb'))\nresult['quantity'] = result['quantity_feb'].combine_first(result['quantity_jan'])\nresult['revenue'] = result['revenue_feb'].combine_first(result['revenue_jan'])\nresult['date'] = result['date_feb'].combine_first(result['date_jan'])\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult = result[['id', 'product_feb', 'quantity', 'date', 'revenue']]\nresult.rename(columns={'product_feb': 'product'}, inplace=True)\n```"
  },
  {
    "input": "Problem:\nContext\nI have two DataFrames representing sales data and product information. The sales DataFrame contains duplicates for products sold in different transactions, while the product DataFrame has relevant product details. \n\nSales DataFrame (sales_df):\n```\nProductID    Date       Quantity \n-----------------------------------\n101          2023-01-01 2\n102          2023-01-01 4\n101          2023-01-02 1\n103          2023-01-01 3\n102          2023-01-01 2\n```\n\nProduct DataFrame (product_df):\n```\nProductID    ProductName    Price\n----------------------------------------\n101          Widget A       25.00\n102          Widget B       15.00\n103          Widget C       20.00\n```\n\nI need to combine these DataFrames to create a new DataFrame that shows the total quantity sold for each product along with the product name and total sales value. The expected final DataFrame should look like this:\n```\nProductID    ProductName    TotalQuantity   TotalSales\n-------------------------------------------------------\n101          Widget A       3               75.00\n102          Widget B       6               90.00\n103          Widget C       3               60.00\n```\n\nTo achieve this, I will refer to the sales DataFrame, group by `ProductID`, sum the `Quantity`, and then merge with the product DataFrame to calculate `TotalSales`.\n\nSo far, I have done the following steps:\n\n1. Group the sales_df by `ProductID` using `groupby()` and calculate the sum of `Quantity`.\n2. Merge the result with product_df to bring in `ProductName` and `Price`.\n\nI am unsure how to calculate `TotalSales` based on the final merged DataFrame.\n\nA:\n```python\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'ProductID': [101, 102, 101, 103, 102],\n    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-01', '2023-01-01'],\n    'Quantity': [2, 4, 1, 3, 2]\n})\n\nproduct_df = pd.DataFrame({\n    'ProductID': [101, 102, 103],\n    'ProductName': ['Widget A', 'Widget B', 'Widget C'],\n    'Price': [25.00, 15.00, 20.00]\n})\n\n# group sales_df by 'ProductID' and sum 'Quantity'\ntotal_quantity = sales_df.groupby('ProductID').agg({'Quantity': 'sum'}).reset_index()\n\n# merge with product_df\nresult = pd.merge(total_quantity, product_df, on='ProductID')\n\n# calculate TotalSales\nresult['TotalSales'] = result['Quantity'] * result['Price']\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = result[['ProductID', 'ProductName', 'Quantity', 'TotalSales']].rename(columns={'Quantity': 'TotalQuantity'})\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a project that involves processing a set of multi-dimensional feature vectors using PyTorch. Each feature vector is structured as a tensor of shape (batch_size, channels, height, width). Your task is to implement a method that applies a specific boolean mask tensor to the feature tensor to zero out certain elements conditionally, based on whether they meet a threshold.\n\nYou have the following tensors:\n\n- `features`: a tensor of shape (100, 3, 32, 32) representing a batch of 100 images with 3 channels and 32x32 pixels.\n- `mask`: a boolean tensor of the same shape, indicating which elements should be kept (True) or zeroed out (False).\n\nThe output should be a tensor of the same shape where elements of `features` are multiplied by the corresponding elements of `mask`. \n\nA:\n\n<code>\nimport numpy as np\nimport torch\nfeatures, mask = load_data()\n</code>\noutput = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\noutput = features * mask.float()\n</code>"
  },
  {
    "input": "Problem:\nIn a natural language processing task, you have a batch of embeddings from two different models, each with a shape of (batch_size, embedding_size). You want to calculate the cosine similarity between correspondingly indexed pairs of embeddings from these two batches and return a tensor of shape (batch_size,). You are using PyTorch for your implementation.\n\nA:\n<code>\nimport torch\n\nbatch_size = 5\nembedding_size = 10\nembeddings_model_a = torch.rand(batch_size, embedding_size)\nembeddings_model_b = torch.rand(batch_size, embedding_size)\n</code>\nsimilarity = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nsimilarity = torch.nn.functional.cosine_similarity(embeddings_model_a, embeddings_model_b)"
  },
  {
    "input": "Problem:\nYou have a dataset that contains the daily average temperatures in degrees Celsius for a city over a year, represented as a Pandas DataFrame. You want to preprocess the temperature data by first filling missing values using the forward fill method, then normalizing the data using Min-Max scaling so that the values range between 0 and 1. Finally, you need to calculate the average temperature for each month after the preprocessing step.\n\nA:\n<code>\nimport pandas as pd\n\n# Assuming `df` is your DataFrame and the column containing temperatures is named 'Temperature'\n# Fill missing values and normalize the temperature data\ndf['Temperature'] = df['Temperature'].fillna(method='ffill')\nnormalized_temperatures = (df['Temperature'] - df['Temperature'].min()) / (df['Temperature'].max() - df['Temperature'].min())\n\n# Now, you need to calculate the average temperature for each month\naverage_monthly_temp = normalized_temperatures.groupby(df['Date'].dt.month).mean()\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = average_monthly_temp.reset_index(name='Average_Temperature')</code>"
  },
  {
    "input": "## Problem\nYou have a dataset containing customer information that includes their last purchase amounts recorded in a DataFrame. You want to preprocess this data by normalizing the purchase amounts using Min-Max scaling, which will transform the amounts into a range between 0 and 1. \n\nThe dataset looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'customer_id': [1, 2, 3, 4, 5],\n    'last_purchase_amount': [150.00, 200.00, 50.00, 300.00, 250.00]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to create a new column called `scaled_purchase_amount` that holds the normalized values. Fill in the missing code below to achieve this:\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\npurchase_amounts = df[['last_purchase_amount']]\ndf['scaled_purchase_amount'] = scaler.fit_transform(purchase_amounts)\n\n# Fill in the missing code to transform these purchase amounts.\nresult = ...\n```\n### BEGIN SOLUTION\n",
    "output": "\n```python\nresult = df\n```"
  },
  {
    "input": "Problem:\nI have two pandas DataFrames representing sales data from two different regions, each containing a timestamp for when the data was recorded. The first DataFrame `sales_a` contains sales data from Region A, and the second DataFrame `sales_b` contains sales data from Region B. I want to merge these DataFrames such that for each timestamp in `sales_b`, I include the total sales from `sales_a` that occurred within a 5-minute window before that timestamp. If no sales are found in that time window, I want to record a `None` in the sales column.\n\nHere is the setup of the two DataFrames:\n\n```python\nimport pandas as pd\n\nsales_a = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:00:00', '2023-10-01 10:02:00', '2023-10-01 10:05:00', '2023-10-01 10:10:00'],\n    'TotalSales': [100, 200, 150, 300]\n})\n\nsales_b = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:01:00', '2023-10-01 10:06:00', '2023-10-01 10:11:00']\n})\n```\n\nNow, after merging, I want the resulting DataFrame to look like this:\n\n```\n+---------------------+------------+\n|      Timestamp      | TotalSales |\n+---------------------+------------+\n| 2023-10-01 10:01:00 |     100    |\n| 2023-10-01 10:06:00 |     200    |\n| 2023-10-01 10:11:00 |     150    |\n+---------------------+------------+\n```\n\nIf no sales were recorded in the 5-minute window, `TotalSales` should be `None`.\n\nTo achieve this, write the code that fills in the blank for the variable `result` below, ensuring that the merged DataFrame keeps the order of `sales_b` and aligns the sales data from `sales_a` correctly.\n\n```python\nimport pandas as pd\n\nsales_a = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:00:00', '2023-10-01 10:02:00', '2023-10-01 10:05:00', '2023-10-01 10:10:00'],\n    'TotalSales': [100, 200, 150, 300]\n})\n\nsales_b = pd.DataFrame({\n    'Timestamp': ['2023-10-01 10:01:00', '2023-10-01 10:06:00', '2023-10-01 10:11:00']\n})\n\n# Convert Timestamp columns to datetime\nsales_a['Timestamp'] = pd.to_datetime(sales_a['Timestamp'])\nsales_b['Timestamp'] = pd.to_datetime(sales_b['Timestamp'])\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = sales_b.copy()\nresult['TotalSales'] = result['Timestamp'].apply(\n    lambda x: sales_a[sales_a['Timestamp'] <= x].loc[\n        sales_a['Timestamp'] > (x - pd.Timedelta(minutes=5)), 'TotalSales'\n    ].sum() or None\n)\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing several numerical features and a categorical outcome. Your task is to preprocess the data: standardize the numerical features and one-hot encode the categorical feature. After preprocessing, you want to check the shape of the resultant dataset to ensure the transformation has been applied correctly.\n\nGiven the following code snippet to prepare your data:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = pd.DataFrame({\n    'Feature1': [1, 2, 3, 4, 5],\n    'Feature2': [10, 20, 30, 40, 50],\n    'Category': ['A', 'B', 'A', 'B', 'A']\n})\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), ['Feature1', 'Feature2']),\n        ('cat', OneHotEncoder(), ['Category'])\n    ])\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n```\n\nYou need to apply the transformations and obtain the shape of the new transformed dataset. How can you do this?\n\n```python\ntransformed_data = pipeline.fit_transform(data)\n```\n\nThe shape of the transformed dataset should be stored in a variable named `result_shape`. \n\nresult_shape = ... # put solution in this variable",
    "output": "\n```python\nresult_shape = transformed_data.shape\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset with various features and a target variable. I want to apply a standardization preprocessing step to my features and then validate the preprocessing by checking the mean and standard deviation of the transformed features. I have defined my features in a variable called `X` and now I want to standardize them using `StandardScaler` from `sklearn.preprocessing`. After standardizing, I need a specific line of code that computes the mean and standard deviation of the resulting scaled features. \n\nA:\n<code>\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nX = np.array([[1, 2], [3, 4], [5, 6]])\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</code>\nmean_std = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nmean_std = (X_scaled.mean(axis=0), X_scaled.std(axis=0))</code>"
  },
  {
    "input": "### Problem:\n\nI have a DataFrame containing e-commerce sales data as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'OrderID': [1, 2, 3, 4, 5],\n    'CustomerID': ['C001', 'C002', 'C001', 'C003', 'C002'],\n    'ProductID': ['P001', 'P002', 'P001', 'P003', 'P002'],\n    'Quantity': [1, 2, 3, 1, 2],\n    'Price': [10.0, 20.0, 10.0, 5.0, 20.0],\n    'OrderDate': ['2021-01-01', '2021-01-02', '2021-01-01', '2021-01-03', '2021-01-02']\n}\ndf = pd.DataFrame(data)\n```\n\nI want to achieve the following tasks:\n\n1. Normalize the `Price` column using min-max normalization so that its values range from 0 to 1.\n2. Create a multi-index DataFrame that organizes the sales by `CustomerID` and `OrderID`.\n3. Extract the normalized price along with the quantity and create a new DataFrame that shows:\n\n```\nCustomerID  OrderID  Quantity  NormalizedPrice\nC001        1       1         0.5\n           3       3         0.5\nC002        2       2         1.0\n           5       2         1.0\nC003        4       1         0.0\n```\n\nWhat is the code to achieve this?\n\n```python\nimport pandas as pd\n\ndata = {\n    'OrderID': [1, 2, 3, 4, 5],\n    'CustomerID': ['C001', 'C002', 'C001', 'C003', 'C002'],\n    'ProductID': ['P001', 'P002', 'P001', 'P003', 'P002'],\n    'Quantity': [1, 2, 3, 1, 2],\n    'Price': [10.0, 20.0, 10.0, 5.0, 20.0],\n    'OrderDate': ['2021-01-01', '2021-01-02', '2021-01-01', '2021-01-03', '2021-01-02']\n}\ndf = pd.DataFrame(data)\n\n# Normalize Price column\ndf['NormalizedPrice'] = (df['Price'] - df['Price'].min()) / (df['Price'].max() - df['Price'].min())\n\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>\ndf.set_index(['CustomerID', 'OrderID'], inplace=True)\nresult = df[['Quantity', 'NormalizedPrice']]\n</code>\n",
    "output": "\n<code>\ndf.set_index(['CustomerID', 'OrderID'], inplace=True)\nresult = df[['Quantity', 'NormalizedPrice']]\n</code>"
  },
  {
    "input": "Problem:\nYou are working with a DataFrame that contains sales data for various products across different regions. The DataFrame consists of product names, regions, sales figures, and dates. Your task is to create a pivot table from this DataFrame where the index comprises the unique product names, the columns represent regions, and the values are the total sales figures for each product by region. Within the pivot table, you want to fill any missing sales figures with zero.\n\nGiven the following code snippet, how would you fill in the blank to achieve this?\n\n```python\nimport pandas as pd\n\ndata = {\n    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget B', 'Widget A'],\n    'region': ['North', 'South', 'South', 'North', 'East'],\n    'sales': [100, 150, 200, 300, 100],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'])\n}\ndf = pd.DataFrame(data)\n\npivot_table = df.pivot_table(index='product', columns='region', values='sales', aggfunc='sum').fillna(0)\n\nresult = ...\n```\n",
    "output": "\n```python\nresult = pivot_table\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames representing sales data from different sources as follows:\n\nsales_df1:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:00:00 | Widget A |   10     |\n |  2     | 2023-01-01 11:00:00 | Widget B |   15     |\n |  3     | 2023-01-01 12:00:00 | Widget A |   5      |\n +--------+----------+----------+----------+\n```\n\nsales_df2:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:05:00 | Widget A |   7      |\n |  3     | 2023-01-01 12:05:00 | Widget A |   3      |\n |  4     | 2023-01-01 12:10:00 | Widget C |   8      |\n +--------+----------+----------+----------+\n```\n\nYour task is to merge these two DataFrames based on the `Product` column and ensure that in the final DataFrame, you include all the original records. If there are overlapping timestamps for the same product, sum the quantities from both DataFrames. Finally, sort the resulting DataFrame by `Timestamp`.\n\nAfter merging them efficiently, the resulting DataFrame should look like this:\n```\n +--------+----------+----------+----------+\n | OrderID| Timestamp| Product  | Quantity |\n +--------+----------+----------+----------+\n |  1     | 2023-01-01 10:00:00 | Widget A |   17     |\n |  2     | 2023-01-01 11:00:00 | Widget B |   15     |\n |  3     | 2023-01-01 12:00:00 | Widget A |   8      |\n |  4     | 2023-01-01 12:10:00 | Widget C |   8      |\n +--------+----------+----------+----------+\n```\n\nUse the following placeholders in your solution:\n```python\nimport pandas as pd\n\nsales_df1 = pd.DataFrame({\n    'OrderID': [1, 2, 3],\n    'Timestamp': pd.to_datetime(['2023-01-01 10:00:00', '2023-01-01 11:00:00', '2023-01-01 12:00:00']),\n    'Product': ['Widget A', 'Widget B', 'Widget A'],\n    'Quantity': [10, 15, 5]\n})\n\nsales_df2 = pd.DataFrame({\n    'OrderID': [1, 3, 4],\n    'Timestamp': pd.to_datetime(['2023-01-01 10:05:00', '2023-01-01 12:05:00', '2023-01-01 12:10:00']),\n    'Product': ['Widget A', 'Widget A', 'Widget C'],\n    'Quantity': [7, 3, 8]\n})\n\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = pd.concat([sales_df1, sales_df2]).groupby(['Product', 'Timestamp'], as_index=False).agg({'Quantity': 'sum'}).reset_index()\nresult['OrderID'] = range(1, len(result) + 1)\nresult = result[['OrderID', 'Timestamp', 'Product', 'Quantity']].sort_values(by='Timestamp').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nI have two dataframes that contain a list of products and their sales over time. The first dataframe `df_sales` contains columns for `product_id`, `date`, and `sales_amount`, while the second dataframe `df_details` contains columns for `product_id`, `product_name`, and `category`. I need to merge them based on `product_id`, ensuring that we also handle any missing product names from `df_details`. After that, I want to create a new column `sales_status` that categorizes sales into 'High' if sales are greater than 1000, 'Medium' if sales are between 500 and 1000, and 'Low' for sales below 500. Finally, I would like the resultant dataframe to be sorted by `date` in ascending order.\n\nHere are the two dataframes:\n\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3, 2],\n    'date': ['2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03'],\n    'sales_amount': [1000, 2000, 300, 500]\n})\n\ndf_details = pd.DataFrame({\n    'product_id': [1, 2, 3],\n    'product_name': ['Product A', 'Product B', None],\n    'category': ['Category 1', 'Category 2', 'Category 1']\n})\n```\n\nAfter merging, how can I generate the final dataframe as described? Use the following placeholder for the result:\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df_sales.merge(df_details, on='product_id', how='left')\nresult['sales_status'] = pd.cut(result['sales_amount'], bins=[-1, 500, 1000, float('inf')], labels=['Low', 'Medium', 'High'])\nresult = result.sort_values(by='date')\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame of users and their associated scores in multiple categories, and you want to compute the maximum score for each user while excluding any scores that are null (NaN). The DataFrame looks like this:\n\n```python\nimport pandas as pd\ndata = {\n    'user': ['Alice', 'Bob', 'Alice', 'Bob', 'Charlie'],\n    'category': ['A', 'A', 'B', 'B', 'A'],\n    'score': [95, None, 80, 100, None]\n}\ndf = pd.DataFrame(data)\n```\n\nThe desired output for each user should show the maximum score across all categories while discarding any null scores:\n\n```\n     user  max_score\n0   Alice        95.0\n1     Bob       100.0\n2 Charlie         0.0\n```\n\nHowever, your current attempt is not yielding the correct results. You want to replace any user's `max_score` with `0.0` if all their scores are null. Here\u2019s the code to modify:\n\n```python\nresult = df.groupby('user')['score'].apply(...).reset_index()\n```\n\nFill in the blank to derive the correct max score with the specified conditions.",
    "output": "\n```python\nresult = df.groupby('user')['score'].apply(lambda x: x.max() if x.notnull().any() else 0.0).reset_index(name='max_score')\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame containing sales data with some missing values. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'product': ['A', 'B', 'A', 'C', 'B', 'C'],\n    'sales': [200, np.nan, 150, 300, np.nan, 500],\n    'region': ['North', 'South', 'North', 'East', 'South', 'East']\n}\ndf = pd.DataFrame(data)\n```\n\nI want to calculate the total sales for each product, but if any sales value is missing (NaN), the total sales for that product should also be reported as NaN. \n\nI\u2019m trying to group by 'product' and sum the 'sales' column, but I cannot get the NaN handling to reflect properly. Here's what I initially attempted:\n\n```python\ndf.groupby('product')['sales'].sum()\n```\n\nWhat should I include to ensure that the presence of NaN values leads to a NaN result for that group's total sales?\n\nA:\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.groupby('product')['sales'].sum(min_count=1)\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for a company over a period of time. The DataFrame includes the columns 'Date', 'Region', 'Sales', and 'Returns'. Some sales entries are missing (NaN) and should be considered zero for calculations. You want to compute the net sales for each region, which is defined as the sum of sales minus the sum of returns. Additionally, you want to create a new column that categorizes the net sales as 'High', 'Medium', or 'Low' based on the following thresholds: High if net sales are above 1000, Medium if between 500 and 1000, and Low if below 500.\n\nGiven the DataFrame as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Date': ['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02'],\n    'Region': ['North', 'South', 'North', 'South'],\n    'Sales': [np.nan, 200, 300, 400],\n    'Returns': [50, np.nan, 20, 30]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to complete the following line of code that will produce a new DataFrame (`result`) which summarizes the net sales and their categories:\n\n```python\nresult = df.groupby('Region').apply(lambda x: pd.Series({\n    'Net_Sales': (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()),\n    'Sales_Category': 'High' if ... else 'Medium' if ... else 'Low'\n}))\n```\n\nFill in the blanks to correctly define the net sales calculations and category assignments.",
    "output": "\n```python\nresult = df.groupby('Region').apply(lambda x: pd.Series({\n    'Net_Sales': (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()),\n    'Sales_Category': 'High' if (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()) > 1000 else 'Medium' if (x['Sales'].fillna(0).sum() - x['Returns'].fillna(0).sum()) >= 500 else 'Low'\n}))\n```"
  },
  {
    "input": "Problem:\nI have a dataset containing various features and target variables. I would like to apply a log transformation to specific features in my dataset to stabilize variance and make the data more normally distributed. The features I want to transform are specified in a list called `features_to_transform`. After transforming these features, I wish to normalize the entire dataset using StandardScaler from scikit-learn to ensure that they are on a similar scale. Finally, I want to obtain the transformed and normalized features as a NumPy array.\n\nHere's the setup I have:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.DataFrame({\n    'feature1': [1, 2, 3, 4],\n    'feature2': [10, 20, 30, 40],\n    'feature3': [100, 200, 300, 400]\n})\nfeatures_to_transform = ['feature1', 'feature2']\n```\n\nThe final transformed and normalized features should be stored in the variable `transformed_data`. \n\nA:\n```python\nimport numpy as np\n\n# Log transform the specified features\ndata[features_to_transform] = np.log(data[features_to_transform])\n\n# Normalize the entire dataset\nscaler = StandardScaler()\ntransformed_data = ... # put solution in this variable\n```",
    "output": "\n```python\ntransformed_data = scaler.fit_transform(data)\n```"
  },
  {
    "input": "## Problem:\n\nYou have two tensors, `A` and `B`, representing feature vectors for a binary classification task. The shapes of these tensors are `(50, 128)` each, where `50` is the number of samples and `128` is the feature dimension. You want to compute a similarity score for each pair of vectors from `A` and `B` using the cosine similarity formula, and return a mask indicating which pairs have a similarity score greater than a given threshold, say `0.8`.\n\nFor the cosine similarity, you need to perform element-wise operations to compute the dot products and magnitudes. Your initial attempt using PyTorch's operations has been partially completed.\n\nHere is the beginning of your code:\n\n```python\nimport torch\n\nA = load_data_A()  # Shape: (50, 128)\nB = load_data_B()  # Shape: (50, 128)\nthreshold = 0.8\n\n# Compute the dot products\ndot_product = torch.mm(A, B.t())\n\n# Compute magnitudes\nnorm_A = torch.norm(A, dim=1).unsqueeze(1)\nnorm_B = torch.norm(B, dim=1).unsqueeze(0)\n\n# Compute cosine similarity\ncosine_similarity = dot_product / (norm_A * norm_B)\n\n# Now, you want to create a boolean mask based on the threshold\nmask = ...\n```\n\nYou need to fill in the blank to produce the `mask` tensor that has the same shape as `cosine_similarity` but contains `True` for elements greater than `0.8` and `False` otherwise.\n\nA:\n\n```python\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nmask = cosine_similarity > threshold\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset containing the sales records for a retail store, structured as a DataFrame with the following columns: 'date', 'store_id', 'product_id', 'units_sold', and 'revenue'. You want to analyze the monthly sales performance by aggregating the data.\n\nYour task is to create a new DataFrame that summarizes the total units sold and total revenue for each store on a monthly basis. \n\nThe DataFrame `df` looks something like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n    'store_id': [1, 2] * 50,\n    'product_id': [101, 102, 103] * 33 + [101],\n    'units_sold': [10, 20, 15, 30, 25] * 20,\n    'revenue': [100, 200, 150, 300, 250] * 20\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to complete the following code to achieve this aggregation, grouping the results by 'store_id' and 'month' (extracted from 'date'), and summing 'units_sold' and 'revenue':\n\n```python\ndf['month'] = df['date'].dt.to_period('M')\nmonthly_sales = df.groupby(['store_id', 'month'])[['units_sold', 'revenue']].sum().reset_index()\n```\n\nReset the index on the resulting DataFrame to ensure it is formatted correctly for final output. \n\nresult = ... # put solution in this variable",
    "output": "\n```python\nresult = monthly_sales.reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames E and F as follows:\nE\n    product_id   price    date\n0          101   250.0  2023-01-01\n1          102   150.0  2023-01-01\n2          103   300.0  2023-01-02\nF\n    product_id   price    date\n0          101   260.0  2023-01-02\n1          102   150.0  2023-01-02\n2          104   180.0  2023-01-03\n\n\nI need to merge these DataFrames on the `product_id` column while keeping all records from both DataFrames and ensuring that for the same `product_id`, the latest `date` is maintained in the merged DataFrame. The `price` value from the DataFrame F should replace the corresponding value in E where applicable. If a `product_id` exists in F but not in E, it should still be included. The final output should be sorted by `date` and `product_id` in ascending order.\n\nYour merging attempt using pd.merge() has resulted in duplicates and incorrect ordering. Here is my partial code:\n```python\nimport pandas as pd\n\nE = pd.DataFrame({\"product_id\": [101, 102, 103], \"price\": [250.0, 150.0, 300.0], \"date\": [\"2023-01-01\", \"2023-01-01\", \"2023-01-02\"]})\nF = pd.DataFrame({\"product_id\": [101, 102, 104], \"price\": [260.0, 150.0, 180.0], \"date\": [\"2023-01-02\", \"2023-01-02\", \"2023-01-03\"]})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = pd.concat([E, F]).sort_values(by=['date', 'product_id']).drop_duplicates(subset=['product_id'], keep='last').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing information about different products in a retail store. Each product has several attributes like category, price, and sales. You want to create a MultiIndex DataFrame using the Pandas library, where the first level of the index is the product category and the second level is the product name. You also want to normalize the sales data for each product within its category to ensure comparability.\n\nAssume you have a dictionary of data as follows:\n```python\ndata = {\n    'category': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Grocery'],\n    'product': ['Smartphone', 'Laptop', 'Sofa', 'Table', 'Rice'],\n    'price': [699.99, 1299.99, 499.99, 199.99, 29.99],\n    'sales': [150, 80, 20, 50, 200]\n}\n```\n\nYour task is to create a MultiIndex DataFrame from this dictionary, normalize the 'sales' column per category, and then return the resulting DataFrame. \n\nThe expected structure should look like this, where sales are normalized within their respective categories:\n```\n                     price  sales\ncategory    product             \nElectronics Smartphone  699.99  1.000\n            Laptop     1299.99  0.533\nFurniture   Sofa        499.99  0.400\n            Table      199.99  1.000\nGrocery     Rice        29.99  1.000\n```\n\nHow can you achieve this in Python?\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'category': ['Electronics', 'Electronics', 'Furniture', 'Furniture', 'Grocery'],\n    'product': ['Smartphone', 'Laptop', 'Sofa', 'Table', 'Rice'],\n    'price': [699.99, 1299.99, 499.99, 199.99, 29.99],\n    'sales': [150, 80, 20, 50, 200]\n}\n\ndf = pd.DataFrame(data)\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df.set_index(['category', 'product']).groupby(level=0).apply(lambda x: x.assign(sales=x['sales'] / x['sales'].max()))</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset that contains the sales information of a retail store over different months for various products. The data is structured in a multi-index DataFrame, with the first level of indexing as 'Product' and the second level as 'Month'. Here is the structure of the DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples(\n    [('A', '2023-01'), ('A', '2023-02'), ('B', '2023-01'), ('B', '2023-02')],\n    names=['Product', 'Month']\n)\ndata = np.random.randint(1, 100, size=(4,))\ndf = pd.DataFrame(data, index=index, columns=['Sales'])\n```\n\nTo prepare the data for analysis, you need to normalize the sales figures across each product so that each product's sales amount is expressed as a percentage of that product's total sales over the monitored months. You want the normalized sales to be stored back in the DataFrame with the same multi-index structure.\n\nComplete the code below to achieve this normalization, storing the result in the variable `normalized_sales`.\n\n```python\n# Calculate total sales per product\ntotal_sales = df.groupby(level='Product').sum()\n\n# Normalize sales\nnormalized_sales = df.div(total_sales).reset_index(level='Product', drop=True)\nnormalized_sales.columns = ['Normalized_Sales']\n```\n\nFill in the blank where you need to reassign the normalized sales to the correct index structure. \n\n```python\nfinal_df = df.copy()\nfinal_df['Normalized_Sales'] = ...\n```",
    "output": "\n```python\nfinal_df = df.copy()\nfinal_df['Normalized_Sales'] = normalized_sales\n```"
  },
  {
    "input": "Problem:\n\nI have a tensor in TensorFlow representing probabilities from a softmax layer, and I want to apply a boolean mask to zero out values below a certain threshold. The tensor shape is (32, 10), representing a batch of 32 samples with 10 classes. Simultaneously, I want to calculate the sum of the masked values along the class dimension. Here's how my current setup looks:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nprobs = tf.constant(np.random.rand(32, 10), dtype=tf.float32)\nthreshold = 0.2\n\n# I need to get the masked tensor and the sum of the masked values\nmasked_probs = ...\nsum_masked_probs = ...\n```\n\nCan you help me fill in the blanks for the variable assignments? Thanks!",
    "output": "\n```python\nmasked_probs = tf.where(probs >= threshold, probs, tf.zeros_like(probs))\nsum_masked_probs = tf.reduce_sum(masked_probs, axis=1)\n```"
  },
  {
    "input": "Problem:\nYou have a multi-index DataFrame representing sales data for various products over different months. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = pd.MultiIndex.from_tuples(\n    [('Product_A', 'January'), ('Product_A', 'February'), \n     ('Product_B', 'January'), ('Product_B', 'February')],\n    names=['Product', 'Month']\n)\ndata = {\n    'Sales': [1500, 1600, 1300, 1800],\n    'Expenses': [500, 700, 400, 600]\n}\ndf = pd.DataFrame(data, index=index)\n```\n\nYour goal is to normalize the 'Sales' column within each product (i.e., subtract the mean and divide by the standard deviation for each product). You need to add this normalized data to the DataFrame as a new column called 'Normalized_Sales'. The final DataFrame should maintain the multi-index structure.\n\nWhat code should replace the ellipsis to achieve this?\n\n```python\ndf['Normalized_Sales'] = ...\n```",
    "output": "\n```python\ndf['Normalized_Sales'] = df.groupby(level='Product')['Sales'].transform(lambda x: (x - x.mean()) / x.std())\n```"
  },
  {
    "input": "Problem:\nYou have a dataset containing information about customer purchases from an online retail store. The dataset is stored in a CSV file called `purchases.csv` and has the following columns: `customer_id`, `purchase_amount`, and `purchase_date`. You want to calculate the monthly purchasing trends by aggregating the total purchase amounts for each customer and each month. How can you get the aggregated totals for each customer per month in a new DataFrame?\n\nAssuming that you have already loaded the data into a DataFrame called `df`, please fill in the blank below to create the new aggregated DataFrame:\n\nA:\n```python\nimport pandas as pd\n\ndf = pd.read_csv('purchases.csv')\ndf['purchase_date'] = pd.to_datetime(df['purchase_date'])\ndf['month'] = df['purchase_date'].dt.to_period('M')\naggregated_df = df.groupby(['customer_id', 'month'])['purchase_amount'].sum().reset_index()\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = aggregated_df\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame that records sales data across different stores and products, and it contains some missing values. The DataFrame looks like this:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'store': ['Store A', 'Store A', 'Store B', 'Store B', 'Store C'],\n    'product': ['Apples', 'Bananas', 'Apples', 'Bananas', 'Apples'],\n    'sales': [100, np.nan, 200, 150, 300],\n    'returns': [10, 5, np.nan, 20, 25]\n})\n  \n  store   product  sales  returns\n0 Store A   Apples  100.0      10.0\n1 Store A  Bananas    NaN       5.0\n2 Store B   Apples  200.0       NaN\n3 Store B  Bananas  150.0      20.0\n4 Store C   Apples  300.0      25.0\n\nMy goal is to calculate the net sales for each store by subtracting the returns from the sales. If there are any NaN values in the sales or returns for a particular store-product combination, the net sales for that combination should also be NaN. \n\nI would like to create a new column 'net_sales' to store these results. \n\nA:\n<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'store': ['Store A', 'Store A', 'Store B', 'Store B', 'Store C'],\n    'product': ['Apples', 'Bananas', 'Apples', 'Bananas', 'Apples'],\n    'sales': [100, np.nan, 200, 150, 300],\n    'returns': [10, 5, np.nan, 20, 25]\n})\n</code>\ndf['net_sales'] = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['net_sales'] = df['sales'] - df['returns']\n</code>"
  },
  {
    "input": "Problem:\n\nYou have two datasets representing product sales. The first DataFrame, `df_sales`, contains records of sales transactions with potential duplicate entries:\n\n```python\ndf_sales = pd.DataFrame({\n    'product_id': [101, 102, 101, 103, 102, 101],\n    'quantity': [1, 2, 1, 1, 2, 1],\n    'sale_date': ['2023-10-01', '2023-10-01', '2023-10-02', '2023-10-01', '2023-10-02', '2023-10-03']\n})\n```\n\nThe second DataFrame, `df_products`, contains product information:\n\n```python\ndf_products = pd.DataFrame({\n    'product_id': [101, 102, 103],\n    'product_name': ['Widget A', 'Widget B', 'Widget C'],\n    'category': ['Gadgets', 'Gadgets', 'Gizmos']\n})\n```\n\nYour goal is to perform the following operations:\n\n1. Remove duplicate sales records from `df_sales` based on `product_id` and `sale_date`, and sum the `quantity` for duplicated rows.\n2. Merge the cleaned `df_sales` with `df_products` on `product_id`, ensuring that the resulting DataFrame retains the columns: `product_id`, `product_name`, `quantity`, and `sale_date`.\n3. Ensure the `sale_date` is formatted as a datetime object.\n4. Sort the resulting DataFrame by `sale_date` in ascending order.\n\nYour merged DataFrame should look like this:\n\n```\n   product_id product_name  quantity sale_date\n0         101     Widget A         3 2023-10-01\n1         102     Widget B         2 2023-10-01\n2         102     Widget B         2 2023-10-02\n3         103     Widget C         1 2023-10-01\n```\n\nGiven this context, what code would you write to achieve the desired result?\n\nA:\n```python\nimport pandas as pd\n\ndf_sales = pd.DataFrame({\n    'product_id': [101, 102, 101, 103, 102, 101],\n    'quantity': [1, 2, 1, 1, 2, 1],\n    'sale_date': ['2023-10-01', '2023-10-01', '2023-10-02', '2023-10-01', '2023-10-02', '2023-10-03']\n})\n\ndf_products = pd.DataFrame({\n    'product_id': [101, 102, 103],\n    'product_name': ['Widget A', 'Widget B', 'Widget C'],\n    'category': ['Gadgets', 'Gadgets', 'Gizmos']\n})\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf_sales_cleaned = df_sales.groupby(['product_id', 'sale_date'], as_index=False).sum()\ndf_sales_cleaned['sale_date'] = pd.to_datetime(df_sales_cleaned['sale_date'])\nresult = df_sales_cleaned.merge(df_products, on='product_id')[['product_id', 'product_name', 'quantity', 'sale_date']].sort_values(by='sale_date')\n```"
  },
  {
    "input": "Problem:\nYou have two DataFrames containing sales data for different products over time.\n\n`sales_df`:\n```\n   product_id    date  quantity\n0          A  2022-01-01       10\n1          A  2022-01-02       15\n2          B  2022-01-01       5\n3          B  2022-01-03       10\n4          C  2022-01-02       20\n```\n\n`promo_df`:\n```\n   product_id    date  discount\n0          A  2022-01-01         1\n1          B  2022-01-02         2\n2          B  2022-01-03         2\n3          C  2022-01-02         3\n4          D  2022-01-01         4\n```\n\nThe goal is to merge these two DataFrames such that each sale in `sales_df` has the corresponding discount from `promo_df` based on `product_id` and `date`. If there is no corresponding promotion for a sale date, fill the `discount` column with `0`. You also need to ensure the results are sorted by `product_id` and `date`.\n\nThe current merge attempt looks like this:\n```python\nmerged_df = pd.merge(sales_df, promo_df, on=['product_id', 'date'], how='outer')\n```\n\nHowever, you want to replace NaN values in the `discount` column with `0` afterwards.\n\nWhat is the correct line of code to replace the NaN values in the `discount` column of `merged_df` with `0`? \n\nProvide your answer as follows:\n```python\nmerged_df['discount'] = \n```\n",
    "output": "\n```python\nmerged_df['discount'] = merged_df['discount'].fillna(0)\n```"
  },
  {
    "input": "Problem:\n\nYou have a DataFrame containing customer purchase data with multi-level indices for 'CustomerID' and 'PurchaseDate'. The DataFrame looks like this:\n\n```\n                        Amount  Category\nCustomerID PurchaseDate                 \n1          2023-01-01      200   Electronics\n1          2023-01-02      150   Groceries\n2          2023-01-01      80    Clothing\n2          2023-01-03      120   Groceries\n3          2023-01-02      60    Electronics\n```\n\nThe 'Category' column is a categorical variable. You want to normalize the 'Amount' column and make sure you're treating 'Category' as a category, while simultaneously rearranging the DataFrame to have a simpler index. \n\nHow can you do this so that the resulting DataFrame has a new index consisting solely of 'CustomerID', with a single normalized 'Amount' column, and maintains 'Category' as a categorical variable? \n\nHere's the structure you have, and you need to fill in the appropriate manipulation:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Amount', 'Electronics'): [200, None, 60],\n    ('Amount', 'Groceries'): [150, 120, None],\n    ('Category', 'Electronics'): ['Electronics', None, 'Electronics'],\n    ('Category', 'Groceries'): [None, 'Groceries', None],\n}\n\nindex = pd.MultiIndex.from_tuples([(1, '2023-01-01'), (1, '2023-01-02'), (2, '2023-01-01'), (2, '2023-01-03'), (3, '2023-01-02')], names=['CustomerID', 'PurchaseDate'])\ndf = pd.DataFrame(data, index=index)\n\nresult = ... # put solution in this variable\n```\n\nYou need to fill in the missing part under `result =`.",
    "output": "\n```python\nresult = df.reset_index(level='PurchaseDate').groupby('CustomerID').agg({'Amount': 'sum', 'Category': 'first'}).reset_index()\nresult['Amount'] = (result['Amount'] - result['Amount'].min()) / (result['Amount'].max() - result['Amount'].min())\nresult['Category'] = result['Category'].astype('category')\n```"
  },
  {
    "input": "Problem:\nGiven a multi-index DataFrame that represents sales data for different products over various regions, structured as follows:\n\n```python\ndata = {\n    ('North', 'Product A'): [100, 150, 200],\n    ('North', 'Product B'): [80, 120, 160],\n    ('South', 'Product A'): [130, 170, 210],\n    ('South', 'Product B'): [90, 130, 190]\n}\ndf = pd.DataFrame(data, index=pd.date_range(start='2023-01-01', periods=3, freq='M'))\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Region', 'Product'])\n```\n\nYour task is to normalize the sales figures by dividing each sales value by the maximum sales value in its corresponding region, and return a DataFrame that maintains the same multi-index structure, but with normalized values. The resulting DataFrame's structure should look like this:\n\n```\n                  Region     Product A   Product B\n2023-01-31        North    0.5          0.5\n2023-02-28        North    0.75         0.75\n2023-03-31        North    1.0          1.0\n2023-01-31        South    0.6190       0.4737\n2023-02-28        South    0.7960       0.6842\n2023-03-31        South    1.0          1.0\n```\n\nPlease fill in the blank to complete this task:\n\n```python\nnormalized_df = ...  # put solution in this variable\n```",
    "output": "\n```python\nnormalized_df = df.div(df.groupby(level='Region', axis=1).transform('max'), axis=1)\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame containing sales data with some missing values and I want to compute a new column that indicates whether each sale is above or below the average for that month, excluding any null sales. The DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Month': ['January', 'February', 'March', 'April', 'May', 'June'],\n    'Sales': [200, None, 150, 300, 250, None]\n}\ndf = pd.DataFrame(data)\n```\n\nI have already attempted to compute the average sales with `df['Sales'].mean()`, but I'm struggling to integrate this into a new column with the desired output as follows:\n\n```\n      Month  Sales               Status\n0   January  200.0                Above\n1  February    NaN       Not Available\n2     March  150.0             Below Average\n3     April  300.0                Above\n4       May  250.0                Above\n5      June    NaN       Not Available\n```\n\nHow can I get the correct output?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'Month': ['January', 'February', 'March', 'April', 'May', 'June'],\n    'Sales': [200, None, 150, 300, 250, None]\n}\ndf = pd.DataFrame(data)\n</code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\ndf['Average'] = df['Sales'].mean()\ndf['Status'] = df['Sales'].apply(lambda x: 'Not Available' if pd.isna(x) else ('Above' if x > df['Average'] else 'Below Average'))\ndf = df.drop(columns=['Average'])\n</code>"
  },
  {
    "input": "Problem:\n\nYou are tasked with managing a dataset of high-dimensional embeddings, represented as a tensor in PyTorch with shape (batch_size, num_features). You need to filter this tensor to keep only the embeddings that meet a certain condition defined by a mask tensor. \n\nSuppose you have the following setup:\n\n```python\nimport torch\n\nbatch_size = 5\nnum_features = 10\nembeddings = torch.randn(batch_size, num_features)\nmask = torch.tensor([True, False, True, False, True])  # This mask indicates which rows to keep\n\n# Fill in the code below to filter the embeddings tensor using the mask\n```\n\nHow can you achieve this filtering and return the resulting tensor of embeddings that pass the mask condition?\n\nA:\n<code>\nfiltered_embeddings = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nfiltered_embeddings = embeddings[mask]\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products, including their categories and prices. The DataFrame looks like this:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Category': ['Electronics', 'Furniture', 'Electronics', 'Clothing', 'Clothing', 'Furniture'],\n    'Product': ['TV', 'Sofa', 'Laptop', 'T-shirt', 'Jeans', 'Table'],\n    'Price': [300, 800, 1200, 40, 60, 150]\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to normalize the 'Price' values within each product category to ensure that the data can be effectively analyzed, i.e., constrain the data values to a range between 0 and 1 within each category. Your goal is to add a new column 'Normalized_Price' to the existing DataFrame.\n\nHow can you achieve this?\n\n```python\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n```",
    "output": "\n<code>\ndf['Normalized_Price'] = df.groupby('Category')['Price'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n</code>"
  },
  {
    "input": "Problem:\nI have two DataFrames `sales` and `returns` as follows:\n`sales`\n```\n   order_id     product      date    revenue\n0       101    Widget A  2023-01-01      100\n1       102    Widget B  2023-01-02      150\n2       103    Widget C  2023-01-03      200\n3       104    Widget A  2023-01-03      120\n4       105    Widget B  2023-01-04      180\n```\n`returns`\n```\n   order_id     product      date    amount\n0       102    Widget B  2023-01-05       50\n1       104    Widget A  2023-01-06       60\n```\n\nI need to create a new DataFrame that merges `sales` and `returns` based on `order_id`. If a sale has a corresponding return, the revenue should be adjusted by subtracting the returned amount, but the returned records should be aligned with their original sale dates. After merging and adjusting, the resultant DataFrame should be sorted by date and include only the columns: `order_id`, `product`, and `adjusted_revenue`. If a product does not have any returns, its revenue should remain unchanged.\n\nThe expected output should look like this:\n```\n   order_id     product  adjusted_revenue\n0       101    Widget A              100\n1       102    Widget B              100\n2       103    Widget C              200\n3       104    Widget A               60\n4       105    Widget B              180\n```\n\nSo far, the result generated with a simple merge looks like this:\n```\n   order_id     product      date    revenue   amount\n0       101    Widget A  2023-01-01      100      NaN\n1       102    Widget B  2023-01-02      150     50.0\n2       103    Widget C  2023-01-03      200      NaN\n3       104    Widget A  2023-01-03      120     60.0\n4       105    Widget B  2023-01-04      180      NaN\n```\n\nThank you!\n\nA:\n<code>\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'order_id': [101, 102, 103, 104, 105],\n    'product': ['Widget A', 'Widget B', 'Widget C', 'Widget A', 'Widget B'],\n    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-03', '2023-01-04'],\n    'revenue': [100, 150, 200, 120, 180]\n})\n\nreturns = pd.DataFrame({\n    'order_id': [102, 104],\n    'product': ['Widget B', 'Widget A'],\n    'date': ['2023-01-05', '2023-01-06'],\n    'amount': [50, 60]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmerged = sales.merge(returns, on='order_id', how='left')\nmerged['adjusted_revenue'] = merged['revenue'] - merged['amount'].fillna(0)\nresult = merged[['order_id', 'product', 'adjusted_revenue']].sort_values(by='date').reset_index(drop=True)\n</code>"
  },
  {
    "input": "Problem:\nI have two pandas DataFrames that represent sales data and returns data. The sales DataFrame contains the following columns: 'order_id', 'customer_id', 'date', 'amount'. The returns DataFrame contains 'order_id', 'return_date', and 'amount_returned'. I need to merge these two DataFrames on 'order_id', ensuring that if an order has been returned, the 'amount' in the sales DataFrame is updated to reflect the return. Additionally, I want to keep the original columns intact while adding a new 'final_amount' column that shows the amount after returns have been deducted.\n\nHere are my DataFrames:\n\nsales_df:\n   order_id  customer_id       date  amount\n0        1           A 2023-01-01     100\n1        2           B 2023-01-02     150\n2        3           A 2023-01-03     200\n3        4           C 2023-01-04     300\n\nreturns_df:\n   order_id return_date  amount_returned\n0        2  2023-01-05               50\n1        3  2023-01-06              100\n\nI want the final DataFrame to look like this:\n   order_id  customer_id       date  amount  final_amount\n0        1           A 2023-01-01     100          100\n1        2           B 2023-01-02     150          100\n2        3           A 2023-01-03     200          100\n3        4           C 2023-01-04     300          300\n\nHow can I accomplish this with pandas? \n\nA:\n<code>\nimport pandas as pd\n\nsales_df = pd.DataFrame({\n    'order_id': [1, 2, 3, 4],\n    'customer_id': ['A', 'B', 'A', 'C'],\n    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']),\n    'amount': [100, 150, 200, 300]\n})\n\nreturns_df = pd.DataFrame({\n    'order_id': [2, 3],\n    'return_date': pd.to_datetime(['2023-01-05', '2023-01-06']),\n    'amount_returned': [50, 100]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmerged_df = sales_df.merge(returns_df, on='order_id', how='left')\nmerged_df['final_amount'] = merged_df['amount'] - merged_df['amount_returned'].fillna(0)\nresult = merged_df[['order_id', 'customer_id', 'date', 'amount', 'final_amount']]\n</code>"
  },
  {
    "input": "Problem:\n\nI am working with a dataset of user ratings for various movies, represented as a tensor where rows correspond to users and columns correspond to movies. I need to compute a similarity score based on cosine similarity between two users, defined as the dot product of their rating vectors divided by the product of their magnitudes. I have extracted the ratings for two users as follows:\n\nimport torch\n\nratings = load_data()  # This returns a tensor of shape (num_users, num_movies)\nuser_a = ratings[0]    # Ratings for user A\nuser_b = ratings[1]    # Ratings for user B\n\nHowever, I\u2019m struggling with the following calculation to obtain the cosine similarity score between user A and user B. \n\nsimilarity_score = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nsimilarity_score = torch.dot(user_a, user_b) / (torch.norm(user_a) * torch.norm(user_b))\n</code>"
  },
  {
    "input": "Problem:\nI have a dataset from a customer review platform that includes ratings from 1 to 5, and I want to preprocess this data to prepare it for a machine learning model. I need to normalize the ratings using Z-score normalization, which involves subtracting the mean from each rating and then dividing by the standard deviation. Given the ratings in a NumPy array, how can I perform this transformation in a single line of code?\n\nA:\n<code>\nimport numpy as np\nratings = np.array([1, 2, 3, 4, 5])\nnormalized_ratings = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nnormalized_ratings = (ratings - np.mean(ratings)) / np.std(ratings)\n</code>"
  },
  {
    "input": "Problem:\nYou are given two DataFrames, `sales` and `returns`:\n\n`sales`\n```\n+----------+------------+-------+\n|  OrderID |   Date     | Amount|\n+----------+------------+-------+\n|    1001  | 2020-01-10 |  200  |\n|    1002  | 2020-01-15 |  150  |\n|    1003  | 2020-02-05 |  300  |\n|    1004  | 2020-02-07 |  250  |\n+----------+------------+-------+\n```\n\n`returns`\n```\n+----------+-----------+-------+\n|  OrderID |   Date    | Amount|\n+----------+-----------+-------+\n|    1002  | 2020-01-20|   50  |\n|    1003  | 2020-02-15|   100 |\n|    1005  | 2020-02-20|   150 |\n+----------+-----------+-------+\n```\n\nYou need to create a new DataFrame that combines `sales` and `returns` based on `OrderID`. Ensure that:\n\n1. For each `OrderID` present in both DataFrames, you'll want to subtract the amount in `returns` from the corresponding `sales` amount.\n2. Any `OrderID` that is only in `sales` should still be included with the original sales amount and a value of `0` for returns.\n3. Any `OrderID` that is only in `returns` should be displayed with its amount, but should not contribute to the combined sales total.\n\nThe resulting DataFrame should have the following format: \n\n```\n+----------+------------+-------+-------+\n|  OrderID |   Sales    | Returns| Net   |\n+----------+------------+-------+-------+\n|    1001  |    200     |   0   |  200  |\n|    1002  |    150     |  50   |  100  |\n|    1003  |    300     |  100  |  200  |\n|    1004  |    250     |   0   |  250  |\n|    1005  |    0      |  150  | -150  |\n+----------+------------+-------+-------+\n```\n\nUse the following code to set up the DataFrames:\n\n```\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'OrderID': [1001, 1002, 1003, 1004],\n    'Date': pd.to_datetime(['2020-01-10', '2020-01-15', '2020-02-05', '2020-02-07']),\n    'Amount': [200, 150, 300, 250]\n})\n\nreturns = pd.DataFrame({\n    'OrderID': [1002, 1003, 1005],\n    'Date': pd.to_datetime(['2020-01-20', '2020-02-15', '2020-02-20']),\n    'Amount': [50, 100, 150]\n})\n\nresult = ... # put your solution here\n```",
    "output": "\n```python\nresult = pd.merge(sales, returns, on='OrderID', how='outer', suffixes=('_Sales', '_Returns')).fillna(0)\nresult['Sales'] = result['Amount_Sales'].astype(int)\nresult['Returns'] = result['Amount_Returns'].astype(int)\nresult['Net'] = result['Sales'] - result['Returns']\nresult = result[['OrderID', 'Sales', 'Returns', 'Net']]\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data for different products across several months. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'Product': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'Month': ['2022-01', '2022-02', '2022-01', '2022-02', '2022-01', '2022-02'],\n    'Sales': [100, 150, np.nan, 200, 300, np.nan]\n})\n```\n\nYour task is to calculate the total sales for each product, replacing any missing sales values in a month with the average sales of that product across all months. The expected output should show each product with the corrected sales total, and the DataFrame should have an additional column 'Adjusted_Sales' that reflects the total sales after this adjustment.\n\nThe resulting DataFrame should appear like this:\n\n```\n  Product     Month  Sales  Adjusted_Sales\n0       A  2022-01  100.0            250.0\n1       A  2022-02  150.0            250.0\n2       B  2022-01    NaN            200.0\n3       B  2022-02  200.0            200.0\n4       C  2022-01  300.0            300.0\n5       C  2022-02    NaN            300.0\n```\n\nFill in the blank:\n```python\ndf['Adjusted_Sales'] = ...\n```",
    "output": "\n```python\ndf['Adjusted_Sales'] = df.groupby('Product')['Sales'].transform(lambda x: x.fillna(x.mean()))\n```"
  },
  {
    "input": "Problem:\n\nI have a batch of data represented as a PyTorch tensor that contains embedding vectors. The tensor `data` has dimensions (batch_size, embedding_dim) and I'm trying to compute the cosine similarity between each pair of embedding vectors in the batch. However, I want to focus on only those pairs that are from a specific category, indicated by a binary mask tensor. The mask tensor `category_mask` has the same batch_size and is of type `torch.BoolTensor`, where a value of `1` indicates the vectors to consider. \n\nHere is the code I've set up:\n\n```python\nimport torch\n\n# Sample data\ndata = torch.rand((5, 10))  # 5 embeddings of dimension 10\ncategory_mask = torch.BoolTensor([1, 0, 1, 0, 1])  # Select specific embeddings\n\n# Now I want to calculate the cosine similarity between the selected embeddings \n# and each of them in a pair-wise fashion. \nsimilarity_matrix = torch.nn.functional.cosine_similarity(data[category_mask], \n                                                           dim=1, \n                                                           eps=1e-8)\n\n# I want to compute a squared similarity score between the pairs of selected embeddings.\nsquared_similarity = ... # put solution in this variable\n```\n\nWhat should I put in `squared_similarity` to achieve this?",
    "output": "\n```python\nsquared_similarity = torch.matmul(similarity_matrix.unsqueeze(1), similarity_matrix.unsqueeze(0)) ** 2\n```"
  },
  {
    "input": "### Problem\n\nYou have been given a 3D tensor representing image data in the shape (batch_size, channels, height, width). You want to apply a boolean mask to filter out specific pixels across all images based on a condition. The condition is that pixel values should be greater than a certain threshold. The remaining values in the tensor should be set to zero. Write a function that takes the tensor and the threshold as inputs, applies the mask, and returns the modified tensor.\n\nExample input:\n```python\nimport torch\n\ntensor = torch.tensor([[[0.1, 0.5, 0.3],\n                         [0.7, 0.2, 0.6]],\n                        \n                        [[0.4, 0.9, 0.1],\n                         [0.8, 0.3, 0.5]]])\nthreshold = 0.5\n```\nThe desired output for the input above, after applying the threshold mask, should look like this:\n```python\ntensor([[ [0.0, 0.0, 0.0],\n          [0.7, 0.0, 0.6]],\n          \n         [ [0.0, 0.9, 0.0],\n          [0.8, 0.0, 0.5]]])\n```\n\nA:\n\n<code>\nimport torch\n\ntensor, threshold = load_data()\n</code>\nresult_tensor = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult_tensor = torch.where(tensor > threshold, tensor, torch.tensor(0.0))\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing customer transaction data, which includes columns for 'Customer_ID', 'Transaction_Amount', and 'Transaction_Date'. Some transactions may have missing amounts indicated by `pd.NA`. Your task is to create a new DataFrame that shows each customer\u2019s total transaction amount along with their average transaction amount, but only for those transactions where the amount is greater than zero. The resulting DataFrame should also handle missing values by excluding them from the calculations. \n\nHere is a sample DataFrame:\n```python\nimport pandas as pd\n\ndata = {\n    'Customer_ID': [1, 1, 2, 2, 3, 3, 3],\n    'Transaction_Amount': [100.0, pd.NA, 50.0, 0.0, pd.NA, 75.0, 25.0],\n    'Transaction_Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04',\n                         '2023-01-05', '2023-01-06', '2023-01-07']\n}\ndf = pd.DataFrame(data)\n```\n\nYou want to achieve this final output DataFrame:\n```\n   Customer_ID  Total_Amount  Average_Amount\n0            1          100.0           100.0\n1            2           50.0            50.0\n2            3          100.0            50.0\n```\n\nFill in the blank below to compute the desired DataFrame:\n```python\nresult = df.groupby('Customer_ID').agg(\n    Total_Amount=('Transaction_Amount', lambda x: x[x > 0].sum()),\n    Average_Amount=('Transaction_Amount', lambda x: x[x > 0].mean())\n)\n```\nresult = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = df.groupby('Customer_ID').agg(\n    Total_Amount=('Transaction_Amount', lambda x: x[x > 0].sum()),\n    Average_Amount=('Transaction_Amount', lambda x: x[x > 0].mean())\n).reset_index()\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset containing information about customers' purchases, including purchase amounts, customer demographics, and timestamps of the transactions. I need to perform preprocessing on this dataset to prepare it for a machine learning model. Specifically, I want to:\n\n1. Normalize the 'purchase_amount' column using MinMax scaling.\n2. Convert the 'timestamp' column to a datetime object and extract the year as a new feature.\n3. One-hot encode the 'gender' column.\n\nGiven the following DataFrame named `df`:\n\n```python\nimport pandas as pd\n\ndata = {\n    'purchase_amount': [100.0, 150.5, 200.75, 130.0],\n    'gender': ['M', 'F', 'F', 'M'],\n    'timestamp': ['2021-05-01', '2021-05-02', '2021-05-03', '2021-05-04']\n}\n\ndf = pd.DataFrame(data)\n```\n\nI want to use MinMaxScaler for normalization and then create the new column and one-hot encode the gender. I have already imported `MinMaxScaler` from `sklearn.preprocessing`. Write the code to apply MinMax scaling to the 'purchase_amount' column and create the new one-hot encoded DataFrame.\n\nFill in the blank for the following code:\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf['normalized_purchase'] = scaler.fit_transform(df[['purchase_amount']])\n\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['year'] = df['timestamp'].dt.year\n\none_hot = pd.get_dummies(df['gender'], prefix='gender')\nfinal_df = pd.concat([df, one_hot], axis=1)\n```\n\nThe missing piece of information is the correct way to adjust `final_df` such that it contains the new features without the original categorical column. \n\nHow do I update `final_df` to drop the original 'gender' column?\n \nA:\n\n<code>\nfinal_df = final_df.drop(columns=['gender'])  # put solution in this variable\n</code>",
    "output": "\n<code>\nfinal_df = final_df.drop(columns=['gender'])  # put solution in this variable\n</code>"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing information about sales data, structured as follows:\n\n| product_id | region   | sales      |\n|------------|----------|------------|\n| 1          | North    | 100        |\n| 2          | South    | 150        |\n| 3          | East     | 90         |\n| 1          | West     | <NA>      |\n| 2          | North    | 200        |\n| 3          | South    | NaN        |\n| 1          | East     | 110        |\n| 2          | West     | 160        |\n| 3          | North    | <NA>      |\n\nYour goal is to calculate the average sales per product_id for each region and fill any missing values with the overall average sales across all products. The desired output is a DataFrame that shows the average sales with no missing values.\n\nThe current code you are working with is:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product_id': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North'],\n    'sales': [100, 150, 90, pd.NA, 200, None, 110, 160, pd.NA]\n})\n\nresult = df.groupby(['product_id', 'region']).mean()\n```\n\nHowever, you are encountering issues with handling null values and cannot directly perform operations that include those null entries. How can you modify your code to fill in the missing values correctly and achieve the desired result?\n\nA:\n<code>\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['sales'] = df['sales'].astype('float')  # Ensure sales are float for mean calculation\nresult = df.groupby(['product_id', 'region']).mean().fillna(df['sales'].mean())\n</code>\n"
  },
  {
    "input": "Problem:\n\nYou have two DataFrames, sales and inventory, structured as follows:\n\nsales:\n```\n   product_id   date    sales\n0           1  2023-01-01     100\n1           2  2023-01-01      50\n2           1  2023-01-02     150\n3           3  2023-01-02      80\n```\n\ninventory:\n```\n   product_id restock_date  stock\n0           1   2023-01-05     200\n1           2   2023-01-03     100\n2           3   2023-01-04     60\n3           4   2023-01-06     50\n```\n\nYou need to merge these two DataFrames such that:\n1. All rows from sales are retained.\n2. Missing inventory information (where `sales.product_id` does not match `inventory.product_id`) should be filled with `NaN`.\n3. Ensure the date formats in the final DataFrame are in the format \"DD-Mon-YYYY\".\n\nThe output DataFrame should look like this:\n\n```\n   product_id       date  sales restock_date  stock\n0           1  01-Jan-2023     100   2023-01-05   200\n1           2  01-Jan-2023      50   2023-01-03   100\n2           1  02-Jan-2023     150   2023-01-05   200\n3           3  02-Jan-2023      80   2023-01-04    60\n```\n\nYou attempt the merge but receive a DataFrame that does not format the dates correctly. Your task is to write the code that combines these DataFrames while applying the necessary transformations for the dates.\n\nHere is your starting point:\n```python\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'product_id': [1, 2, 1, 3],\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'sales': [100, 50, 150, 80]\n})\n\ninventory = pd.DataFrame({\n    'product_id': [1, 2, 3, 4],\n    'restock_date': ['2023-01-05', '2023-01-03', '2023-01-04', '2023-01-06'],\n    'stock': [200, 100, 60, 50]\n})\n```\n\nYour solution should be assigned to `result`:\n```python\nresult = ...  # put your solution in this variable\n```",
    "output": "\n```python\nresult = pd.merge(sales, inventory, on='product_id', how='left')\nresult['date'] = pd.to_datetime(result['date']).dt.strftime('%d-%b-%Y')\nresult['restock_date'] = pd.to_datetime(result['restock_date']).dt.strftime('%Y-%m-%d')\n```"
  },
  {
    "input": "## Problem\nYou have a DataFrame that contains sales data for different products over several months. The DataFrame is structured as follows:\n\n```plaintext\n         Product  Jan  Feb  Mar  Apr  May  Jun\n0        A       10  15   0   0   25  30\n1        B       20   0   0   10  15  20\n2        C        0  25  30  0   5   0\n3        D       40   5   0   0  10  12\n```\n\nThe zeros represent months with no sales data, and you want to calculate the rolling sum of sales for each product, excluding any months with zero sales. For each product, if there are consecutive non-zero sales, they should be included in the sum, while skipping zero values.\n\nYour task is to write the code to generate a new DataFrame that replaces each product's sales for each month with this calculated rolling sum. Please create your DataFrame with the following code:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Product': ['A', 'B', 'C', 'D'],\n    'Jan': [10, 20, 0, 40],\n    'Feb': [15, 0, 25, 5],\n    'Mar': [0, 0, 30, 0],\n    'Apr': [0, 10, 0, 0],\n    'May': [25, 15, 5, 10],\n    'Jun': [30, 20, 0, 12]\n}\ndf = pd.DataFrame(data)\n```\n\nFill in the blank with the appropriate code to compute the rolling sum while excluding zeros:\n\n```python\nrolling_sum = df.set_index('Product').rolling(window=len(df.columns)-1, min_periods=1, axis=1).apply(lambda x: x[x != 0].sum(), raw=False)\n```\n\nMake sure your final DataFrame looks like this when printed:\n\n```plaintext\n         Product  Jan  Feb  Mar  Apr  May  Jun\n0        A       10  25   25  25  50  80\n1        B       20  20   20  30  45  65\n2        C        0  25  55  55  60  60\n3        D       40  45  45  45  55  67\n```\n\nYour task is to complete the code where indicated.",
    "output": "\n```python\nrolling_sum = df.set_index('Product').rolling(window=len(df.columns)-1, min_periods=1, axis=1).apply(lambda x: x[x != 0].sum(), raw=False).reset_index()\nrolling_sum.columns = df.columns\nprint(rolling_sum)\n```"
  },
  {
    "input": "Problem:\nI have a Pandas DataFrame which contains information about sales transactions with hierarchical column names representing different categories. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    ('Store', 'North'): [100, 150, 200],\n    ('Store', 'South'): [200, 250, 300],\n    ('Store', 'East'): [300, 350, 400],\n    ('Store', 'West'): [400, 450, 500],\n    ('Returns', 'North'): [10, 15, 20],\n    ('Returns', 'South'): [20, 25, 30],\n    ('Returns', 'East'): [30, 35, 40],\n    ('Returns', 'West'): [40, 45, 50]\n}\n\ndf = pd.DataFrame(data, index=['Q1', 'Q2', 'Q3'])\n```\n\nI want to calculate the total sales after accounting for returns and create a new multi-index DataFrame that shows these totals under a new level called `Total` with two sub-levels: `Store` and `Returns`. The output DataFrame should look like this:\n\n```\n           Store          Returns\n           North South East West\nTotal\nQ1        90    180  270  360\nQ2       135    225  315  405\nQ3       180    270  360  450\n```\n\nHow can I create this new DataFrame? Please provide the code to fill in the missing part.\n\nA:\n```python\ntotals = ... # put solution in this variable\n```",
    "output": "\n```python\ntotals = pd.DataFrame({\n    ('Total', 'Store'): df[('Store', '')].sum(axis=1) - df[('Returns', '')].sum(axis=1),\n    ('Total', 'Returns'): df[('Returns', '')].sum(axis=1)\n}).set_index(df.index)\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset containing customer purchase information for an e-commerce platform. The dataset consists of the following columns: `customer_id`, `purchase_amount`, `purchase_date`, and `product_category`. You need to preprocess the data to perform a time series analysis of monthly sales. Specifically, you want to fill in any missing months in the time series with a total sales amount of zero for that month and return the modified DataFrame.\n\nAssuming you have a DataFrame `df` created from your dataset that looks like this:\n\n```\n   customer_id  purchase_amount purchase_date product_category\n0            1              120    2023-01-05        Electronics\n1            2              200    2023-02-15        Clothing\n2            1              300    2023-03-10        Electronics\n3            3              150    2023-03-20        Home\n```\n\nPerform the preprocessing to ensure that every month in the year 2023 is present in the DataFrame, with any months not present initialized to a `purchase_amount` of 0. Place the code that achieves this in the variable `monthly_sales`.\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'customer_id': [1, 2, 1, 3],\n    'purchase_amount': [120, 200, 300, 150],\n    'purchase_date': pd.to_datetime(['2023-01-05', '2023-02-15', '2023-03-10', '2023-03-20']),\n    'product_category': ['Electronics', 'Clothing', 'Electronics', 'Home']\n}\n\ndf = pd.DataFrame(data)\n</code>\nmonthly_sales = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nmonthly_sales = df.resample('M', on='purchase_date').sum().reindex(pd.date_range(start='2023-01-01', end='2023-12-31', freq='M'), fill_value=0)"
  },
  {
    "input": "Problem:\nI am working with a TensorFlow model that outputs a tensor of shape (batch_size, num_classes), where each row contains the logits for a binary classification task across different classes. I want to calculate the dot product of this tensor with a weight tensor of shape (num_classes, embedding_dim) to project the output to an embedding space. Additionally, I need to apply a boolean mask to selectively include only classes that should be evaluated during this transformation. \n\nGiven the logit tensor `logits` and the weight tensor `weights`, how can I compute the resulting projected tensor, ensuring that only the classes specified in the mask (a boolean tensor of shape (batch_size, num_classes)) are included in the calculation? \n\nExample input:\n```python\nlogits = tf.constant([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])  # shape (2, 3)\nweights = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])  # shape (3, 2)\nmask = tf.constant([[True, False, True], [False, True, True]])  # shape (2, 3)\n```\n\nA:\n<code>\nimport tensorflow as tf\n\nlogits = tf.constant([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nweights = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\nmask = tf.constant([[True, False, True], [False, True, True]])\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nmasked_logits = tf.where(mask, logits, tf.zeros_like(logits))\nresult = tf.matmul(masked_logits, weights)\n</code>"
  },
  {
    "input": "Problem:\nYou have two DataFrames, sales and returns, with the following data:\n\nsales:\n``` \n   order_id product_id quantity sold_date\n0        1        A         10  2023-01-01\n1        2        B         20  2023-01-02\n2        3        A         15  2023-01-03\n3        4        C         25  2023-01-04\n```\n\nreturns:\n```\n   order_id product_id quantity return_date\n0        1        A          5  2023-01-05\n1        3        A          10  2023-01-06\n2        5        D          7   2023-01-07\n```\n\nYou need to merge these two DataFrames such that you calculate the total quantity sold (subtracting any returns) for each product. The resulting DataFrame should contain the following columns: product_id, total_sold, and last_sold_date. The 'total_sold' should reflect the total quantity sold minus the quantity returned. The 'last_sold_date' should keep the maximum (latest) sold_date for each product.\n\nYour expected DataFrame should look like this:\n```\n  product_id total_sold last_sold_date\n0          A         15    2023-01-03\n1          B         20    2023-01-02\n2          C         25    2023-01-04\n3          D          0    NaT\n```\n\nYou tried using the following code:\n```\nresult = pd.merge(sales, returns, on='product_id', how='outer')\n```\n\nHowever, this did not give you the correct 'total_sold' and 'last_sold_date'.\n\nHow can you achieve the expected result in one line of code?\nA:\n```python\nimport pandas as pd\nsales = pd.DataFrame({'order_id': [1, 2, 3, 4], \n                      'product_id': ['A', 'B', 'A', 'C'], \n                      'quantity': [10, 20, 15, 25], \n                      'sold_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']})\nreturns = pd.DataFrame({'order_id': [1, 3, 5], \n                        'product_id': ['A', 'A', 'D'], \n                        'quantity': [5, 10, 7], \n                        'return_date': ['2023-01-05', '2023-01-06', '2023-01-07']})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```python\n\n\n```",
    "output": "\n```python\nresult = sales.groupby('product_id').agg(total_sold=('quantity', 'sum'), last_sold_date=('sold_date', 'max')).reset_index().merge(returns.groupby('product_id').agg(total_return=('quantity', 'sum')).reset_index(), on='product_id', how='outer').fillna(0).assign(total_sold=lambda x: x['total_sold'] - x['total_return']).drop(columns='total_return')\n```"
  },
  {
    "input": "Problem:\nI'm using PyTorch 1.12.0. I have a tensor representing user-item ratings, where missing ratings are indicated by NaN values. I want to compute the cosine similarity between users' ratings, only considering the ratings that are not NaN. How can I achieve this?\n\nA:\n<code>\nimport torch\nimport numpy as np\n\nratings = torch.tensor([[5.0, 3.0, np.nan, 1.0],\n                        [4.0, np.nan, np.nan, 1.0],\n                        [1.0, 1.0, np.nan, 5.0],\n                        [np.nan, 1.0, 5.0, 4.0],\n                        [np.nan, 2.0, 1.0, 4.0]])\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = torch.nn.functional.cosine_similarity(ratings[~torch.isnan(ratings)].view(-1, ratings.size(1)), dim=0)"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame that contains the sales data of different products across multiple months. The DataFrame has columns 'product_id', 'month', 'sales', and 'returns', where 'returns' can contain null values. Your goal is to calculate the net sales by subtracting the returns from the sales and also to categorize the net sales into performance tiers: \"poor\" for negative or zero net sales, \"average\" for net sales between 1 and 100, and \"excellent\" for net sales above 100. \n\nThe initial DataFrame is as follows:\n```python\ndf = pd.DataFrame({\n    'product_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02'],\n    'sales': [150, 200, 50, 70, 300, 0],\n    'returns': [10, pd.NA, 5, 20, pd.NA, 5]\n})\n```\n\nYou attempted to calculate the net sales and categorize them but encountered issues when handling the null values in the returns column. Here is an incomplete approach you tried:\n```python\ndf['net_sales'] = df['sales'] - df['returns']\ndf['performance'] = df['net_sales'].apply(lambda x: 'excellent' if x > 100 else ('average' if x > 0 else 'poor'))\n```\n\nHowever, this code resulted in errors due to null values interfering with the calculations. \n\nA:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'product_id': ['A', 'A', 'B', 'B', 'C', 'C'],\n    'month': ['2023-01', '2023-02', '2023-01', '2023-02', '2023-01', '2023-02'],\n    'sales': [150, 200, 50, 70, 300, 0],\n    'returns': [10, pd.NA, 5, 20, pd.NA, 5]\n})\n```\ndf = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf['returns'] = df['returns'].fillna(0)\ndf['net_sales'] = df['sales'] - df['returns']\ndf['performance'] = df['net_sales'].apply(lambda x: 'excellent' if x > 100 else ('average' if x > 0 else 'poor'))\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame structured as follows:\n```\n   employee_id   department   salary   bonus\n0           1          HR    55000     NaN\n1           2          IT    75000    5000\n2           3          HR    62000    3000\n3           4          IT    82000    NaN\n4           5          HR      NaN    2000\n5           6          IT    70000    4000\n```\n\nI want to calculate the total compensation for each employee, which is the sum of their salary and bonus. However, if the salary or bonus is null, it should be treated as 0 for the calculation. After that, I would like to get the average total compensation for each department.\n\nWhat code should I use to accomplish this task?\n\nA:\n<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'employee_id': [1, 2, 3, 4, 5, 6],\n                   'department': ['HR', 'IT', 'HR', 'IT', 'HR', 'IT'],\n                   'salary': [55000, 75000, 62000, 82000, None, 70000],\n                   'bonus': [None, 5000, 3000, None, 2000, 4000]})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nresult = df.fillna(0).assign(total_compensation=lambda x: x['salary'] + x['bonus']).groupby('department')['total_compensation'].mean()"
  },
  {
    "input": "Problem:\n\nI have a multi-dimensional tensor of shape (batch_size, channels, height, width) representing an image dataset. I want to compute the mean of each channel while ignoring any NaN values present in the tensor. Specifically, for each channel, I want to create a tensor that contains the mean value of that channel, considering only non-NaN values from the entire batch.\n\nHere's a sample tensor:\n\n```python\ntensor = torch.tensor([[[[1.0, 2.0, float('nan')], \n                          [4.0, 5.0, 6.0]],\n\n                         [[7.0, 8.0, float('nan')], \n                          [10.0, float('nan'), 12.0]]],\n\n                        [[[float('nan'), 2.0, 3.0], \n                          [float('nan'), 5.0, 6.0]],\n\n                         [[7.0, float('nan'), 9.0], \n                          [10.0, 11.0, 12.0]]]])\n```\n\nHow can I compute the mean of each channel across the batch, ignoring NaN values?\n\nA:\n```python\nimport torch\ntensor = torch.tensor([[[[1.0, 2.0, float('nan')], \n                          [4.0, 5.0, 6.0]],\n\n                         [[7.0, 8.0, float('nan')], \n                          [10.0, float('nan'), 12.0]]],\n\n                        [[[float('nan'), 2.0, 3.0], \n                          [float('nan'), 5.0, 6.0]],\n\n                         [[7.0, float('nan'), 9.0], \n                          [10.0, 11.0, 12.0]]]])\n```\nmean_values = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nmean_values = torch.nanmean(tensor, dim=0)\n```"
  },
  {
    "input": "Problem:\n\nIn a machine learning project, I have a tensor representing the scores of various predictions from a model, and I need to rank them to identify the top N predictions per batch sample. The tensor `scores` has a shape of (batch_size, num_predictions) and contains floating-point numbers representing the predicted scores. I want to create a new tensor that contains the indices of the top N predictions for each batch sample.\n\nHere's an example structure of the `scores` tensor:\n\n```\n[[0.1, 0.4, 0.2],\n [0.5, 0.3, 0.9],\n [0.6, 0.2, 0.8]]\n```\nIf `N=2`, the desired output should look like:\n\n```\n[[1, 2],\n [2, 0],\n [0, 2]]\n```\n\nPlease fill in the blank in the code snippet below to achieve this using PyTorch:\n\n```python\nimport torch\n\nscores = torch.tensor([[0.1, 0.4, 0.2],\n                       [0.5, 0.3, 0.9],\n                       [0.6, 0.2, 0.8]])\n\nN = 2\ntop_indices = ... # put solution in this variable\n```",
    "output": "\n```python\ntop_indices = torch.topk(scores, N, dim=1).indices\n```"
  },
  {
    "input": "Problem:\n\nI have two tensors representing embeddings of words in a vocabulary. I need to compute the cosine similarity between each pair of word embeddings from two different tensors and then create a boolean mask indicating which pairs have a similarity score greater than a specified threshold.\n\nGiven two tensors, `embeddings_a` and `embeddings_b`, how can I calculate the cosine similarity and then obtain a mask indicating which similarities exceed the threshold of 0.5?\n\nA:\n\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nembeddings_a, embeddings_b = load_data()\n</code>\nsimilarity_mask = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ncosine_similarity = torch.nn.functional.cosine_similarity(embeddings_a.unsqueeze(1), embeddings_b.unsqueeze(0), dim=2)\nsimilarity_mask = cosine_similarity > 0.5\n</code>"
  },
  {
    "input": "Problem:\nYou have been given a dataset representing customer transactions in a retail store, where each transaction record includes information about the customer, product, and purchase amount. The dataset is stored in a Pandas DataFrame as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'CustomerID': [1, 2, 1, 3, 2, 1],\n    'Product': ['A', 'B', 'A', 'C', 'B', 'C'],\n    'Amount': [20, 30, 25, 40, 35, 50],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03'])\n}\ndf = pd.DataFrame(data)\n```\n\nThe DataFrame looks like this:\n\n```\n   CustomerID Product  Amount       Date\n0           1       A      20 2023-01-01\n1           2       B      30 2023-01-01\n2           1       A      25 2023-01-02\n3           3       C      40 2023-01-02\n4           2       B      35 2023-01-03\n5           1       C      50 2023-01-03\n```\n\nYou want to pivot this data such that each unique customer is represented as a row, each unique product is represented as a column, and the values in the DataFrame reflect the sum of the amounts spent by each customer on each product. Additionally, you want to assign a multi-index to the columns composed of 'Product' and 'Transaction', where 'Transaction' indicates the order of the transactions (1st, 2nd, etc.).\n\nAfter these operations, the resulting DataFrame should look like the following structure:\n\n```\n                 Amount       \nProduct          A   B   C\nCustomerID                \n1                45  NaN  50\n2               NaN  65  NaN\n3               NaN  NaN  40\n```\n\nPlease fill in the blank with the appropriate code to accomplish the above task in Pandas:\n\n```python\nresult = ... # put solution in this variable\n```",
    "output": "\n```python\nresult = df.pivot_table(index='CustomerID', columns='Product', values='Amount', aggfunc='sum').fillna(np.nan)\n```"
  },
  {
    "input": "Problem:\nI have a sales dataset represented in a pandas DataFrame with the following columns: 'OrderID', 'Product', 'Quantity', 'Price', 'Discount', and 'Date'. Some of the rows in the 'Discount' column contain null values. I want to compute the total sales for each product while applying a discount of 10% where the 'Discount' is null and using the given value when it's not null. Here's a snippet of my DataFrame:\n\n```\n   OrderID   Product  Quantity  Price  Discount       Date\n0        1  Widget A        2   20.0      NaN  2023-01-01\n1        2  Widget B        1   30.0      5.0  2023-01-02\n2        3  Widget A        4   20.0      NaN  2023-01-03\n3        4  Widget C        5   15.0      3.0  2023-01-04\n4        5  Widget B        3   30.0      NaN  2023-01-05\n```\n\nAfter performing the necessary calculations, I want to get the resulting DataFrame that shows the total sales for each product. How can I achieve this?\n\nA:\n<code>\nimport pandas as pd\n\ndata = {\n    'OrderID': [1, 2, 3, 4, 5],\n    'Product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],\n    'Quantity': [2, 1, 4, 5, 3],\n    'Price': [20.0, 30.0, 20.0, 15.0, 30.0],\n    'Discount': [None, 5.0, None, 3.0, None],\n    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']\n}\ndf = pd.DataFrame(data)\n</code>\ntotal_sales = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf['Discount'] = df['Discount'].fillna(0.1 * df['Price'])\ndf['TotalSales'] = df['Quantity'] * df['Price'] * (1 - df['Discount'] / 100)\ntotal_sales = df.groupby('Product')['TotalSales'].sum().reset_index()\n</code>"
  },
  {
    "input": "Problem:  \nYou are tasked with comparing two sets of multi-dimensional tensor embeddings to identify which embeddings are similar based on a cosine similarity threshold. You have two sets of embeddings, `embeddings_a` and `embeddings_b`, both represented as 2D tensors (where each row represents an embedding). Your goal is to create a boolean mask indicating which rows in `embeddings_a` are similar to any row in `embeddings_b` based on a cosine similarity above a specified threshold (let's say `0.8`). \n\nHere's the initial setup for your code:\n\n```python\nimport torch\n\nembeddings_a = torch.rand(5, 128)  # 5 embeddings of dimension 128\nembeddings_b = torch.rand(7, 128)  # 7 embeddings of dimension 128\nthreshold = 0.8\n```\n\nYou need to fill in the blank to compute the boolean mask indicating which embeddings from `embeddings_a` are similar to those in `embeddings_b`:\n\n```python\ncosine_similarity = torch.nn.functional.cosine_similarity(embeddings_a.unsqueeze(1), embeddings_b.unsqueeze(0), dim=2)\nmask = (cosine_similarity > threshold)\n```\n\nWhat should you assign to `mask` so that it indicates whether each embedding in `embeddings_a` is similar to at least one embedding in `embeddings_b`? \n\nA:\n```python\nmask = ...  # put solution in this variable\n```",
    "output": "\n```python\nmask = mask.any(dim=1)\n```"
  },
  {
    "input": "## Problem\n\nI am working with a dataset that contains various features about houses, including their prices. I want to preprocess the data to better prepare it for a machine learning model. Specifically, I need to handle missing values in the 'price' column, standardize the 'size' column, and encode the categorical 'location' column.\n\nI have the following code snippet to start with:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Load dataset\ndf = pd.read_csv(\"houses.csv\")\n\n# Define columns\nnumeric_features = ['size']\ncategorical_features = ['location']\ntarget_column = 'price'\n\n# Imputer for numerical data\nnum_imputer = SimpleImputer(strategy='mean')\n# Encoder for categorical data\ncat_encoder = OneHotEncoder()\n\n# Create a preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_imputer, numeric_features),\n        ('cat', cat_encoder, categorical_features)\n    ]\n)\n\n# Apply preprocessing\nX = df[numeric_features + categorical_features]\ny = df[target_column]  # target variable\n\n# I need to fit and transform the features using preprocessor and also use StandardScaler on 'size'.\nX_transformed = ... # put solution in this variable\n\n```\nWhat code should I place where the comment indicates to preprocess `X` correctly?\n\nA:\n```python\nX_transformed = ... # put solution in this variable\n```",
    "output": "\n```python\nX_transformed = preprocessor.fit_transform(X)\nX_transformed[:, 0] = StandardScaler().fit_transform(X_transformed[:, 0].reshape(-1, 1)).flatten()\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames containing sales data as follows:\n\ndf1:\n```\n   Product    Sales    Date\n0    A       1500.00  2023-01-01\n1    B       2000.00  2023-01-05\n2    C       2500.00  2023-01-10\n3    A       3000.00  2023-01-15\n4    B       4000.00  2023-01-20\n```\n\ndf2:\n```\n   Product    Sales    Date\n0    B       3500.00  2023-01-15\n1    D       4500.00  2023-01-20\n2    A       1000.00  2023-01-25\n3    C       5000.00  2023-01-30\n```\n\nI need to combine these DataFrames so that for each product, the rows are merged based on the \"Product\" column. If both DataFrames have sales for the same product on the same date, the value from df2 should overwrite the value in df1. The resulting DataFrame should preserve the order of the original DataFrames and include a new column called 'Updated' which indicates whether the row contains updated values (True if the sales came from df2, False if from df1).\n\nYour resulting DataFrame should look like this:\n```\n   Product    Sales       Date   Updated\n0    A       1500.00   2023-01-01    False\n1    B       3500.00   2023-01-05    True\n2    C       2500.00   2023-01-10    False\n3    A       3000.00   2023-01-15    False\n4    B       4000.00   2023-01-20    False\n5    D       4500.00   2023-01-20    True\n6    A       1000.00   2023-01-25    True\n7    C       5000.00   2023-01-30    True\n```\n\nPlease fill in the code below to achieve this:\n\n<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'Product': ['A', 'B', 'C', 'A', 'B'],\n    'Sales': [1500.00, 2000.00, 2500.00, 3000.00, 4000.00],\n    'Date': pd.to_datetime(['2023-01-01', '2023-01-05', '2023-01-10', '2023-01-15', '2023-01-20'])\n})\n\ndf2 = pd.DataFrame({\n    'Product': ['B', 'D', 'A', 'C'],\n    'Sales': [3500.00, 4500.00, 1000.00, 5000.00],\n    'Date': pd.to_datetime(['2023-01-15', '2023-01-20', '2023-01-25', '2023-01-30'])\n})\n\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = pd.concat([df1.assign(Updated=False), df2.assign(Updated=True)]).sort_values(by=['Product', 'Date']).drop_duplicates(subset=['Product', 'Date'], keep='last').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nI have a DataFrame containing sales data for different products across multiple stores. Here is the DataFrame:\n\n```\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'store': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'B', 'C'],\n    'product': ['Apple', 'Banana', 'Apple', 'Banana', 'Banana', 'Apple', 'Apple', 'Apple', 'Banana'],\n    'sales': [10, np.nan, 15, 12, 5, np.nan, 8, 20, np.nan]\n}\ndf = pd.DataFrame(data)\n```\n\nI want to compute the total sales for each product, but if any of the sales values for a product in a store are NaN, I want the total for that product to also be NaN. I can achieve this using groupby and an aggregate function, but I'm not sure how to handle the NaNs correctly.\n\nThe expected result should look like this:\n\n```\nproduct\nApple    NaN\nBanana   NaN\nName: sales, dtype: float64\n```\n\nHow can I write the code to achieve this?\n\nA:\n<code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nresult = df.groupby('product')['sales'].agg(lambda x: x.sum() if x.isnull().any() == False else np.nan)\n</code>"
  },
  {
    "input": "Problem:\nI have a DataFrame with multi-index columns representing different categories of sales data for various products over the years. The structure is as follows:\n\n```plaintext\n               Electronics       Clothing      Groceries     \n                    2019 2020    2019 2020    2019 2020\nProduct A          500   600   300   400   200   250\nProduct B          700   800   500   600   300   350\nProduct C          400   450   200   250   150   175\n```\n\nI need to normalize the sales data for each product over the years, so that all values fall within the range of 0 and 1 for each category. How can I efficiently achieve this using Pandas?\n\nPlease provide the following code, which initializes this DataFrame:\n\n```python\nimport pandas as pd\nimport numpy as np\n\nindex = ['Product A', 'Product B', 'Product C']\ncolumns = pd.MultiIndex.from_product([['Electronics', 'Clothing', 'Groceries'], [2019, 2020]])\ndata = [[500, 600, 300, 400, 200, 250],\n        [700, 800, 500, 600, 300, 350],\n        [400, 450, 200, 250, 150, 175]]\n\ndf = pd.DataFrame(data, index=index, columns=columns)\n```\n\nYour task is to write the code that normalizes the sales data in the DataFrame `df` such that the new DataFrame `normalized_sales` contains normalized values. Fill in the `...` with the appropriate normalization logic. \n\n```python\nnormalized_sales = ...\n```",
    "output": "\n```python\nnormalized_sales = df.groupby(level=0, axis=1).apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n```"
  },
  {
    "input": "Problem:\nYou have two dataframes: one contains employee information with some missing data, and the other contains the latest updates for those employees. Your goal is to update the employee dataframe with the latest information from the updates dataframe, ensuring that only unique employee records are kept, and that any missing values in the employee dataframe are filled with data from the updates dataframe when applicable.\n\nThe dataframes are as follows:\n\nemployees:\n```\n   employee_id    name       position       salary\n0           1     Alice      Manager          60000\n1           2      Bob       Developer        55000\n2           3      Charlie     Analyst        NaN\n3           4      David      Designer       50000\n4           5      Eva        Manager          NaN\n```\n\nupdates:\n```\n   employee_id    name       position       salary\n0           1     Alice      Manager          62000\n1           2      Bob       Developer        55000\n2           3      Charlie     Data Scientist 70000\n3           4      David      Designer       51000\n4           6      Frank      Tester         40000\n```\n\nYour task is to merge these two dataframes on `employee_id`, filling in missing values in `employees` with the corresponding values from `updates` where applicable. The final dataframe should maintain only the most recent entries for each employee, resulting in a deduplicated and updated dataframe.\n\nHow would you fill in the following variable with the merged result?\n```python\nimport pandas as pd\n\nemployees = pd.DataFrame({\n    'employee_id': [1, 2, 3, 4, 5],\n    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n    'position': ['Manager', 'Developer', 'Analyst', 'Designer', 'Manager'],\n    'salary': [60000, 55000, None, 50000, None]\n})\n\nupdates = pd.DataFrame({\n    'employee_id': [1, 2, 3, 4, 6],\n    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Frank'],\n    'position': ['Manager', 'Developer', 'Data Scientist', 'Designer', 'Tester'],\n    'salary': [62000, 55000, 70000, 51000, 40000]\n})\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = employees.set_index('employee_id').combine_first(updates.set_index('employee_id')).reset_index()\n```"
  },
  {
    "input": "Problem:\n\nYou are working on a recommendation system using PyTorch. You have a user-item interaction tensor `interactions` of shape (n_users, n_items) where each element represents a score indicating the user's preference for a particular item. You want to filter this tensor to get only the scores of items that have been positively rated (greater than zero). After filtering, you want to compute the mean score per user from the remaining items. \n\nHere\u2019s the code stub you have:\n\n```python\nimport torch\n\n# Sample tensor representing user-item interactions\ninteractions = torch.tensor([[5.0, 0.0, 3.0],\n                             [0.0, 2.0, 0.0],\n                             [4.0, 0.0, 1.0]])\n\n# Create a mask for positive ratings\nmask = interactions > 0\n\n# Filter the interactions tensor\nfiltered_interactions = interactions[mask] # This line needs correction\n\n# Compute the mean score per user\nmean_scores = ...\n\n```\n\nFill in the blank in the `filtered_interactions` assignment to correctly apply the mask. The result should maintain the original structure of filtering while considering the user-item relations.",
    "output": "\n```python\nfiltered_interactions = interactions * mask\nmean_scores = filtered_interactions.sum(dim=1) / mask.sum(dim=1)\n```"
  },
  {
    "input": "Problem:\nYou are given a DataFrame containing sales data for different products over various months, structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('Product_A', 'Q1'): [200, 250, 300],\n    ('Product_A', 'Q2'): [220, 240, 320],\n    ('Product_B', 'Q1'): [150, 175, 200],\n    ('Product_B', 'Q2'): [180, 190, 210],\n}\n\nindex = ['January', 'February', 'March']\ndf = pd.DataFrame(data, index=index)\n```\n\nYou need to normalize the sales data for each product (column-wise normalization) and then create a new DataFrame that retains the original multi-index structure while also appending a new row at the bottom with the normalized sums for each product across all months. How can you accomplish this?\n\nA:\n<code>\ndf_normalized = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\ndf_normalized = df.div(df.sum(axis=0), axis=1)\nnormalized_sums = df_normalized.sum(axis=0).to_frame().T\nnormalized_sums.index = ['Normalized_Sum']\ndf_normalized = pd.concat([df_normalized, normalized_sums])\n</code>"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame that contains sales data with certain entries marked as NaN. The goal is to create a new column called 'adjusted_sales' that applies a custom discount of 10% to all sales values greater than 100, and sets the 'adjusted_sales' to 0 for any missing (NaN) values. The DataFrame is structured as follows:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'product': ['A', 'B', 'C', 'D', 'E'],\n    'sales': [150, 80, np.nan, 250, 60]\n})\n```\n\nWhen applying the discount and handling NaN values, I want to use a conditional operation combined with a custom function. Here's a part of the code I'm working on:\n\n```python\ndef apply_discount(sale):\n    if pd.isna(sale):\n        return ...\n    elif sale > 100:\n        return sale * 0.9\n    else:\n        return sale\n\ndf['adjusted_sales'] = ...\n```\n\nWhat code should I put in place of the blank to achieve my desired outcome? \n",
    "output": "\n```python\n        return 0\n\ndf['adjusted_sales'] = df['sales'].apply(apply_discount)\n```"
  },
  {
    "input": "Problem:\n\nYou have a dataset consisting of several numerical features representing various measurements from an IoT device. The dataset is prone to missing values, and you wish to preprocess the data by applying normalization and imputation techniques before utilizing it for machine learning. \n\nYour task is to fill in the blank to create a pre-processing pipeline that handles missing values by imputing them with the median of each feature, and then normalizes the data using Min-Max scaling.\n\nGiven the data and imports below, complete the code that constructs a preprocessing pipeline:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = pd.DataFrame({\n    'feature1': [1, 2, np.nan, 4, 5],\n    'feature2': [np.nan, 1, 2, 3, 4],\n    'feature3': [5, 4, 3, np.nan, 1]\n})\n\npipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])\n\nprocessed_data = pipeline.fit_transform(data)\n```\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>processed_data</code>",
    "output": "\n<code>processed_data</code>"
  },
  {
    "input": "Problem:\n\nYou have collected a dataset containing various features of houses including their sizes (in square feet), number of bedrooms, and prices. Before training a machine learning model, you need to perform data preprocessing steps which include scaling the features and one-hot encoding any categorical variables.\n\nGiven the following dataset:\n\n```python\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'size': [1500, 1600, 1700, 1800, 1900],\n    'bedrooms': [3, 3, 3, 4, 4],\n    'price': [300000, 320000, 340000, 360000, 380000],\n    'location': ['city', 'suburb', 'city', 'suburb', 'city']\n})\n```\n\nYou need to scale the 'size' and 'price' columns to have a mean of 0 and a standard deviation of 1, and apply one-hot encoding to the 'location' column.\n\nPlease fill in the blank to create a preprocessed DataFrame called `X_processed`, which includes the necessary transformations:\n\n```python\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Define the preprocessing steps\nnumeric_features = ['size', 'price']\ncategorical_features = ['bedrooms', 'location']\n\n# Create preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n\n# Apply the transformations\nX_processed = ...  # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>X_processed = preprocessor.fit_transform(data)</code>"
  },
  {
    "input": "Problem:\n\nI have a dataset containing customer information with various features such as age, income, and purchase history. Some of these features have missing values, and I want to preprocess them before feeding them into a machine learning model. Specifically, I need to apply the following transformations:\n\n1. Fill missing values in the 'age' column with the median age.\n2. Normalize the 'income' column using Min-Max scaling.\n3. One-hot encode the 'purchase_history' categorical column.\n\nHow can I achieve this using Pandas and Scikit-Learn?\n\nA:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = load_data()\n```\npipeline = ...  # put solution in this variable\nBEGIN SOLUTION\n```python\n\n",
    "output": "\n```python\npipeline = Pipeline(steps=[\n    ('preprocessor', ColumnTransformer(\n        transformers=[\n            ('age_imputer', 'passthrough', ['age']),\n            ('income_scaler', MinMaxScaler(), ['income']),\n            ('purchase_encoder', OneHotEncoder(), ['purchase_history'])\n        ],\n        remainder='drop'\n    )),\n    ('age_median_imputer', SimpleImputer(strategy='median'))\n])\n```"
  },
  {
    "input": "Problem:\nI have a pandas DataFrame containing sales data for different products across several regions. The DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'C', 'B', 'B', 'C'],\n    'Region': ['North', 'South', 'North', 'East', 'South', 'East', 'North'],\n    'Sales': [100, np.nan, 150, 200, 300, np.nan, 250],\n    'Discount': [10, 20, 15, 25, 30, np.nan, 5],\n    'Discounted_Sales': [90, np.nan, 127.5, 150, 210, np.nan, 237.5]\n}\n\ndf = pd.DataFrame(data)\n```\n\nI need to compute the total sales for each product in the 'North' region after applying a discount if available. If a discount is 'NaN', I want to treat it as zero. Here\u2019s the logic:\n1. Calculate the effective sales after discount for all entries in the 'North' region. If 'Discount' is NaN, use 0 instead.\n2. Sum the 'Sales' for each product in the 'North' region using a conditional aggregation.\n\nHow can I achieve this in pandas? \n\nA:\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Product': ['A', 'B', 'A', 'C', 'B', 'B', 'C'],\n    'Region': ['North', 'South', 'North', 'East', 'South', 'East', 'North'],\n    'Sales': [100, np.nan, 150, 200, 300, np.nan, 250],\n    'Discount': [10, 20, 15, 25, 30, np.nan, 5],\n    'Discounted_Sales': [90, np.nan, 127.5, 150, 210, np.nan, 237.5]\n}\n\ndf = pd.DataFrame(data)\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\ndf_north = df[df['Region'] == 'North']\ndf_north['Effective_Discount'] = df_north['Discount'].fillna(0)\ndf_north['Effective_Sales'] = df_north['Sales'] - df_north['Effective_Discount']\nresult = df_north.groupby('Product')['Effective_Sales'].sum()\n```"
  },
  {
    "input": "Problem:\nYou are working with a dataset of image embeddings produced from a neural network. Each image is represented by a tensor of shape `[batch_size, embedding_dimension]`, where `batch_size` is 200 and `embedding_dimension` is 512. You want to compute the cosine similarity between each pair of embeddings to create a similarity matrix. To do this, you need to normalize the embeddings first. You have the following tensor containing the embeddings:\n\n```python\nimport torch\n\nembeddings = torch.randn(200, 512)  # Example tensor of image embeddings\n```\n\nTo proceed with calculating the cosine similarity, which requires the embeddings to be normalized, how would you normalize the `embeddings` tensor so that each row has a Euclidean norm of 1?\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = embeddings / embeddings.norm(dim=1, keepdim=True)\n</code>\n```"
  },
  {
    "input": "Problem:\nI have two DataFrames, sales and returns, as follows:\n\nsales:\n```\n    OrderID    OrderDate   ProductID   Quantity\n0      1001  2023-01-01       A123         10\n1      1002  2023-01-02       B234          5\n2      1003  2023-01-03       A123          3\n3      1004  2023-01-04       C345         20\n4      1005  2023-01-05       B234          2\n```\n\nreturns:\n```\n   ReturnID  ReturnDate   ProductID   Quantity\n0       R001  2023-01-02       A123          2\n1       R002  2023-01-05       B234          1\n2       R003  2023-01-06       C345          5\n```\n\nI need to create a new DataFrame combining both sales and returns, maintaining the integrity of the sales data while adjusting the quantity based on the returns. Specifically, if a product was returned, the quantity of that product in the sales DataFrame should be decreased accordingly. The resulting DataFrame should contain the original sales data with an additional column indicating the adjusted quantity.\n\nThe desired output should look like this:\n\n```\n    OrderID    OrderDate   ProductID   Quantity  Adjusted_Quantity\n0      1001  2023-01-01       A123         10                  8\n1      1002  2023-01-02       B234          5                  4\n2      1003  2023-01-03       A123          3                  3\n3      1004  2023-01-04       C345         20                 15\n4      1005  2023-01-05       B234          2                  1\n```\n\nI'm currently unsure how to adjust the quantities in the sales DataFrame according to the returns without losing data integrity. \n\nA:\n<code>\nimport pandas as pd\n\nsales = pd.DataFrame({\n    'OrderID': [1001, 1002, 1003, 1004, 1005],\n    'OrderDate': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']),\n    'ProductID': ['A123', 'B234', 'A123', 'C345', 'B234'],\n    'Quantity': [10, 5, 3, 20, 2]\n})\n\nreturns = pd.DataFrame({\n    'ReturnID': ['R001', 'R002', 'R003'],\n    'ReturnDate': pd.to_datetime(['2023-01-02', '2023-01-05', '2023-01-06']),\n    'ProductID': ['A123', 'B234', 'C345'],\n    'Quantity': [2, 1, 5]\n})\n</code>\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n",
    "output": "\n<code>\n# Group returns by ProductID and sum the quantities\nreturns_grouped = returns.groupby('ProductID')['Quantity'].sum().reset_index()\n\n# Merge sales with the grouped returns\nmerged = pd.merge(sales, returns_grouped, on='ProductID', how='left', suffixes=('', '_Returned'))\n\n# Fill NaN values with 0 for returned quantities\nmerged['Quantity_Returned'] = merged['Quantity_Returned'].fillna(0)\n\n# Calculate adjusted quantity\nmerged['Adjusted_Quantity'] = merged['Quantity'] - merged['Quantity_Returned']\n\n# Select relevant columns for the final result\nresult = merged[['OrderID', 'OrderDate', 'ProductID', 'Quantity', 'Adjusted_Quantity']]\n</code>"
  },
  {
    "input": "Problem:\n\nYou are analyzing a dataset containing housing prices and various features such as square footage, number of bedrooms, and neighborhood quality. You want to preprocess this data using the `StandardScaler` from `sklearn.preprocessing` to normalize the features before training a regression model. The dataset is loaded into a Pandas DataFrame called `housing_data`. You need to scale the features in the DataFrame and return the transformed data as a NumPy array.\n\nConsider the following setup:\n\n```python\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load your data\nhousing_data = pd.DataFrame({\n    'square_footage': [1500, 1800, 2400, 3000],\n    'bedrooms': [3, 4, 4, 5],\n    'neighborhood_quality': [8, 7, 9, 6]\n})\n\nscaler = StandardScaler()\n```\n\nWhat code would you use to transform the features in `housing_data` and store them in the variable `scaled_features`?\n\nA:\n\n<code>\nscaled_features = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nscaled_features = scaler.fit_transform(housing_data)\n</code>"
  },
  {
    "input": "## Problem\n\nI am working on a recommendation system and need to calculate the similarity between user preferences represented as a tensor in PyTorch. The preferences are stored in a 3D tensor `user_preferences` with dimensions corresponding to `[users, items, attributes]`. \n\nI want to compute the cosine similarity for each user\u2019s preferences against a target item represented by a tensor `target_item`. The goal is to create an output tensor of shape `[users, 1]` that contains the similarity scores.\n\nHere is how my input tensors look:\n\n```python\nimport torch\n\n# Shape: [users, items, attributes]\nuser_preferences = torch.tensor([\n    [[0.2, 0.3], [0.5, 0.7]],\n    [[0.1, 0.4], [0.8, 0.6]],\n    [[0.9, 0.3], [0.2, 0.1]]\n])\n\n# Shape: [1, attributes]\ntarget_item = torch.tensor([[0.5, 0.5]])\n```\n\nHow can I calculate the cosine similarity between each user\u2019s preferences (flattened to item dimension) and the target item?\n\nA:\n\n```python\nimport torch\n\nuser_preferences, target_item = load_data()\n# Calculate the cosine similarity scores\nsimilarity_scores = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nuser_preferences_flat = user_preferences.view(user_preferences.size(0), -1)\ntarget_item_flat = target_item.view(-1)\n\nsimilarity_scores = torch.nn.functional.cosine_similarity(user_preferences_flat, target_item_flat.unsqueeze(0), dim=1).unsqueeze(1)\nEND SOLUTION\n```"
  },
  {
    "input": "Problem:\nContext\nI have two DataFrames that need to be combined while also ensuring data integrity and managing duplicates effectively. \n\nThe first DataFrame is as follows:\n```python\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n```\n\nAnd the second DataFrame is:\n```python\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\n\nI want to merge these two DataFrames on the `product_id` column, but I need to handle the case where `product_id` values in `df2` may result in duplicate rows in the final merged DataFrame, and keep only the first occurrence. The output should include all products and their corresponding stores, with an added column for availability.\n\nFinally, I would like to make sure the final DataFrame is sorted by `product_id`, in ascending order. The expected output should resemble:\n```python\n   product_id        products  price    store  availability\n0         101          Widget  20.99      NaN           NaN\n1         102          Gadget  25.50      NaN           NaN\n2         103          Doodad  15.75  Store A          True\n3         104  Thingamajig  22.10  Store B         False\n4         105           NaN     NaN  Store D          True\n```\n\nYou can find below the beginning of my implementation:\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    'product_id': [101, 102, 103, 104],\n    'products': ['Widget', 'Gadget', 'Doodad', 'Thingamajig'],\n    'price': [20.99, 25.50, 15.75, 22.10]\n})\n\ndf2 = pd.DataFrame({\n    'product_id': [103, 104, 104, 105],\n    'store': ['Store A', 'Store B', 'Store C', 'Store D'],\n    'availability': [True, False, True, True]\n})\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n```",
    "output": "\n```python\nresult = pd.merge(df1, df2.drop_duplicates(subset='product_id'), on='product_id', how='outer').sort_values(by='product_id').reset_index(drop=True)\n```"
  },
  {
    "input": "Problem:\nYou have a DataFrame containing sales data from a retail store with missing values. Each row represents a transaction, including columns for the product category, transaction amount, and discount applied. Your task is to compute a new column called 'final_price' that represents the price after applying the discount, ensuring that you handle cases where either the discount or the transaction amount is null (treat null discounts as 0 and null amounts as 0). \n\nThe DataFrame looks like this:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'C', 'B'],\n    'amount': [100, np.nan, 200, 150, 300],\n    'discount': [10, 20, np.nan, 5, np.nan]\n})\n```\n\nYou are asked to complete the following line of code to create the `final_price` column using a custom function:\n\n```python\ndf['final_price'] = df.apply(lambda row: row['amount'] * (1 - (row['discount'] / 100)) if pd.notnull(row['amount']) and pd.notnull(row['discount']) else np.nan, axis=1)\n```\n\nHowever, using the above code, you find it repetitive and want a more elegant solution for handling null values gracefully using DataFrame methods. \n\nFill in the blank to replace the lambda function:\n\n```python\ndf['final_price'] = df.apply(__________, axis=1)\n```\n\nDesired output when applied:\n```python\n   category  amount  discount  final_price\n0        A   100.0      10.0          90.0\n1        B     NaN      20.0           NaN\n2        A   200.0       NaN         200.0\n3        C   150.0       5.0         142.5\n4        B   300.0       NaN         300.0\n```",
    "output": "\n```python\ndf['final_price'] = df.apply(lambda row: row['amount'] * (1 - (row['discount'] / 100)) if pd.notnull(row['amount']) else 0, axis=1).fillna(0)\n```"
  },
  {
    "input": "## Problem\n\nI have a Pandas DataFrame that represents survey results from different demographic groups, and the structure looks something like this:\n\n```\n      Age    Income  Gender    Response\nGroup1 25.0  50000  Male    [Yes, No]\nGroup2 30.0  60000  Female  [Yes, Yes, No]\nGroup3 35.0  70000  Male    [No]\nGroup4 40.0  80000  Female  [Yes]\n```\n\nI want to expand the 'Response' column into multiple binary indicator columns for each unique response (like Yes and No) while also normalizing the 'Income' column to have a mean of 0 and standard deviation of 1. The final DataFrame should have the structure:\n\n```\n      Age   Income       Gender  Yes  No\nGroup1 25.0  -1.341641  Male  1.0  1.0\nGroup2 30.0  -0.447214  Female  1.0  0.0\nGroup3 35.0   0.447214  Male  0.0  1.0\nGroup4 40.0   1.341641  Female  1.0  0.0\n```\n\nHow can I normalize the 'Income' column and expand the 'Response' column appropriately using Pandas?\n\n```python\nimport pandas as pd\n\ndata = {\n    'Age': [25.0, 30.0, 35.0, 40.0],\n    'Income': [50000, 60000, 70000, 80000],\n    'Gender': ['Male', 'Female', 'Male', 'Female'],\n    'Response': [['Yes', 'No'], ['Yes', 'Yes', 'No'], ['No'], ['Yes']]\n}\ndf = pd.DataFrame(data, index=['Group1', 'Group2', 'Group3', 'Group4'])\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\n# Normalize the 'Income' column\ndf['Income'] = (df['Income'] - df['Income'].mean()) / df['Income'].std()\n\n# Expand the 'Response' column into binary indicator columns\nresponse_dummies = df['Response'].explode().str.get_dummies().sum(level=0)\n\n# Combine the original DataFrame with the new response columns\nresult = pd.concat([df.drop(columns='Response'), response_dummies], axis=1)\n```"
  },
  {
    "input": "Problem:\n\nI have a dataset containing information about various houses, and I need to encode a categorical feature called 'Neighborhood' into numerical values to use in a linear regression model. The dataset looks like this:\n\n```\n   Size  Bedrooms  Neighborhood\n0   1500        3        Suburban\n1   2000        4        Urban\n2   1200        2        Suburban\n3   1800        3        Rural\n4   2400        5        Urban\n```\n\nI want to create a one-hot encoding for the 'Neighborhood' feature while keeping the original columns intact. After encoding, the DataFrame should include new columns for each neighborhood without dropping the original 'Neighborhood' column.\n\nA:\n\n<code>\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Size': [1500, 2000, 1200, 1800, 2400],\n    'Bedrooms': [3, 4, 2, 3, 5],\n    'Neighborhood': ['Suburban', 'Urban', 'Suburban', 'Rural', 'Urban']\n}\ndf = pd.DataFrame(data)\n\n# One-hot encoding of the 'Neighborhood' column\nencoded_df = df.copy()\n</code>\nencoded_df = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n<code>\nencoded_df = pd.get_dummies(df, columns=['Neighborhood'], drop_first=False)\n</code>"
  },
  {
    "input": "Problem:\nYou have a pandas DataFrame containing sales data for different products across various regions, structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Product': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'D'],\n    'Region': ['North', 'South', 'East', 'West', 'North', 'East', 'South', 'West'],\n    'Sales': [200, None, 150, 300, 100, None, 250, 400],\n    'Profit': [50, 20, 30, None, 10, 40, None, 70]\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou need to accomplish the following tasks:\n1. Replace any `None` values in the 'Sales' column with the mean sales from the DataFrame.\n2. Calculate the total profit for each product, using only the non-null profit values, and store this in a new DataFrame.\n3. Finally, filter this results DataFrame to include only those products whose total profit exceeds 40.\n\nCan you fill in the blank to achieve this? \n\n```python\nmean_sales = df['Sales'].mean()\ndf['Sales'].fillna(mean_sales, inplace=True)\n\ntotal_profit = df.groupby('Product')['Profit'].sum().reset_index()\nresult = total_profit[total_profit['Profit'] > 40]\n```\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\n```python\nresult = total_profit[total_profit['Profit'] > 40]\n</code>\n```"
  },
  {
    "input": "Problem:\n\nI have a 4D tensor representing a batch of images with the shape `(batch_size, channels, height, width)`. I want to apply a boolean mask to this tensor to zero out all pixels in the images where a corresponding condition fails (e.g., pixels below a certain threshold). The mask is of shape `(batch_size, height, width)` and contains boolean values indicating which pixels should be retained (True) or set to zero (False).\n\nHow can I apply this boolean mask to the 4D tensor to modify the images accordingly?\n\nA:\n```python\nimport torch\nimages = load_data()  # shape (batch_size, channels, height, width)\nmask = load_mask()  # shape (batch_size, height, width)\nresult = ...  # put solution in this variable\n```",
    "output": "\n```python\nresult = images * mask.unsqueeze(1).float()\n```"
  },
  {
    "input": "Problem:\n\nI am working with a dataset of customer transactions from an online store. The dataset contains the following columns: 'customer_id', 'transaction_amount', and 'transaction_date'. I want to calculate the average transaction amount for each customer and identify customers who have an average transaction amount greater than a set threshold of $100.\n\nHere is my initial code setup:\n\n```python\nimport pandas as pd\n\ndata = {'customer_id': [1, 2, 1, 2, 3, 1, 3], \n        'transaction_amount': [120, 80, 200, 150, 90, 60, 300],\n        'transaction_date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-05', '2023-01-06', '2023-01-09', '2023-01-10', '2023-01-15'])}\n\ndf = pd.DataFrame(data)\n```\n\nI would like to create a new DataFrame that contains only the 'customer_id' and their average transaction amount, filtered to include only those customers whose average amount is greater than $100.\n\nThe code returns the result in this variable:\n\n```python\nresult = ... # put solution in this variable\n```\n\n### BEGIN SOLUTION\n```python\n```",
    "output": "\n```python\nresult = df.groupby('customer_id')['transaction_amount'].mean().reset_index()\nresult = result[result['transaction_amount'] > 100]\n```"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset of customer transactions and want to derive meaningful features from the timestamps of these transactions. Your goal is to extract features such as the hour of the day, day of the week, and whether the transaction occurred on a weekend. You have a dataframe called `transactions` with a column named `timestamp` in datetime format. \n\nYou need to create a new dataframe that contains the original timestamps along with these three new features. Fill in the blank with a function that achieves this transformation using pandas.\n\nA:\n<code>\nimport pandas as pd\n\n# Sample transaction data\ndata = {'timestamp': pd.to_datetime(['2022-01-01 08:23', '2022-01-02 14:45', '2022-01-03 18:12'])}\ntransactions = pd.DataFrame(data)\n\ndef extract_features(df):\n    # Add the hour of the day, day of the week, and weekend flag\n    df['hour'] = df['timestamp'].dt.hour\n    df['day_of_week'] = df['timestamp'].dt.day_name()\n    df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5\n    return df\n\n# Fill in the blank\nnew_transactions = extract_features(transactions)\n</code>\nnew_transactions = ... # put solution in this variable\nBEGIN SOLUTION\n<code>",
    "output": "\nnew_transactions = extract_features(transactions)  \n</code>"
  },
  {
    "input": "Problem:\n\nYou are working with a dataset that provides daily sales data for multiple product categories across different regions. Your DataFrame, `sales_data`, looks like this:\n\n```\n                      sales\nregion  category  date\nNorth  Electronics 2023-01-01    1000\n       Clothing    2023-01-01     500\n       Electronics 2023-01-02    1200\n       Clothing    2023-01-02     600\nSouth  Electronics 2023-01-01     800\n       Clothing    2023-01-01     400\n       Electronics 2023-01-02     950\n       Clothing    2023-01-02     500\n```\n\nYou want to normalize the sales values across each category within each region. How can you do this while maintaining the original multi-index structure? Here is the beginning of the solution:\n\n```python\nimport pandas as pd\n\ndata = {\n    ('North', 'Electronics', '2023-01-01'): 1000,\n    ('North', 'Clothing', '2023-01-01'): 500,\n    ('North', 'Electronics', '2023-01-02'): 1200,\n    ('North', 'Clothing', '2023-01-02'): 600,\n    ('South', 'Electronics', '2023-01-01'): 800,\n    ('South', 'Clothing', '2023-01-01'): 400,\n    ('South', 'Electronics', '2023-01-02'): 950,\n    ('South', 'Clothing', '2023-01-02'): 500,\n}\nsales_data = pd.Series(data).unstack(level=[0, 1]).stack()\nsales_data.index = pd.MultiIndex.from_tuples(sales_data.index)\n\nnormalized_sales = ...  # code to normalize the sales data while keeping the multi-index\n```\nFill in the blank for `normalized_sales`.",
    "output": "\n```python\nnormalized_sales = sales_data.groupby(level=[0, 1]).apply(lambda x: x / x.sum())\n```"
  },
  {
    "input": "Problem:\nYou are working with a sales dataset in a pandas DataFrame that includes sales records for different products across multiple regions over a period of time. The DataFrame `df` is structured as follows:\n\n```python\nimport pandas as pd\n\ndata = {\n    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],\n    'Product': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n    'Sales': [150, 200, 100, 250, 300, 150, 400, 100],\n    'Date': pd.date_range(start='2023-01-01', periods=8, freq='W')\n}\n\ndf = pd.DataFrame(data)\n```\n\nYou want to pivot this DataFrame to analyze total sales by each product and region, while simultaneously normalizing the sales values using Min-Max normalization within each region. The final DataFrame should include the total sales for each product-region combination, and an additional column for the normalized sales value.\n\nWhat would be the most efficient way to achieve this in Pandas? \n\nFill in the blank below with the appropriate code:\n\n```python\npivoted_df = df.pivot_table(index='Product', columns='Region', values='Sales', aggfunc='sum').fillna(0)\nnormalized_sales = (pivoted_df - pivoted_df.min()) / (pivoted_df.max() - pivoted_df.min())\nfinal_result = pd.concat([pivoted_df, normalized_sales.add_suffix('_Normalized')], axis=1)\n```\n\nresult = ... # put solution in this variable\nBEGIN SOLUTION\n<code>\n\n\n",
    "output": "\n```python\nresult = final_result\n</code>"
  },
  {
    "input": "Problem:\n\nI am working with a PyTorch model that takes a tensor of shape (batch_size, sequence_length, feature_dimension) as input. I need to compute the cosine similarity between the first and second entries across the batch, and I want to return a tensor of shape (batch_size, 1) that contains the cosine similarity for each pair.\n\nThe formula for cosine similarity is given by:\n\\[ \\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} \\]\n\nFor instance, if my tensor looks like this:\n```python\ninput_tensor = torch.tensor([\n    [[1, 2, 3], [4, 5, 6]],\n    [[7, 8, 9], [1, 0, 0]]\n])\n```\nThen I want a tensor that contains the cosine similarity of the first and second elements of the sequences from each batch.\n\nHow can I achieve this in PyTorch?\n\nA:\n<code>\nimport torch\n\ninput_tensor = torch.tensor([\n    [[1, 2, 3], [4, 5, 6]],\n    [[7, 8, 9], [1, 0, 0]]\n])\n\ndef compute_cosine_similarity(tensor):\n    # return the cosine similarity in this function\n    # similarity = compute_cosine_similarity(tensor)\n    ### BEGIN SOLUTION\n",
    "output": "\n<code>\n    A = tensor[:, 0, :]  # First entry across the batch\n    B = tensor[:, 1, :]  # Second entry across the batch\n\n    dot_product = (A * B).sum(dim=1)  # Compute dot product\n    norm_A = A.norm(dim=1)  # Compute norm of A\n    norm_B = B.norm(dim=1)  # Compute norm of B\n\n    cosine_similarity = dot_product / (norm_A * norm_B)  # Compute cosine similarity\n    return cosine_similarity.unsqueeze(1)  # Return shape (batch_size, 1)\n\nsimilarity = compute_cosine_similarity(input_tensor)\n</code>"
  }
]